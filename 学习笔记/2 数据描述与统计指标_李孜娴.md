# 第二章 数据描述与统计指标

## 2.1数据预处理

由于原始数据不符合进一步分析和建模的格式，所以需要对其进行一系列处理，使其转化为适合进一步分析和建模的格式，数据预处理是我们进行数据挖掘时非常重要的一个环节。

### 2.1.1数据预处理概要

#### 2.1.1.1数据质量

数据质量是跨多维度对数据健康状况的评估，用于衡量数据集满足完整性、一致性、准确性、可靠性、可用性和时效性标准的程度。

为了判断数据的质量是否可以支持准确、有效的分析和挖掘，我们需要在进行数据挖掘钱对数据质量进行分析和评估。数据质量分析涉及到完整性、一致性等六个要素，如表2-1所示。

表2-1数据质量要素

|  要素  |                          描述                          |
| :----: | :----------------------------------------------------: |
| 完整性 |    数据是否包含所有必要信息，是否缺失重要字段或记录    |
| 一致性 |     数据是否在不同的数据源、数据集或时间中保持一致     |
| 准确性 |   数据是否与真实情况的匹配程度，是否存在错误或异常值   |
| 可靠性 |                 数据是否来自可信的来源                 |
| 可用性 | 数据能否访问和使用，数据存储、检索、传输和分析的便捷性 |
| 时效性 |               数据是否及时更新，是否过时               |

#### 2.1.1.2数据预处理的主要任务

数据预处理是我们根据数据质量分析中发现的错误、缺失、重复、异常等问题，对原始数据进行清洗、转换、整合等处理，或将数据处理成适合下游任务的格式，进而提高数据质量的过程。

为了保证数据质量、优化分析效率和提高模型准确度，我们必须在进行数据分析和挖掘前对数据进行预处理。数据预处理主要包括数据清洗、数据集成、数据规约和数据变换等，如图2-1所示。

<img src="assets\image-20241215103400316.png" alt="image-20241215103400316" style="zoom:20%;" />

#### 2.1.1.3数据处理的整体流程和逻辑框架

数据预处理通常遵循以下流程：

（1）数据收集与理解 ；

（2）数据清洗（处理缺失值、异常值等）； 

（3）数据集成（合并多个数据源）；

（4）数据变换（标准化、归一化等） ；

（5）特征工程（特征选择、构造新特征）；

（6）数据降维（如需要） ；

（7）数据平衡与增强（处理类别不平衡问题）；

这个流程并非严格线性，可能需要多次迭代和调整。

#### 2.1.1.4不同类型数据的预处理差异和挑战

不同类型的数据需要不同的预处理方法：

（1）数值型数据

挑战：异常值检测、缩放、分布调整

方法：标准化、归一化、对数变换等

（2）类别型数据

挑战：高基数类别、编码方式选择

方法：One-Hot编码、标签编码、目标编码等

（3）文本数据

挑战：非结构化、高维度、语义理解

方法：分词、去停用词、词向量化、主题建模等

（4）图像数据

挑战：高维度、噪声、变形不变性

方法：归一化、数据增强（旋转、缩放、翻转）、降维

（5）音频数据

挑战：时间依赖性、噪声、特征提取

方法：滤波、频谱分析、MFCC特征提取

（6）时序数据

挑战：时间依赖性、趋势和季节性、不规则采样

方法：滑动窗口、差分、时间特征工程

每种类型的数据都有其独特的挑战和相应的处理技巧。在实际应用中，我们常常需要结合多种方法来处理复杂的数据集。

### 2.1.2数据清洗

数据清洗是去除数据中的缺失值、噪声，并提高数据质量的数据预处理方法。

#### 2.1.2.1缺失值处理

##### （1）删除数据

删除数据是指从数据集中删除缺少某些变量的记录，这样可以减少缺失值对分析结果的影响，并且可以保证数据的一致性和可靠性。

类型：整例删除和变量删除。

①整例删除是剔除含有缺失值的样本，适用于样本量很大，且缺失值所占比例较小(<5%)的情况。

②变量删除是直接删除包含缺失值的变量，当一个变量的缺失值比例(>70%)很高时，可以考虑采用变量删除来处理缺失值。

##### （2）人工填充

人工填充是指使用专家知识或者其他外部信息来填补缺失值，适用于实验室实验设计、调查问卷研究、医疗记录等场景，通常需要专业判断或领域知识。然而，该方法可能给数据引入主观因素和不确定性，并且需要花费较多的时间和资源，适用于缺失值较少的情况。

##### （3）均值/众数填充

均值填充和众数填充能够快速简便地填充缺失值。

①均值填充指用变量的平均值来代替其中的缺失值，适用于数值型数据。

②众数填充是用变量的众数来代替缺失值，适用于离散数据。

均值填充和众数填充可能会对数据集的统计性质和模型效果产生影响，需要根据具体情况决定是否使用。例如，使用均值填充会使数据集的均值产生偏移，因为缺失值被替换为固定的数值，这可能会影响数据的中心趋势和离散程度。而使用众数填充会导致数据集的众数更加频繁出现，可能使数据的类别分布不再符合实际情况。

##### （4）模型填充

模型填充是利用已有特征和目标变量之间的关系，基于已知数据建立模型来预测缺失值。对于连续型属性，可基于已有数据建立回归模型来预测缺失数据的值，常用的回归模型包括线性回归、多项式回归、岭回归等。对于离散型属性，常用的模型包括K-近邻、决策树、随机森林、神经网络等模型。模型填充充分利用了数据之间的相关性，但在使用时需要注意特征和模型的选择，并进行合理的验证和评估，以保证填充结果的可靠性。

##### （5）插值方法

插值方法也是一种常见且有效的填充技术。插值通过在已知数据点之间进行估计来推断缺失值，主要有以下几种经典方法:

线性插值：假设在已知数据点之间的数据变化是线性的。例如，对于一维数据，线性插值通过连接相邻数据点的直线来估计中间的缺失值。

多项式插值:通过拟合一个多项式函数来逼近已知数据点之间的数据变化，可以通过拉格朗日插值或牛顿插值等方法实现。

克里金插值:是一种基于空间统计的插值方法，通过在空间上建立半变异函数来估计未知位置的值。它考虑了数据点之间的空间相关性，能提供更加平滑和逼真的插值结果。这些插值方法在数据填充过程中能够有效地利用已有数据点之间的关系，帮助预测和填补缺失值。

#### 2.1.2.2缺失值处理示例

##### （1）均值、中位数和众数填充

代码：

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer

#创建示例数据集
np.random.seed(42)
df = pd.DataFrame({'A': np.random.randn(100),
                   'B': np.random.randn(100),
                   'C': np.random.randn(100)
                   })
df.loc[np.random.choice(df.index, 20), 'A'] = np.nan
df.loc[np.random.choice(df.index, 20), 'B'] = np.nan
df.loc[np.random.choice(df.index, 20), 'C'] = np.nan

#不同填充方法
methods = ['mean','median','most_frequent']
imputed_dfs = {}

for method in methods:
    imputer = SimpleImputer(strategy=method)
    imputed_dfs[method] = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)

#可视化比较
plt.figure(figsize=(15,5))
for i, (name, imputed_df) in enumerate([('Original', df)] + [(m, imputed_dfs[m]) for m in methods]):
    plt.subplot(1,4, i+1)
    plt.scatter(imputed_df['A'],imputed_df['B'], alpha=0.5)
    plt.title(f'{name.capitalize()} Imputation')
    plt.xlabel('Feature A')
    plt.ylabel('Feature B')

plt.tight_layout()
plt.show()

#统计比较
for name, imputed_df in[('Original', df)] + [(m, imputed_dfs[m]) for m in methods]:
    print(f"\n{name.capitalize()} Imputation Statistics")
    print(imputed_df.describe())
```

![image-20250116162016881](assets\image-20250116162016881.png)

##### （2）基于模型的缺失值填补（如K-最近邻 KNN 、多重插补链式方程 MICE）

代码：

```
import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
import matplotlib.pyplot as plt

#创建示例数据集
np. random.seed(42)
df = pd.DataFrame({
    'A': np.random.randn(100),
    'B': np.random.randn(100),
    'C': np.random.randn(100)
})
df.loc[np.random.choice(df.index, 20), 'A'] = np.nan
df.loc[np.random.choice(df.index, 20), 'B'] = np.nan
df.loc[np.random.choice(df.index, 20), 'C'] = np.nan

# KNN填充
knn_imputer = KNNImputer(n_neighbors=5)
df_knn = pd.DataFrame(knn_imputer.fit_transform(df), columns=df.columns)

# MICE填充
mice_imputer=IterativeImputer(random_state=0)
df_mice = pd.DataFrame(mice_imputer.fit_transform(df), columns=df.columns)

#可视化比较
plt.figure(figsize=(15, 5))
for i, (name, imputed_df) in enumerate([('Original', df), ('KNN', df_knn),('MICE', df_mice)]):
    plt.subplot(1,3,i+1)
    plt.scatter(imputed_df['A'], imputed_df['B'], alpha=0.5)
    plt.title(f'{name} Imputation')
    plt.xlabel('Feature A')
    plt.ylabel('Feature B')
    
plt.tight_layout()
plt.show()

#统计比较
for name, imputed_df in [('Original', df),('KNN', df_knn),('MICE', df_mice)]:
    print(f"\n{name} Imputation statistics:")
    print(imputed_df.describe())
```

![image-20250116164623823](assets\image-20250116164623823.png)

##### （3）示例3

```
import pandas as pd
import numpy as np

# 创建一个包含缺失值的DataFrame
data = {
    '城市': ['北京', '上海', '广州', '深圳', '杭州'],
    '人口（百万）': [21.5, np.nan, 18.5, 17.5, np.nan],
    'GDP（万亿）': [3.6, 4.1, np.nan, 3.0, 2.5],
    '旅游景点数量': [50, 45, np.nan, 40, 35]
}
df = pd.DataFrame(data)
print("原始数据：")
print(df)

# 查看缺失值情况
print("\n查看缺失值情况：")
print(df.isnull().sum())

# 删除含有缺失值的行
df_dropna = df.dropna()
print("\n删除含有缺失值的行后：")
print(df_dropna)

# 用均值填充GDP（万亿）的缺失值
gdp_mean = df['GDP（万亿）'].mean()
df['GDP（万亿）'] = df['GDP（万亿）'].fillna(gdp_mean)
print("\n用均值填充GDP（万亿）的缺失值后：")
print(df)

# 用中位数填充人口（百万）的缺失值
population_median = df['人口（百万）'].median()
df['人口（百万）'] = df['人口（百万）'].fillna(population_median)
print("\n用中位数填充人口（百万）的缺失值后：")
print(df)

# 用众数填充旅游景点数量的缺失值
attractions_mode = df['旅游景点数量'].mode()[0]
df['旅游景点数量'] = df['旅游景点数量'].fillna(attractions_mode)
print("\n用众数填充旅游景点数量的缺失值后：")
print(df)
```

![image-20250116170843417](assets\image-20250116170843417.png)

![image-20250116170904745](assets\image-20250116170904745.png)

![image-20250116170921167](assets\image-20250116170921167.png)

#### 2.1.2.3噪声处理

噪声（Noise）是指数据集中由于测量误差、环境干扰、传感器故障等多种随机性因素引起的错误或异常的部分。

摄像头捕捉图像时，可能会因为光线不足或镜头质量等原因导致图像中出现噪点，从而对数据分析结果造成不良影响，因此需要采取合适的措施处理噪声。常见处理方法包括分箱、回归、离群点检测、均值平滑、主成分分析降噪等。

##### （1）分箱

分箱(Binning)是将有序数据按特定的规则进行分组，进而实现数据离散化的降噪技术。

在分箱处理中，首先按照属性值划分子区间，如果一个属性值处于某个子区间范围内，就称为把该属性值放进这个子区间所代表的“箱”内。箱的宽度表示箱中数据的取值范围差值，箱的深度表示箱中数据的数量，具体的分箱方式可分为等宽分箱和等深分箱：

1）等宽分箱是指每个箱的宽度区间一致。假设有一个数据集{x1,x2,...xn}，要将其分为k个等宽的箱，每个箱的宽度用公式表示为：

<img src="assets\image-20241215151303752.png" alt="image-20241215151303752" style="zoom:25%;" />

2）等深分箱是指每个箱中数据的个数基本相等。假设有一个数据集{x1,x2,...xn}，要将其分为k个等深的箱，每个箱的数据点个数用公式表示为：

<img src="assets\image-20241215151412593.png" alt="image-20241215151412593" style="zoom:70%;" />

完成分箱之后，需要对数据进行平滑处理:

1)箱均值平滑，将箱内的数据替换为该箱中数据的均值;

2)箱中位数平滑，将箱内的数据替换为该箱中数据的中位数:

3)箱边界平滑将箱中的最大值和最小值作为箱边界，其余数据替换为距离边界最近的边界值。

由于分箱方法考虑相邻的值，因此是一种局部平滑方法，箱的宽度越大，平滑效果越好。

例子：价格，排序后数据：3，8，16，21，25，25，32，39，44，使用等深分箱将数据分为3组

|      | 等深分箱   | 箱均值平滑 | 箱中位数平滑 | 箱边界平滑 |
| ---- | ---------- | ---------- | ------------ | ---------- |
| 箱1  | 3，8，16   | 9，9，9    | 8，8，8      | 3，3，16   |
| 箱2  | 21，25，25 | 24，24，24 | 25，25，25   | 21，25，25 |
| 箱3  | 32，39，44 | 38，38，38 | 39，39，39   | 32，44，44 |

##### （2）回归

回归分析是一种利用统计学方法对数据进行平滑处理的技术，旨在从包含噪声的数据中提取出真实的趋势和模式，用公式表示为：

![image-20241215155850821](assets\image-20241215155850821.png)

回归降噪的实现原理基于回归分析，对数据进行拟合并去除噪声，从而使数据更加稳定和可靠。

在使用回归进行降噪时，首先，将原始数据按照一定的顺序排列。然后，通过回归模型拟合这些数据，例如最小二乘法或局部加权回归。接下来，用回归模型的输出值替代原始数据中的噪声部分，从而去除噪声的影响。值得注意的是，在使用回归降噪前，需要观察数据的变化趋势，判断其是否存在规律性，如果存在，才能通过回归的方式进行有效降噪。图 2-2 展示了某高温反应釜在实验过程中的温度变化，其变化存在明显的周期趋势，使用多项式回归法可以将该趋势较好地提取出来。

<img src="assets\image-20241215161021022.png" alt="image-20241215161021022" style="zoom:80%;" />

##### （3）离群点检测

离群点(Outlier) 是数据集中不同于其他大部分数据对象，或是相对于其属性的典型值来说不同寻常的数据点。

常用的离群点检测方法包括聚类和3σ准则，如图2-3所示。在基于聚类的离群点检测中，根据相似性原则将数据集分为不同的簇，使得簇内数据相似，簇间数据相异，因此落在簇集合之外的样本被直观地标记为离群点。在3σ准则中，若数据服从正态分布，约99.7%的数据落在平均值μ附近的三个标准差σ范围内，因此将该范围外的数据视为离群点。

<img src="assets\image-20241215161223132.png" alt="image-20241215161223132" style="zoom:80%;" />

通常情况下，离群点是需要剔除的，但在一些特殊应用场景中，离群点具有较高的价值，例如热点事件、技术趋势、欺诈和网络攻击的检测等。

##### （4）均值平滑

均值平滑方法基于一个简单的假设:数据中的噪声是随机波动的，即该波动不是由于真实的趋势或周期性变化造成的，通过平均多个数据点可以消除这种噪声。均值平滑通过计算一组数据点的平均值来减少随机波动，从而使数据更加平滑，以便更好地观察数据中的趋势和周期性变化。假设对一组数据xt,...,xt+8，均值平滑后的结果如下:

![image-20241215163010213](assets\image-20241215163010213.png)

在均值平滑中，数据点被分成数量相等的子组，再将每个子组的所有数据替代为该组的平均值。子组中数据点的个数g也被称为“窗口”，其用于控制平滑程度，窗口越大，平滑效果越明显。一般来说，窗口大小应该足够大以捕获数据中的趋势和周期性，但也不能太大，以免模糊了数据的细节。

##### (5)小波降噪

小波降噪是一种信号处理技术，旨在通过使用小波变换将信号转换到小波域来降低噪声和提高信号质量，其基本思想是通过去除小波域中的低幅度高频噪声系数来过滤掉噪声。这种方法可以避免信号失真，并且在保留信号细节的同时去除噪声。

在小波变换中，待分析信号x(t)是一维连续或离散信号，通常表示为关于时间t的函数。小波变换先把某一被称为基本小波的函数φ(t)作位移τ后，再在不同尺度α下，与待分析信号x(t)作内积，即

<img src="assets\image-20241215163646543.png" alt="image-20241215163646543" style="zoom:80%;" />

其中α>0，称为尺度因子，其作用是对基本小波φ(t)函数作伸缩，τ反映位移，其值可正可负，τ和α都是连续的变量，故又称为连续小波变换。

小波降噪可以分为分解和重构两个阶段。在分解阶段中，每个小波子带代表不同频率的信号成分，其中高频子带包含噪声成分。在重构阶段中，通过保留信号中的重要小波系数，并去除低幅度高频噪声系数，重构出降噪后的信号。图2-4 展示了某股票的开盘价格变化趋势，使用小波降噪可保留主要变动趋势，有助于提高投资决策的可靠性。值得一提的是，不同于均值平滑，小波降噪在去除噪声的同时尽可能保留了数据的重要细节和结构，能为数据分析和决策提供更为可靠和精确的结果。

![image-20241215163917260](assets\image-20241215163917260.png)

#### 2.1.2.4噪声处理示例

##### （1）分箱

```
import pandas as pd
import numpy as np

# 生成示例数据
np.random.seed(0)
data = pd.DataFrame({
    'value': np.random.normal(loc=50, scale=10, size=100)  # 生成100个均值为50，标准差为10的正态分布数据
})

# 对数据进行分箱处理
bin_edges = np.arange(start=data['value'].min(), stop=data['value'].max() + 10, step=10)
data["bin"] = pd.cut(data["value"], bins=bin_edges)

# 对每个箱子中的数据进行平均值处理，以减少噪声
smoothed_values = data.groupby("bin", observed=True)["value"].mean()
data["smoothed_value"] = data["bin"].map(smoothed_values)

print("原始数据：")
print(data[['value', 'bin']].head(10))

print("\n处理后的数据：")
print(data[['smoothed_value', 'bin']].head(10))
```

![image-20250116204859355](assets\image-20250116204859355.png)

![image-20250116204923335](assets\image-20250116204923335.png)

##### （2）回归

```
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
import matplotlib.pyplot as plt

# 生成示例数据
np.random.seed(0)
x = np.linspace(0, 4 * np.pi, 100)
y = np.sin(x) + np.random.normal(0, 0.5, 100)  # 生成100个数据点，正弦波关系加上正态分布噪声

# 创建DataFrame
data = pd.DataFrame({'x': x, 'y': y})

# 拟合多项式回归模型
poly = PolynomialFeatures(degree=10)  # 使用10次多项式来拟合正弦波的波动趋势
x_poly = poly.fit_transform(data[['x']])
model = LinearRegression()
model.fit(x_poly, data['y'])

# 使用模型预测y值，以减少噪声
data['y_pred'] = model.predict(x_poly)

# 绘制原始数据和预测数据
plt.figure(figsize=(10, 6))
plt.scatter(data['x'], data['y'], color='blue', label='Original Data')
plt.plot(data['x'], data['y_pred'], color='red', label='Polynomial Regression Line')
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Noise Reduction using Polynomial Regression')
plt.legend()
plt.show()

# 打印前10个数据点的原始值和预测值
print("原始数据和预测数据的前10行：")
print(data[['x', 'y', 'y_pred']].head(10))
```

![image-20250116205514306](assets\image-20250116205514306.png)

![image-20250116205535811](assets\image-20250116205535811.png)

##### （3）离群点检测

###### ①基于统计学方法（Z-Score、IQR）

a) Z-Score方法：公式：Z = (X - μ) / σ 其中，X是原始值，μ是平均值，σ是标准差 原理：假设数据呈正态分布，将原始数据转换为标准正态分布。通常|Z| > 3被视为异常值。

b) IQR（四分位距）方法：公式：IQR = Q3 - Q1 下界 = Q1 - 1.5 * IQR 上界 = Q3 + 1.5 * IQR 原理：利用数据的四分位数来定义异常值范围，对非正态分布数据也适用。

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
#创建包含异常值的示例数据
np.random.seed(42)
data = np.concatenate([np.random.normal(0,1,980),np.random.normal(5, 1, 20)])
df = pd.DataFrame({'value': data})
# Z-Score方法
z_scores = np.abs(stats.zscore(df['value']))
z_score_outliers = df[z_scores >3]
# IQR方法
Q1 = df['value'].quantile(0.25)
Q3 = df['value'].quantile(0.75)
IQR = Q3 - Q1
iqr_outliers = df[(df['value']<(Q1 - 1.5 * IQR)) | (df['value']>(Q3 + 1.5 * IQR))]
# 可视化
plt.figure(figsize=(12,6))
plt.subplot(121)
plt.hist(df['value'], bins=50, edgecolor='black')
plt.title('Data Distribution')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.subplot(122)
plt.boxplot(df['value'])
plt.title('Box Plot')
plt.ylabel('Value')
plt.tight_layout()
plt.show()
print(f"Z-Score method detected {len(z_score_outliers)} outliers")
print(f"IQR method detected {len(iqr_outliers)} outliers")
#显示检测到的异常值
print("\nZ-Score 0utliers:")
print(z_score_outliers)
print("\nIOR Outliers:")
print(iqr_outliers)
```

![image-20250116210610824](assets\image-20250116210610824.png)

该代码示例展示了如何使用Z-Score和IQR方法检测异常值，并通过直方图和箱线图可视化数据分布。

###### ② 基于模型的检测（Isolation Forest、LOF）

a) Isolation Forest：

原理：通过随机选择特征和分割点来构建决策树，异常点往往更容易被隔离。

算法步骤：

（1）随机选择一个特征 （2）在该特征的最大值和最小值之间随机选择一个分割点 （3）重复1-2，直到每个样本被隔离或达到指定深度 （4）计算平均路径长度，路径较短的被视为异常点

b) LOF（局部异常因子）：

原理：比较一个点的密度与其邻居的密度，如果一个点的密度明显低于其邻居，则可能是异常点。

算法步骤：

（1）计算每个点的k-距离（到第k个最近邻居的距离） （2）计算每个点的可达距离 （3）计算每个点的局部可达密度 （4）计算LOF值（邻居的平均局部可达密度与点自身局部可达密度的比值）

```
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.datasets import make_blobs
#创建包含异常值的示例数据
X, _ = make_blobs(n_samples=300, centers=1, cluster_std=0.5, random_state=0)
X = np.concatenate([X,np.random.uniform(low=-4,high=4,size=(15,2))])
# Isolation Forest
iso_forest = IsolationForest(contamination=0.1, random_state=42)
iso_forest_labels = iso_forest.fit_predict(X)
# LOF
lof = LocalOutlierFactor(n_neighbors=20,contamination=0.1)
lof_labels = lof.fit_predict(X)
#可视化
plt.figure(figsize=(12,5))

plt.subplot(121)
plt.scatter(X[:,0], X[:,1],c=iso_forest_labels, cmap='viridis')
plt.title('Isolation Forest')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.subplot(122)
plt.scatter(X[:,0], X[:,1],c=lof_labels, cmap='viridis')
plt.title('Local Outlier Factor')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.tight_layout()
plt.show()
print(f"Isolation Forest detected {sum(iso_forest_labels == -1)} outliers")
print(f"LOF detected {sum(lof_labels == -1)} outliers")
```

![image-20250116211443633](assets\image-20250116211443633.png)

该代码示例展示了如何使用Isolation Forest和LOF方法检测异常值，并通过散点图可视化检测结果。

##### （4）均值平滑

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# 生成示例数据
np.random.seed(0)
x = np.linspace(0, 10, 100)
y = 2 * x + np.random.normal(0, 2, 100)  # 生成100个数据点，线性关系加上正态分布噪声

# 创建DataFrame
data = pd.DataFrame({'x': x, 'y': y})

# 使用均值平滑方法进行噪声处理
window_size = 5  # 定义窗口大小
data['y_smooth'] = data['y'].rolling(window=window_size, center=True).mean()

# 绘制原始数据和平滑后的数据
plt.figure(figsize=(10, 6))
plt.plot(data['x'], data['y'], label='Original Data', color='blue', alpha=0.5)
plt.plot(data['x'], data['y_smooth'], label='Smoothed Data', color='red')
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Noise Reduction using Mean Smoothing')
plt.legend()
plt.show()

# 打印前10个数据点的原始值和平滑后的值
print("原始数据和平滑后数据的前10行：")
print(data[['x', 'y', 'y_smooth']].head(10))
```

![image-20250116211959499](assets\image-20250116211959499.png)

窗口大小（`window_size`）是一个重要的参数，它决定了平滑的程度。窗口越大，平滑效果越明显，但可能会丢失更多的细节信息。需要根据具体数据和需求来选择合适的窗口大小。

##### （5）小波降噪

```
import numpy as np
import matplotlib.pyplot as plt
import pywt

def add_noise(signal, snr_db):
    """
    向信号中添加高斯白噪声
    参数：
    signal: 原始信号
    snr_db: 信噪比（dB）
    返回：
    noisy_signal: 含噪信号
    """
    signal_power = np.mean(signal ** 2)
    snr_linear = 10 ** (snr_db / 10)
    noise_power = signal_power / snr_linear
    noise = np.random.normal(0, np.sqrt(noise_power), signal.shape)
    noisy_signal = signal + noise
    return noisy_signal

def wavelet_denoise(noisy_signal, wavelet, level, threshold):
    """
    小波去噪
    参数：
    noisy_signal: 含噪信号
    wavelet: 小波基名称
    level: 分解层数
    threshold: 阈值
    返回：
    denoised_signal: 去噪后信号
    """
    # 小波分解
    coeffs = pywt.wavedec(noisy_signal, wavelet, level=level)
    # 阈值处理（软阈值）
    coeffs_thresh = coeffs[:]
    coeffs_thresh[1:] = [pywt.threshold(i, value=threshold, mode='soft') for i in coeffs_thresh[1:]]
    # 小波重构
    denoised_signal = pywt.waverec(coeffs_thresh, wavelet)
    return denoised_signal

# 生成原始信号（示例：正弦波）
t = np.linspace(0, 1, 500)
freq = 5  # 5 Hz
original_signal = np.sin(2 * np.pi * freq * t)

# 添加噪声
snr_db = 5  # 信噪比为5 dB
noisy_signal = add_noise(original_signal, snr_db)

# 小波去噪参数设置
wavelet = 'db8'  # Daubechies 8
level = 4        # 分解层数
threshold = 0.4  # 阈值，可以根据实际情况调整

# 执行小波去噪
denoised_signal = wavelet_denoise(noisy_signal, wavelet, level, threshold)

# 绘制结果
plt.figure(figsize=(12, 8))
plt.subplot(3,1,1)
plt.plot(t, original_signal, label='Original Signal')
plt.legend()
plt.title('Original Signal')
plt.subplot(3,1,2)
plt.plot(t, noisy_signal, label='Noisy Signal')
plt.legend()
plt.title('Noisy Signal (SNR = 5 dB)')
plt.subplot(3,1,3)
plt.plot(t, denoised_signal, label='Denoised Signal')
plt.legend()
plt.title('Denoised Signal by Wavelet')
plt.tight_layout()
plt.show()
```

![image-20250116213244154](assets\image-20250116213244154.png)

### 2.1.3数据集成

数据集成(Data integration)是将不同格式、来源以及性质的数据合并为一个完整数据集的数据预处理方法，

数据集成可以扩大数据的范围和覆盖面，提高数据综合价值和可用性。数据集成主要包括实体识别和冗余数据处理。

#### 2.1.3.1 实体识别

实体识别是指识别一个或多个数据源中的两条不同记录是否描述同一实体。具体地，实体识别主要解决数据的字段意义和字段结构问题，如表2-3所示。在数据字段中，同名字段可能表示不同的含义，异名字段也可能表示相同的含义，因此需要对字段命名规则进行统一处理。此外，属性值的数据类型、数据格式、规格单位、取值范围不同，也会使得同一对象在不同的数据源中属性值不同，需进行统一处理。
![image-20241215165554685](assets\image-20241215165554685.png)

<img src="assets\image-20241215165858848.png" alt="image-20241215165858848" style="zoom: 67%;" />

#### 2.1.3.2冗余数据处理

在数据集成过程中，往往会产生冗余数据，它可能会影响数据分析的效率和准确性，同时也会浪费存储空间和计算资源。常见的冗余数据及其处理方式包括：

(1)重复数据:数据集中存在完全相同或几乎相同的记录或观测值，这些数据可能是由于数据输入错误、重复录入或其他原因导致的。为处理重复数据,可以先通过比较记录或观测值来确定是否存在重复数据，再删除其中的一条或多条重复记录。

(2)重复属性:数据集中存在具有相同信息的属性，这些属性以不同的形式出现。重复属性的处理是将具有相同信息或可以相互转化(如出生年份与年龄的属性合并为一个属性，进而减少属性数量，提高数据处理效率。如果两个或多个属性完全相同，可以选择删除其中一个属性。如果重复属性具有略微不同的值但表达的是相同的含义，可以将它们合并为一个属性。

(3)高相关属性:数据集中存在相关程度极高的不同属性，通过其中一个数据可很大程度上推导出另一个属性。对于高相关属性的处理，可通过删除其中一个或多个属性、合并属性或降维的方式减少冗余。标称属性和数值属性的相关性评估方式不尽相同。标称属性可使用卡方检验评估不同属性的关联程度，数值数据可通过计算相关系数和协方差来评估两个属性值间的相关关系。

### 2.1.4 数据规约

数据规约(Datareduction) 是对数据进行采样、聚合、抽取等进而降低数据的维度和规模的数据预处理方法。

在实际应用中，海量数据往往会占据大量的存储空间和计算资源，数据规约在尽可能保持数据原貌的前提下，最大限度地精简数据量，可以提高数据加工处理和开发利用的效率。不仅如此，数据归约还对模型的通用性和模型性能有重要作用。数据规约适用于数据集非常大，超出处理能力，需要减少数据量的情况。数据规约策略包括维度规约、数量规约和数据压缩。

#### 2.1.4.1 维度规约

维度规约(Dimensionality reduction)通过减少数据中的属性个数，摒弃掉不重要的属性，保留少数关键属性来描述数据。

维度规约的核心思想是通过去除数据中的冗余属性或者将数据投影到低维空间中，从而降低数据的复杂性。本小节介绍的主要方法包括主成分分析、属性子集选择。

##### (1)主成分分析（PCA）

主成分分析的核心思想是将高维数据映射到低维空间中，同时保留数据的主要特征。具体来说，主成分分析通过计算数据集中的协方差矩阵，然后找到最大的协方差方向，即第一主成分，将数据映射到这个方向上。接着，找到与第一主成分正交的方向，即第二主成分，将数据再次映射到这个方向上。以此类推，可以得到一组新的低维特征，它们是原始数据的线性组合，同时保留了原始数据的大部分方差。最后，根据主成分的重要性(根据方差解释比例计算)进行降序排列，可以通过去掉重要性较低的成分来达到规约数据的目的。

###### 示例1：主成分分析

```
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import make_classification

# 生成示例数据
X, y = make_classification(n_samples=200, n_features=3, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)

# 标准化数据
X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)

# 创建PCA模型
pca = PCA(n_components=2)

# 拟合PCA模型并转换数据
X_pca = pca.fit_transform(X)

# 打印解释的方差比例
print("Explained variance ratio:", pca.explained_variance_ratio_)

# 可视化原始数据和PCA转换后的数据
plt.figure(figsize=(12, 6))

# 原始数据
plt.subplot(1, 2, 1)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k', s=50)
plt.title("Original Data")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")

# PCA转换后的数据
plt.subplot(1, 2, 2)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=50)
plt.title("PCA Transformed Data")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")

plt.tight_layout()
plt.show()
```

![image-20250116234314050](assets\image-20250116234314050.png)

原始数据图表显示原始数据的前两个特征，PCA转换后的数据图表是显示经过PCA转换后的数据在新的两个主成分上的散点图，PCA的目的是用较少的成分来近似原始数据。

![image-20250116234626526](assets\image-20250116234626526.png)

这意味着第一个主成分解释了大约41.89%的方差，第二个主成分解释了大约32.53%的方差。这两个主成分加起来解释了主要的方差，因此可以用这两个主成分来很好地近似原始数据。

###### 示例2：主成分分析

```
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification

#生成高维数据
X, y = make_classification(n_samples=1000, n_features=20,n_informative=10,n_redundant=10,
                         n_classes=2,random_state=42)


# 标准化数据
scaler=StandardScaler()
X_scaled= scaler.fit_transform(X)

# 应用PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# 计算累积解释方差比
cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)

#可视化累积解释方差比
plt.figure(figsize=(10,6))
plt.plot(range(1, len(cumulative_variance_ratio)+ 1),cumulative_variance_ratio, 'bo-')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance Ratio')
plt.title('PCA:Cumulative Explained Variance Ratio')
plt.grid(True)
plt.show()

# 选择保留95%方差的组件数量
n_components = np.argmax(cumulative_variance_ratio >= 0.95)+ 1
print(f"保留95%方差所需的组件数量:{n_components}")

# 使用选定的组件数量进行PCA
pca = PCA(n_components=n_components)
X_pca_reduced= pca.fit_transform(X_scaled)

#可视化降维后的数据(使用前两个主成分)
plt.figure(figsize=(10,8))
scatter = plt.scatter(X_pca_reduced[:,0],X_pca_reduced[:, 1], c=y, cmap='viridis')
plt.colorbar(scatter)
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title('PCA:First Two Principal Components')
plt.show()
print(f"原始特征数量:{X.shape[1]}")
print(f"降维后的特征数量:{X_pca_reduced.shape[1]}")
```

![image-20250117163552030](assets\image-20250117163552030.png)

![image-20250117163601183](assets\image-20250117163601183.png)

![image-20250117163623512](assets\image-20250117163623512.png)

这个代码示例展示了如何使用PCA对高维数据进行降维，包括选择合适的组件数量和可视化降维结果。

##### (2)属性子集选择

属性子集选择是将原数据中冗余或者不相关的属性进行删减，或者通过对属性进行重组来减少属性个数。其中，判断属性优劣的方法是对其进行显著性检验将一个或多个属性加入回归模型，并判断是否使残差平方和显著减少。如是，则说明有必要将该属性保留，否则，可将该属性删除。如表2-4所示，常用的属性子集选择方法包括合并属性、逐步向前选择、逐步向后删除、组合法、决策树归纳法等。

![image-20241215170555672](assets\image-20241215170555672.png)

![image-20241215170613258](assets\image-20241215170613258.png)

##### （3）高维数据可视化 （t-SNE：t-distributed Stochastic Neighbor Embedding）

t-SNE是一种非线性降维方法，特别适合于高维数据的可视化。它将高维数据映射到低维（通常为2维或3维）空间，确保相似数据点在低维空间中保持邻近关系。

###### 示例： t-SNE

```
from sklearn.manifold import TSNE
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt
import numpy as np

# 生成高维数据
X, y = make_classification(n_samples=1000, n_features=50, n_informative=10, n_redundant=40,
                           n_classes=3, random_state=42)

# 应用t-SNE
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)

# 可视化t-SNE结果
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')
plt.colorbar(scatter)
plt.title('t-SNE Visualization of High-Dimensional Data')
plt.xlabel('t-SNE feature 1')
plt.ylabel('t-SNE feature 2')
plt.show()

# 比较不同perplexity值的效果
perplexities = [5, 30, 50, 100]
fig, axes = plt.subplots(2, 2, figsize=(15, 15))
axes = axes.ravel()

for i, perplexity in enumerate(perplexities):
    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)
    X_tsne = tsne.fit_transform(X)
    
    axes[i].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')
    axes[i].set_title(f't-SNE (perplexity = {perplexity})')
    axes[i].set_xlabel('t-SNE feature 1')
    axes[i].set_ylabel('t-SNE feature 2')
    fig.colorbar(axes[i].collections[0], ax=axes[i])

plt.tight_layout()
plt.show()
```

![image-20250117165722038](assets\image-20250117165722038.png)

![image-20250117165730040](assets\image-20250117165730040.png)

这个代码示例展示了如何使用t-SNE对高维数据进行降维和可视化（从50维降到2维并呈现一定的聚类效果），并比较了不同perplexity值对结果的影响。

#### 2.1.4.2 数量规约

数量规约(Numerosity reduction)是通过直方图、聚类、抽样等方法用更少数量级的数据替换原始数据。

数量规约旨在减少数据量的同时保持数据特征和信息价值，减少数据处理的复杂性和计算成本，提高计算效率，改善模型性能和方便可视化。常用的数量规约方法包括直方图和抽样。

##### (1)直方图规约

直方图规约是使用分箱的方式来近似原始数据的分布进而实现数据归约。图2-5展现了某产品在一年内的每日销量情况，并分别通过等宽分箱和等深分箱对销量数据进行了处理。

<img src="assets\image-20241215170816017.png" alt="image-20241215170816017" style="zoom:67%;" />

图2-5 直方图数据规约案例

###### 示例：直方图规约

```
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import KBinsDiscretizer

# 设置随机种子以便结果可复现
np.random.seed(0)

# 生成模拟的一年内每日销量数据，假设销量服从正态分布
days = 365
sales = np.random.normal(loc=100, scale=20, size=days)

# 绘制原始数据的直方图
plt.figure(figsize=(12, 6))
plt.hist(sales, bins=30, color='skyblue', edgecolor='black')
plt.title('Original Sales Data Histogram')
plt.xlabel('Sales')
plt.ylabel('Frequency')
plt.show()

# 等宽分箱
num_bins = 10
bin_edges = np.linspace(sales.min(), sales.max(), num_bins + 1)
hist, _ = np.histogram(sales, bins=bin_edges)
bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])

# 绘制等宽分箱的直方图
plt.figure(figsize=(12, 6))
plt.bar(bin_centers, hist, width=np.diff(bin_edges), edgecolor='black', align='center')
plt.title('Histogram with Equal Width Bins')
plt.xlabel('Sales')
plt.ylabel('Frequency')
plt.show()

# 等深分箱
kbins = KBinsDiscretizer(n_bins=num_bins, encode='ordinal', strategy='quantile')
discretized_sales = kbins.fit_transform(sales.reshape(-1, 1))
hist, bin_edges = np.histogram(discretized_sales, bins=num_bins, range=(0, num_bins))

# 绘制等深分箱的直方图
plt.figure(figsize=(12, 6))
plt.bar(range(num_bins), hist, width=1, edgecolor='black')
plt.title('Histogram with Equal Depth Bins')
plt.xlabel('Sales Bin')
plt.ylabel('Frequency')
plt.xticks(range(num_bins))
plt.show()
```

![image-20250117131517264](assets\image-20250117131517264.png)

![image-20250117131539888](assets\image-20250117131539888.png)

![image-20250117131611117](assets\image-20250117131611117.png)

##### (2)抽样规约

抽样规约通过在原始数据集中进行抽样来减少数据量。抽样(Sampling)是常用的数据规约技术，它用比原始数据规模小很多的随机样本表示原始数据集，从而减少对原始数据的处理和存储需求。常用的抽样方法按照是否遵循随机原则分为概率抽样和非概率抽样。

概率抽样(Probability sampling)是指抽样时按一定的概率以随机原则抽取样本，总体中每个单位i有一定的机会(Pri=入样概率)被选入样本。概率抽样通常是基于概率理论的基础上进行的，可以通过严格的概率分布来推断总体参数的估计和抽样误差。表2-5和图2-6分别展示了部分常用的概率抽样方法及其示意图。

表2-5 常用的概率抽样方法

| 抽样方法     | 描述                                                         |                                               |
| ------------ | ------------------------------------------------------------ | --------------------------------------------- |
| 简单随机抽样 | 从包括总体N个单位的数据集中随机地、单个地抽取n个单位作为样本，每个单位的入样概率是相等的。根据抽取的单位是否放回总体，可进一步分为有放回简单随机抽样和无放回简单随机抽样。 | Pri=n/N                                       |
| 系统抽样     | 先对数据集中N个单位进行编号，然后确定分段间隔k并对数据分段。在第一段中随机选择一个起始点，然后以k为间隔逐个抽样，直到获取所有样本。 | Pri =n/N=1/k                                  |
| 分层随机抽样 | 按某种特征或某种规则，将总体分为多个不相交的部分(称为“层”)，然后按照某个固定的比例在每层进行独立抽样，如简单随机抽样或系统抽样。 | Prij =nj/Nj,(分层j中单位i的入样概率)          |
| 整群抽样     | 将总体分为G个若干不相交的集合(称为群)，以g个群为抽样单位抽取样本。 | Prik=(g/G)(nk /Nk )(单位i在集群k中的入样概率) |

![image-20241222085437624](assets\image-20241222085437624.png)

###### 示例：系统抽样

```
import numpy as np
import pandas as pd

# 设置随机种子以便结果可复现
np.random.seed(0)

# 生成模拟数据
data = pd.DataFrame({
    'ID': np.arange(1, 1001),
    'Feature1': np.random.rand(1000),
    'Feature2': np.random.rand(1000),
    'Feature3': np.random.rand(1000),
    'Target': np.random.randint(0, 2, 1000)
})

# 显示原始数据的前几行
print("Original data head:")
print(data.head())

# 确定抽样间隔
sample_size = 200
interval = len(data) // sample_size

# 从第一个间隔中随机选择一个起点
start = np.random.randint(0, interval)

# 执行系统抽样
systematic_sampled_data = data.iloc[start::interval]

# 显示系统抽样后数据的形状
print("\nSystematic sampled data shape:", systematic_sampled_data.shape)

# 显示系统抽样后数据的前几行
print("\nSystematic sampled data head:")
print(systematic_sampled_data.head())
```

![image-20250117132101659](assets\image-20250117132101659.png)

非概率抽样(Non-probability sampling)是基于某些特定因素进行的抽料力例如非来的可获得性、研究者的经验判断、研究目的等。在非概率抽样中，例如将杰为群东的概率不是已知的。由于样本选择的主观性和不确定性，所的槿来可能并不能很好地反映总体的特征。尽管如此，在样本数量有限或研究的较为特殊的情况下，该方法仍然是一种有用的抽样方式。表2-6展示了部分用的非概率抽样方法并对其简要描述。
表2-6常用的非概率抽样方法

| 抽样方法 | 描述                                                         |
| -------- | ------------------------------------------------------------ |
| 判断抽样 | 研究者依据自己的专业知识和经验选择样本。适用于对象稀少或难以获取的情况。由于研究者主观判断的影响，可能导致样本选择偏差。 |
| 配额抽样 | 在抽样前，将抽样单位按照某些特定属性划分为若干组，并且对每组制定一个特定的配额。然后，通过方便抽样或判断抽样的方式在每组中选择合配额要求的样本。 |

抽样不仅可以完成数据规约，还可以处理不平衡数据集。不平衡数据集是在机器学习和数据挖掘任务中常见的问题，是指不同类别的样本数量差异很大。这可能会导致模型对多数类别过于偏向，而对少数类别预测效果不佳。以下是一些处理不平衡数据集的常见方法:

过采样:增加少数类别的样本数量，可以通过复制已有的少数类样本或生成合成样。

欠采样:减少多数类别的样本数量，随机删除一些多数类样本。

#### 2.1.4.3 数据压缩

数据压缩(Data compression)是指通过一系列算法和技术，在不丢失有用信息的前提下，缩减数据量以节省存储空间，提高其传输、存储和处理效率。

数据压缩技术可以通过多种方式实现，包括无损压缩和有损压缩。无损压缩在压缩过程中不丢失任何数据，使得压缩后的数据能够完全还原为原始数据，适用于需要精确还原数据的情况。其原理是利用数据的冗余信息和统计性质进行压缩，可完全恢复原始数据而不引起任何失真。无损压缩的常用算法包括霍夫曼编码、字典编码、预测编码等。有损压缩在压缩过程中会牺牲一些数据精度以获得更高的压缩率，广泛应用于图像、音频、视频等多媒体数据中。其基本原理是利用人对细微变化的不敏感性，去除或近似人类感知上不明显的信息。有损压缩的常用算法包括转换编码、量化等。维度规约和数量规约也可以视为某种形式的数据压缩。

### 2.1.5数据变换

数据变换(Data transformation)是通过对原始数据进行特定的转换使其更易于理解和分析的数据预处理方法。

数据变换旨在将原始数据转换成更便于处理的格式。在数据挖掘任务中，原始数据的格式可能各不相同，需要经过一定的转换和处理，才能够进行数据分析和建模。常用的数据变换策略包括数据规范化、连续数据离散化和分类数据编码

#### 2.1.5.1 数据规范化

在实际应用中，不同变量的数据单位、大小、范围和精度等方面可能存在差异，这会影响到数据分析的结果。数据规范化的主要目的就是消除数据之间量纲的差异，使得不同数据可以在同一个基准上进行比较和分析。常用的数据规范化方法包括最小-最大规范化、Z分数规范化、小数定标规范化。表2-7展示了部分常用的数据规范化方法并对其简要描述。
表2-7常用的数据规范化方法

| 数据规范化方法                                 | 描述                                                         | 公式                                                         |
| ---------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 最小-最大规范化( Min-max normalization）       | 将原始数据缩放到[0,1]范围内，最小值和最大值分别映射为0和1中间的数值按照线性比例进行映射。 | ![image-20241222095806117](assets\image-20241222095806117.png) |
| Z分数规范化 (Z-score normalization )           | 将原始数据按照均值和标准差进行线性变换，使数据具有标准正态分布的特性，均值为0，标准差为1. | ![image-20241222095816561](assets\image-20241222095816561.png) |
| 小数定标规范化(Decimal scaling normalization ) | 将所有数据除以一个固定基数，使所有数据绝对值小于1。这个基数通常是数据集最大绝对值的幂次方。 | ![image-20241222095830410](assets\image-20241222095830410.png) |

注意:当数据中存在离群点或极端值时，最小-最大规范化后的数据可能会偏离原始分布假设有以下一组数据表示某小区的房屋面积(单位:平方米):[80,90,100,110,120,500]最小最大规范化后的数据如下：[0.0,0.02940.0588,0.0882.0.1176,1.0]。从结果可以看出由于原始数据中存在离群点 500，这导致了规范化后的数据整体上偏离了原始分布，使得其它值集中分布在较小范围内，差异变得不明显。

##### 示例1：最小-最大规范化( Min-max normalization）

```
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

# 生成模拟数据
np.random.seed(0)
data = pd.DataFrame({
    'Feature1': np.random.normal(loc=0, scale=1, size=100),
    'Feature2': np.random.normal(loc=10, scale=2, size=100),
    'Feature3': np.random.normal(loc=5, scale=1.5, size=100)
})

# 显示原始数据的前几行
print("Original data head:")
print(data.head())

# 初始化最小-最大规范化器
scaler = MinMaxScaler(feature_range=(0, 1))

# 规范化数据
normalized_data = scaler.fit_transform(data)

# 将规范化后的数据转换为DataFrame
normalized_df = pd.DataFrame(normalized_data, columns=data.columns)

# 显示规范化后数据的前几行
print("\nNormalized data head:")
print(normalized_df.head())

# 绘制原始数据和规范化后数据的直方图进行比较
plt.figure(figsize=(12, 8))

# 绘制原始数据的直方图
plt.subplot(2, 3, 1)
plt.hist(data['Feature1'], bins=20, color='blue', alpha=0.7)
plt.title('Original Feature 1')
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.subplot(2, 3, 2)
plt.hist(data['Feature2'], bins=20, color='green', alpha=0.7)
plt.title('Original Feature 2')
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.subplot(2, 3, 3)
plt.hist(data['Feature3'], bins=20, color='red', alpha=0.7)
plt.title('Original Feature 3')
plt.xlabel('Value')
plt.ylabel('Frequency')

# 绘制规范化后数据的直方图
plt.subplot(2, 3, 4)
plt.hist(normalized_df['Feature1'], bins=20, color='blue', alpha=0.7)
plt.title('Normalized Feature 1')
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.subplot(2, 3, 5)
plt.hist(normalized_df['Feature2'], bins=20, color='green', alpha=0.7)
plt.title('Normalized Feature 2')
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.subplot(2, 3, 6)
plt.hist(normalized_df['Feature3'], bins=20, color='red', alpha=0.7)
plt.title('Normalized Feature 3')
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()
```

![image-20250117134612959](assets\image-20250117134612959.png)

##### 示例2：Z分数规范化 (Z-score normalization )

```
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# 生成模拟数据
np.random.seed(0)
data = pd.DataFrame({
    'Feature1': np.random.normal(loc=0, scale=1, size=100),
    'Feature2': np.random.normal(loc=10, scale=2, size=100),
    'Feature3': np.random.normal(loc=5, scale=1.5, size=100)
})

# 显示原始数据的前几行
print("Original data head:")
print(data.head())

# 初始化Z分数规范化器
scaler = StandardScaler()

# 规范化数据
normalized_data = scaler.fit_transform(data)

# 将规范化后的数据转换为DataFrame
normalized_df = pd.DataFrame(normalized_data, columns=data.columns)

# 显示规范化后数据的前几行
print("\nNormalized data head:")
print(normalized_df.head())

# 绘制原始数据和规范化后数据的直方图进行比较
plt.figure(figsize=(12, 8))

# 绘制原始数据的直方图
plt.subplot(2, 3, 1)
plt.hist(data['Feature1'], bins=20, color='blue', alpha=0.7)
plt.title('Original Feature 1')
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.subplot(2, 3, 2)
plt.hist(data['Feature2'], bins=20, color='green', alpha=0.7)
plt.title('Original Feature 2')
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.subplot(2, 3, 3)
plt.hist(data['Feature3'], bins=20, color='red', alpha=0.7)
plt.title('Original Feature 3')
plt.xlabel('Value')
plt.ylabel('Frequency')

# 绘制规范化后数据的直方图
plt.subplot(2, 3, 4)
plt.hist(normalized_df['Feature1'], bins=20, color='blue', alpha=0.7)
plt.title('Normalized Feature 1')
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.subplot(2, 3, 5)
plt.hist(normalized_df['Feature2'], bins=20, color='green', alpha=0.7)
plt.title('Normalized Feature 2')
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.subplot(2, 3, 6)
plt.hist(normalized_df['Feature3'], bins=20, color='red', alpha=0.7)
plt.title('Normalized Feature 3')
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()
```

![image-20250117134918017](assets\image-20250117134918017.png)

##### 示例3：小数定标规范化(Decimal scaling normalization )

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 生成模拟数据
np.random.seed(0)
data = pd.DataFrame({
    'Feature1': np.random.normal(loc=0, scale=1, size=100),
    'Feature2': np.random.normal(loc=10, scale=2, size=100),
    'Feature3': np.random.normal(loc=5, scale=1.5, size=100)
})

# 显示原始数据的前几行
print("Original data head:")
print(data.head())

# 定义小数定标规范化函数
def decimal_scaling(data):
    max_val = np.max(np.abs(data))
    exponent = np.ceil(np.log10(max_val))
    norm_data = data / (10 ** exponent)
    return norm_data

# 对每个特征进行小数定标规范化
normalized_data = data.apply(decimal_scaling)

# 将规范化后的数据转换为DataFrame
normalized_df = pd.DataFrame(normalized_data, columns=data.columns)

# 显示规范化后数据的前几行
print("\nNormalized data head:")
print(normalized_df.head())

# 绘制原始数据和规范化后数据的直方图进行比较
plt.figure(figsize=(12, 8))

# 绘制原始数据的直方图
plt.subplot(2, 3, 1)
plt.hist(data['Feature1'], bins=20, color='blue', alpha=0.7)
plt.title('Original Feature 1')
plt.xlabel('Value')  # 标注横坐标
plt.ylabel('Frequency')  # 标注纵坐标

plt.subplot(2, 3, 2)
plt.hist(data['Feature2'], bins=20, color='green', alpha=0.7)
plt.title('Original Feature 2')
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.subplot(2, 3, 3)
plt.hist(data['Feature3'], bins=20, color='red', alpha=0.7)
plt.title('Original Feature 3')
plt.xlabel('Value')
plt.ylabel('Frequency')

# 绘制规范化后数据的直方图
plt.subplot(2, 3, 4)
plt.hist(normalized_df['Feature1'], bins=20, color='blue', alpha=0.7)
plt.title('Normalized Feature 1')
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.subplot(2, 3, 5)
plt.hist(normalized_df['Feature2'], bins=20, color='green', alpha=0.7)
plt.title('Normalized Feature 2')
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.subplot(2, 3, 6)
plt.hist(normalized_df['Feature3'], bins=20, color='red', alpha=0.7)
plt.title('Normalized Feature 3')
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()
```

![image-20250117135340667](assets\image-20250117135340667.png)

##### 示例4：Robust Scaler

当数据中存在异常值时，Z-Score标准化可能会受到影响。在这种情况下，我们可以使用更加稳健的标准化方法，如Robust Scaler。

Robust Scaler使用中位数和四分位距来代替均值和标准差，其公式如下：
$$
X scaled = (X - median(X)) / IQR(X)
$$

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, RobustScaler

#创建包含异常值的示例数据
np.random.seed(42)
data = np.random.randn(100,1)
data = np.concatenate([data,np.array([[20],[-20]])])#添加异常值
df = pd.DataFrame(data, columns=['Feature'])

#应用Z-Score标准化和Robust scaler
standard_scaler =StandardScaler()
robust_scaler = RobustScaler()

df_standardized = pd.DataFrame(standard_scaler.fit_transform(df), columns=df.columns)
df_robust = pd.DataFrame(robust_scaler.fit_transform(df), columns=df.columns)

#可视化比较
fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))

ax1.boxplot(df)
ax1.set_title('Original Data')
ax1.set_ylabel('Value')

ax2.boxplot(df_standardized)
ax2.set_title('Z-Score standardization')
ax2.set_ylabel('Value')
ax3.boxplot(df_robust)
ax3.set_title('Robust scaler')
ax3.set_ylabel('Value')
plt.tight_layout()
plt.show()
#打印统计信息
print("Original Data statistics:")
print(df.describe())
print("\nZ-Score Standardized Data statistics:")
print(df_standardized.describe())
print("\nRobust Scaled Data statistics:")
print(df_robust.describe())
```

![image-20250117142406893](assets\image-20250117142406893.png)

这个代码示例展示了Z-Score标准化和Robust Scaler在处理含有异常值的数据时的表现差异。通过箱线图，我们可以直观地看到Robust Scaler对异常值的抵抗能力更强。

##### 示例5：分位数变换和Box-Cox变换

分位数变换是一种非参数化的变换方法，它将特征映射到均匀分布或正态分布。Box-Cox变换则是一种参数化的方法，用于将非正态分布的数据转换为近似正态分布。

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import QuantileTransformer, PowerTransformer

#创建偏斜分布的示例数据
np.random.seed(42)
data =np.random.lognormal(0,1,1000).reshape(-1,1)
df = pd.DataFrame(data, columns=['Feature'])

#应用分位数变换和Box-Cox变换
quantile_transformer = QuantileTransformer(output_distribution='normal')
power_transformer = PowerTransformer(method='box-cox')
df_quantile = pd.DataFrame(quantile_transformer.fit_transform(df), columns=df.columns)
df_boxcox = pd.DataFrame(power_transformer.fit_transform(df), columns=df.columns)

# 可视化比较
fig,(ax1,ax2,ax3)= plt.subplots(1,3,figsize=(15,5))
ax1.hist(df, bins=50)
ax1.set_title('0riginal Data')
ax1.set_xlabel('Value')
ax1.set_ylabel('Frequency')
ax2.hist(df_quantile, bins=50)
ax2.set_title('Quantile Transform')
ax2.set_xlabel('Value')
ax2.set_ylabel('Frequency')
ax3.hist(df_boxcox, bins=50)
ax3.set_title('Box-Cox Transform')
ax3.set_xlabel('Value')
ax3.set_ylabel('Frequency')
plt.tight_layout()
plt.show()

#打印统计信息
print("0riginal Data statistics:")
print(df.describe())
print("\nQuantile Transformed Data statistics:")
print(df_quantile.describe())
print("\nBox-Cox Transformed Data statistics:")
print(df_boxcox.describe())
```

![image-20250117143806317](assets\image-20250117143806317.png)

这个代码示例展示了如何使用分位数变换和Box-Cox变换来处理偏斜分布的数据。通过直方图，我们可以直观地看到这些变换如何将原始的偏斜分布转换为更接近正态分布的形式。

#### 2.1.5.2 连续数据离散化

数据离散化是将连续变量划分成有限数量的离散区间的数据预处理技术，有助于简化数据分析和建模过程、降低计算复杂度、削弱异常值影响等。

常用数据离散化方法包括分箱离散化，聚类分析离散化、决策树离散化。其中，分箱就是一种非监督的离散化技术，首先通过指定箱的深度或宽度完成数据分箱，然后利用箱均值或箱中位数等方法对箱中的数值进行平渭处理，实现属性值的离散化。聚类分析通过将属性的观测值划分为，实现对数据的离散化。决策树分析是一种监督的高化方法、其核心思想是基于数据集的特征值选择最优划分点，将连续的数值期器据转化为离散的类别型数据。

##### 示例：自定义分箱

自定义分箱是一种将连续数据分割成具有特定边界的离散区间的方法。这种方法允许我们根据数据的分布和分析需求来定义分箱的边界。

```
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 设置随机种子以便结果可复现
np.random.seed(0)

# 生成模拟的连续数据
data = np.random.normal(loc=0, scale=1, size=1000)

# 将连续数据转换为pandas Series
data_series = pd.Series(data)

# 定义自定义分箱边界
# 这些边界将数据分为四个区间：小于-1，-1到0之间，0到1之间，大于1
bins = [-np.inf, -1, 0, 1, np.inf]
labels = ['< -1', '-1 to 0', '0 to 1', '> 1']

# 使用pandas的cut函数进行自定义分箱
discretized_series = pd.cut(data_series, bins=bins, labels=labels, right=False)

# 显示离散化后的数据的前几行
print("Discretized data head:")
print(discretized_series.head())

# 绘制原始数据和离散化后数据的直方图进行比较
plt.figure(figsize=(12, 6))

# 绘制原始数据的直方图
plt.subplot(1, 2, 1)
plt.hist(data, bins=30, color='blue', alpha=0.7)
plt.title('Original Continuous Data')
plt.xlabel('Value')
plt.ylabel('Frequency')

# 绘制离散化后数据的直方图
plt.subplot(1, 2, 2)
discretized_counts = discretized_series.value_counts(sort=False)
plt.bar(discretized_counts.index.astype(str), discretized_counts, color='green', alpha=0.7)
plt.title('Discretized Data (Custom Bins)')
plt.xlabel('Bin')
plt.ylabel('Frequency')
plt.xticks(rotation=45)

plt.tight_layout()
plt.show()
```

![image-20250117180357895](assets\image-20250117180357895.png)

#### 2.1.5.3 分类数据编码

在数据挖掘中，分类变量是常见的一种数据类型。然而，许多机器学习算法和统计分析方法都只能处理数值变量。因此需要将分类变量转换为数值变量，见的方法包括序数编码、独热编码、二进制编码等。表2-8展示了部分常用的分类数据编码方法并对其简要描述。
表2-8 常用的分类数据编码方法

![image-20241222100528791](assets\image-20241222100528791.png)

##### 示例1：独热编码

独热编码为每个类别创建一个新的二进制特征。这种方法避免了引入顺序关系，但可能会导致特征数量急剧增加。

```
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt

# 示例数据
data = {
    'Color': ['Red', 'Green', 'Blue', 'Green', 'Red', 'Blue', 'Red'],
    'Size': ['Small', 'Medium', 'Large', 'Small', 'Large', 'Medium', 'Small'],
    'Price': [10, 15, 20, 12, 18, 17, 11]
}
df = pd.DataFrame(data)

# 应用独热编码
ohe = OneHotEncoder()
encoded_features = ohe.fit_transform(df[['Color', 'Size']])
feature_names = ohe.get_feature_names_out()  # 使用 get_feature_names_out 方法

df_encoded = pd.DataFrame(encoded_features.toarray(), columns=feature_names)
df_encoded['Price'] = df['Price']

print("独热编码后的数据:")
print(df_encoded)

# 可视化编码结果
plt.figure(figsize=(12, 6))
for i, feature in enumerate(feature_names):
    plt.subplot(2, 3, i+1)
    for label in df_encoded[feature].unique():
        plt.scatter(df_encoded.loc[df_encoded[feature] == label, feature], 
                     df_encoded.loc[df_encoded[feature] == label, 'Price'], 
                     label=label)
    plt.xlabel(feature)
    plt.ylabel('Price')
    plt.title(f'{feature} vs Price')
    plt.legend()

plt.tight_layout()
plt.show()

# 比较独热编码和标签编码的特征数量
print("\n特征数量比较:")
original_features = len(df.columns) - 1  # 减去Price列
print(f"原始特征数量: {original_features}")

# 假设标签编码，每个类别特征转换为一个数值特征
label_encoded_features = original_features + len(df['Color'].unique()) + len(df['Size'].unique())
print(f"标签编码后的特征数量: {label_encoded_features}")

# 独热编码后的特征数量
onehot_encoded_features = len(df_encoded.columns) - 1  # 减去Price列
print(f"独热编码后的特征数量: {onehot_encoded_features}")
```

![image-20250117183812208](assets\image-20250117183812208.png)

![image-20250117183746374](assets\image-20250117183746374.png)

这个代码示例展示了如何使用OneHotEncoder对类别型数据进行独热编码。它还通过可视化展示了编码后的每个特征与目标变量（价格）的关系，并比较了独热编码和标签编码在特征数量上的差异。

##### 示例2：目标编码和频率编码

目标编码和频率编码是处理类别型特征的高级方法。目标编码用目标变量的平均值来替换类别，而频数编码则用类别的出现频率来替换类别。

```
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# 创建包含类别型特征的示例数据
np.random.seed(42)
data = {
    'Category': np.random.choice(['A', 'B', 'C', 'D'], size=1000),
    'Target': np.random.randint(0, 2, size=1000)
}
df = pd.DataFrame(data)

# 分割数据集
train, test = train_test_split(df, test_size=0.2, random_state=42)

# 目标编码
target_means = train.groupby('Category')['Target'].mean()
train['Target_Encoding'] = train['Category'].map(target_means)
test['Target_Encoding'] = test['Category'].map(target_means)

# 频率编码
frequency = train['Category'].value_counts(normalize=True)
train['Frequency_Encoding'] = train['Category'].map(frequency)
test['Frequency_Encoding'] = test['Category'].map(frequency)

# 打印结果
print("Target Encoding:")
print(train[['Category', 'Target', 'Target_Encoding']].head(10))
print("\nFrequency Encoding:")
print(train[['Category', 'Frequency_Encoding']].head(10))

# 可视化编码结果
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
train.groupby('Category')['Target_Encoding'].mean().plot(kind='bar', ax=ax1)
ax1.set_title('Target Encoding')
ax1.set_ylabel('Encoded Value')


train.groupby('Category')['Frequency_Encoding'].mean().plot(kind='bar', ax=ax2)
ax2.set_title('Frequency Encoding')
ax2.set_ylabel('Encoded Value')


plt.tight_layout()
plt.show()

# 评估编码效果
from sklearn.metrics import mutual_info_score

def calculate_mi(x, y):
    return mutual_info_score(x, y)

print("\nMutual Information with Target:")
# 将类别特征转换为数字编码
category_to_num = {category: i for i, category in enumerate(train['Category'].unique())}
original_category = train['Category'].map(category_to_num)
print(f"Original category: {calculate_mi(original_category, train['Target']):.4f}")
print(f"Target Encoding: {calculate_mi(train['Target_Encoding'], train['Target']):.4f}")
print(f"Frequency Encoding: {calculate_mi(train['Frequency_Encoding'], train['Target']):.4f}")
```

![image-20250117205459878](assets\image-20250117205459878.png)

![image-20250117205512387](assets\image-20250117205512387.png)

### 2.1.6 实践案例:线上零售交易数据预处理

数字化经济时代，线上零售已经成为了现代商业领域中不可忽视的重要组成部分，越来越多的消费者选择通过网络购物，产生了海量的线上零售交易数据。这些数据包含了关于商品、消费者行为、购买习惯以及市场趋势等宝贵信息。然而，这些数据复杂多变，通常包含噪音、缺失值等。因此，为了能够更好地利用这些数据来指导业务决策和市场战略，需要对其进行有效的预处理。

本节将对线上零售交易数据进行预处理，该数据集由2021月12月日至2023年12月9日期间发生的英国注册非商店在线零售的所有交易订单组成(数据集来源: https://www.kaggle.com/datasets/puneetbhaya/online-retail)，共包含541909个样本，部分数据样例见表2-9，具体字段包括:发票编号、商品代码、商品描述、数量、发票日期、单价、客户ID、国家(交易发生的国家)。

![image-20241222101321695](assets\image-20241222101321695.png)

数据预处理。使用isnull 函数查看各字段数据缺失的情况。结果显示,“商品描述”和“客户ID”分别存在1454和135080条数据缺失，由于“商品描述”字段的缺失值较少，占比约0.27%，可予以删除处理。表2-10介绍了Python中常用的数据预处理函数并对其进行简要描述。

![image-20241222101358170](assets\image-20241222101358170.png)

表2-10常用的数据预处理函数

![image-20241222101506215](assets\image-20241222101506215.png)

![image-20241222101643632](assets\image-20241222101643632.png)

查看数据分布情况。通过describe 数查看数值型数据的分布情况。从输出结果中可以观察到，“数量”和“单价”字段的最小值存在负数的情况，这显然有悖于事实，因此需要筛选出数量大于等于1，单价大于0的数据进行后续分析。

![image-20241222101726403](assets\image-20241222101726403.png)

![image-20241222161321338](assets\image-20241222161321338.png)

索引过滤。使用 data_processed函数对数量大于等于1，单价大于0的数据进行索引和筛选。使用方法如下:

![image-20241222102014064](assets\image-20241222102014064.png)

![image-20241222161405541](assets\image-20241222161405541.png)

分层抽样。“客户ID”缺失数据较多，不宜直接删除，可根据后续的具体挖掘任务进行合适的处理。为了节省模型训练时间，可对数据进行分层抽样，根据“国家”字段从全部数据中抽取10000条数据。在这个过程中，每个国家的数据按比例被抽取，以确保抽样后的样本仍能保持总体数据集中不同国家的分布特征本例中使用 sklearn 库中的 StratifedShufleSplit 类，定义一个分层抽样函数。这个函数将接受数据集、目标特征和所需的样本大小等参数，并返回一个包含分层抽样后样本的数据集。

![image-20241222102106975](assets\image-20241222102106975.png)

连续数据离散化。“国家”字段是离散型数据，使用哑编码的方式将其转为数值特征。pandas库中 get_dummies()方法可直接对离散特征进行哑编码处理。

![image-20241222102515419](assets\image-20241222102515419.png)

![image-20241222161721217](assets\image-20241222161721217.png)

在本例中，主要使用pandas和sklearn库对数据进行了缺失值处理、异常值处理、分层抽样、哑编码，读者可以尝试使用其他的数据预处理方法对本章内容加以实践。

在 Python中进行数据预处理时，除了前面已经介绍的Pandas和Skleam库，还涉及到许多其他常用的库和技术，下面进行简要介绍:

SciPy:一个用于数学、科学和工程的库，它建立在 NumPy 之上，并提供了更多的科学计算工具。在数据预处理中，Scipy的一些模块可以用于处理图像、信号处理、统计检验等任务。

Statsmodels:一个用于估计统计模型和进行统计测试的库。在数据预处理阶段，它可以用于探索数据的分布和特征之间的相关性，以及进行一些假设检验。

Scrapy:一个强大的网络爬虫框架，主要应用于从网页抓取的数据进行预处理。

Regex(re 模块): Pyhon 内置的 re 模块可以使用正则表达式实现字符串匹配、查找和替换，常用于数据清洗和提取。

## 2.2 数据属性

在数据挖掘中，结构化数据是一种最常见的数据形式，通常以表格或矩阵的方式展现，其中每行代表一个数据对象，也称为样本或实例，每列代表一个属性(Attribute)，用以描述数据对象的特定特征。

根据性质和可测量程度，属性可分为标称属性、序数属性和数值属性。另外基于属性值的类型和范围，它们也可以分为离散属性和连续属性。表2-11展示了某平台二手车交易的数据，包括交易ID、汽车注册年份、车身类型、燃油类型等属性信息。

![image-20241222102923446](assets\image-20241222102923446.png)

### 2.2.1 标称属性

标称属性(Nominal attribute) 是由离散的符号或名称表示的属性，代表不同的类别、状态或编码。

标称属性的取值是枚举型的，即它们是一个预定义集合中的元素，而不是连续或可度量的数值。例如，表2-11中“车身类型”就是一个标称属性，其取值可能是“微型车”、“厢型车”、“豪华轿车”等。“燃油类型”也是一个标称属性,其取值可能是“汽油”、“柴油”、“液化石油气”等。这些取值通常不具有数学意义，不能进行大小比较。

二元属性(Binary attribute) 是只有两种可能取值的属性，是标称属性的特例。

在统计学和数据分析中，二元属性常常用来表示两种互斥的状态或类别，比如是/否、成功/失败、有/无等。二元属性可进一步分为对称的二元属性和不对称的二元属性。如果属性的两种状态结果同等重要，则为对称的二元属性，例如，表 2-11中“车身类型”属性，取值可能是“自动”和“手动”，以及生活中常见的“性别”属性，该属性一般具有“男”和“女”两种状态;若两种结果的重要性不同，则为非对称的二元属性，例如，疾病诊断中的“阴性”和“阳性”，舆情监测中信息是“虚假”或“真实”，在这类任务中,“阳性”和“虚假”是稀有但更加受关注的结果。

数据不平衡性:在对具有非对称二元属性的数据集进行挖据时，数据集中两个类别的样本数量可能存在严重的不均衡问题。当其中一个类别的样本数量远远少于另一个类别时，称为数据不平衡。这种不平衡性会对数据挖掘算法的性能产生负面影响。举例来说，当使用深度学习算法解决分类问题时，训练集中不平衡类别的样本数量过少，导致模型难以学习到该类别数据的特征，从而影响模型对该类别数据的识别能力。因此，在挖掘具有非对称二元属性的数据集时，需要对数据进行预处理，如重采样、使用不同的成本敏感权重、合成少数类过采样技术等，以降低数据类别失衡对结果的影响。

### 2.2.2 序数属性

序数属性(Ordinal atribute) 是一种用来描述对象在一组事物中相对排列顺序的属性。

序数属性的取值存在有意义的秩，可以进行大小和顺序的比较，但不能进行加减乘除等数学计算。该属性取值的大小能够表示数据对象的等级、方位、大小等顺序关系。例如，学生的“考试名次”就是一个序数属性，表示学生在这组生中的排名。服装的“尺码”也是一个序数属性，用S、M、L、XL、XXL 等R码表示服装的尺寸大小。
秩(Rank):在统计学中,“秩”是指将一组数据按照从小到大(或从大到小)的顺序排列后，每个数据所在的位置或排名，用于表示每个数据在整个数据集中的相对位置。例如,数据集[5,3,8,2,7]按照从小到大排列后为[2,3,5,7,8]，则每个数据的秩分别为:[3,2.51,4]。

### 2.2.3 数值属性

数值属性(Numeric attribute)用于表示具有数值含义的属性，其取值是一个数字。

数值属性是可度量的量，既可以进行大小比较，也可以进行数学计算。例如表 2-11 中“发动机功率”、“汽车已行驶公里数”、“交易价格”均为数值属性，它们的取值通常用数值表示。数值属性可进一步分为比率标度属性和区间标度属性。

比率标度属性(Ratio-scaled attribute)是存在绝对零点的数值属性。

在比率标度属性中，属性的取值可进行比例运算，如果某物体的长度是20cm，宽度是10cm，则可认为该物体的长度是宽度的两倍。此外，比率标度属性的不同数值之间具有相等的间隔，例如，在速度属性中，1m/s和2m/s的差距与100m/s和101m/s的差距是相等的。表2-11中“发动机功率”、“汽车已行驶公里数”、“交易价格”均为比率标度的属性。比率标度属性的例子还包括重量、开尔文温度、年龄、货币量等。

绝对零点:是指一个具体的零值，表示完全没有某种属性或数量。绝对零点表示了属性的最小可能值，存在绝对零点的数据其取值不存在负值。

区间标度属性(Interval-scaled attribute)是不存在绝对零点的数值属性。

区间标度属性没有固有零点，例如摄氏温度(），0并不表示完全没有温度，而是表示冰点。区间标度属性只能进行大小比较，不能进行比率比较。例如30和20的温差是10，可以认为前者比后者温度高，但不能认为前者是后者的1.5 倍热。同样的，华氏温度()也是区间标度属性，但开尔文温度(K)是比率标度属性。区间标度属性的例子还包括日期、经纬度、成绩评分等。

开尔文温度与摄氏、华氏温度:开尔文温度(K)是区间标度属性，而摄氏温度()和华氏温度( )是比率标度属性。从温度测量尺度的原理理解，开尔文温度基于热力学温标，通过绝对温度零点的概念进行定义和测量。绝对零度是温度的最低可能值，表示物质中粒子的运动完全停止，即没有任何热量的状态。摄氏温度和华氏温度是以水的冰点(0)和沸点(100 )为基准的温度尺度。其零点不代表绝对零度，因为温度的负值在摄氏温度和华氏温度的尺度上是有意义的。三者之间的转换公式为K=℃+273.15=(F+459.67)/1.8.

### 2.2.4 离散属性与连续属性

根据性质和取值方式，数据的属性又可分为离散属性和连续属性。在数据挖掘中，将数据正确地标识为离散属性或连续属性非常重要，这直接影响到数据处理和分析的方法选择以及结果的解释和可靠性。

离散属性(Categorical attribute)是指数据取值个数有限的属性可定性地将每个观察单位分配给特定类别。例如，一个人的“发色”就是一个离散属性，其取值可能是“黑色”、“金色”、“白色”等;一个地区的“邮政编码”也是一个离散属性，其取值可能是“100010”，“410073”等。

连续属性(Continuous attribute)是具有无限个可能取值的属性这些取值通常表示一定范围内的测量结果或数量，可无限细分。连续属性的取值通常用浮点变量来表示，例如，人的身高数据是连续的，其取值是无限的(1.7米、1.71米、1.72 米、1.7234 米等)。

![image-20241222104648277](assets\image-20241222104648277.png)

## 2.3数据描述性统计

描述性统计是最基本的数据探索方式，它应用基本统计分析方法对数据进行分析、旨在了解数据分布的基本特性，主要包括集中趋势度量、离散程度度量分布形态度量。

### 2.3.1集中趋势度量

集中趋势度量是寻找数据水平的代表值或中心值的计算方法，常用于表征一组数据向某一中心值靠拢的程度。常见的集中趋势度量主要包括平均值、中位数、众数。

##### 示例：画平均数、中位数、众数图

```
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats


# 示例数据
data = np.array([50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 60, 60, 60, 65, 65, 70, 70, 70, 70, 75, 75, 75, 75, 75])

# 计算平均数、中位数、众数
mean = np.mean(data)
median = np.median(data)
mode = stats.mode(data).mode  # 直接获取众数

# 绘制直方图
plt.hist(data, bins=10, alpha=0.7, color='skyblue', edgecolor='black')

# 标记平均数、中位数、众数
plt.axvline(mean, color='red', linestyle='dashed', linewidth=2, label='Mean')
plt.axvline(median, color='green', linestyle='dashed', linewidth=2, label='Median')
plt.axvline(mode, color='blue', linestyle='dashed', linewidth=2, label='Mode')

# 添加标题和标签
plt.title('Mean, Median, and Mode', fontsize=16, fontweight='bold')
plt.xlabel('Value', fontsize=14)
plt.ylabel('Frequency', fontsize=14)

# 添加图例
plt.legend()

# 设置网格线
plt.grid(axis='y', linestyle='--', alpha=0.7)

# 显示图形
plt.show()
```

![image-20250117234226595](assets\image-20250117234226595.png)

### 2.3.2离散程度度量

在衡量数据的特性时，平均值等集中趋势度量指标能反映数据整体的中心位置或一般水平，但它们往往无法全面揭示数据的复杂性。仅仅依赖于平均值，我们可能会忽略数据点的离散情况，即数据点围绕平均值的波动或偏离程度。

![image-20241222144717279](assets\image-20241222144717279.png)

离散程度为掌握数据特征提供了另一个角度，常见的离散程度度量统计量有极差、平均差、方差、标准差、异众比率、四分位差等。

#### 2.3.2.1 极差和平均

极差(Range)又称全距，是数据中最大值与最小值的差值，计算公式为:

![image-20241222144800952](assets\image-20241222144800952.png)

极差反映了数据的取值范围，是最基本的衡量数据离散程度的指标。极差可直接反映数据分布情况，易受极端值的影响且不考虑数据的分布。

平均差(Mean deviation)是一个数据中每个变量值与平均值之差的绝对值的平均值，计算公式为:

![image-20241222144848496](assets\image-20241222144848496.png)

平均差以平均值为中心，能全面地反映一组数据的离散程度。平均差越大，说明数据离散程度越大，反之，离散程度越小。

#### 2.3.2.2 方差和标准差

方差(Variance)是数据中各变量值与平均值离差平方的算术平均值，计算公式为：

![image-20241222145101508](assets\image-20241222145101508.png)

其中，μ是总体均值。

方差在实际应用中使用最广，能较好地反映各变量值与平均值的平均差异，从而全面体现一组数据的离散程度。在统计研究中，有时会对数据进行分组，依据特定的标准将原始数据划分为不同的区间(组)，每个组包含一定范围内的数据值，并统计每个组内的频数或频率fi。分组数据的方差计算公式为:

<img src="assets\image-20241222145152390.png" alt="image-20241222145152390" style="zoom:33%;" />

其中，M,是第i组的中位数，fi是其频数，k为组数。

对于数据量为n的样本数据，通常使用样本方差来估计总体方差，未分组数据和分组数据的样本方差计算公式分别为：

<img src="assets\image-20241222145246072.png" alt="image-20241222145246072" style="zoom:33%;" />

![image-20241222145304063](assets\image-20241222145304063.png)

![image-20241222145516958](assets\image-20241222145516958.png)

标准差(stand devianion)是方差的算术平方根。未分组数据的样本标准差、总体标准差分别用s和σ表示。

#### 2.3.2.3异众比率

异众比率(Variation ratio)是指非众数变量值的频数占总频数的比例。

#### 2.3.2.4四分位差

四分位数(Quartile)也称四分位点，是指把所有数据按照递增顺序排列并将其分成四等份后，处于三个分割点位置的变量值。

定义2-30四分位差(Interquartile range,IQR)是上四分位数与下四分位数之差。

![image-20241222150023726](assets\image-20241222150023726.png)

#### 2.3.2.5变异系数

变异系数(Coefficient of variation)是一组数据的标准差与平均值的比值，通常用百分比表示。

变异系数用于衡量数据的相对变异程度，可比较不同数据集之间的变异性且不受其测量单位的影响。变异系数越大说明数据的离散程度越大，变异系数越小说明数据的离散程度越小。

#### 2.3.2.6示例

```
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# 示例数据
data = np.array([10, 20, 30, 40, 50, 30, 30, 30, 60, 70, 80, 90, 100])

# 计算统计量
mean_value = np.mean(data)
variance = np.var(data)
std_dev = np.std(data)
range_value = np.ptp(data)
mode_result = stats.mode(data)
mode = mode_result.mode  # 直接获取众数
ratio = 1 - (np.count_nonzero(data == mode) / len(data))
q1 = np.percentile(data, 25)
q3 = np.percentile(data, 75)
iqr_value = q3 - q1

# 绘制直方图
plt.figure(figsize=(12, 8))
plt.hist(data, bins=10, alpha=0.7, color='skyblue', edgecolor='black', label='Data Distribution')

# 标记平均数
plt.axvline(mean_value, color='red', linestyle='dashed', linewidth=2, label=f'Mean: {mean_value:.2f}')

# 标记方差和标准差
plt.axvline(mean_value - std_dev, color='orange', linestyle='dashed', linewidth=2, label=f'Std Dev: {std_dev:.2f}')
plt.axvline(mean_value + std_dev, color='orange', linestyle='dashed', linewidth=2)

# 标记众数
plt.axvline(mode, color='green', linestyle='dashed', linewidth=2, label=f'Mode: {mode}')

# 标记四分位差
plt.axvline(q1, color='purple', linestyle='dashed', linewidth=2, label=f'Q1: {q1:.2f}')
plt.axvline(q3, color='purple', linestyle='dashed', linewidth=2, label=f'Q3: {q3:.2f}')
plt.axhspan(0, plt.gca().get_ylim()[1], q1, q3, color='yellow', alpha=0.3, label=f'IQR: {iqr_value:.2f}')

# 添加标题和标签
plt.title('Statistical Measures', fontsize=16, fontweight='bold')
plt.xlabel('Value', fontsize=14)
plt.ylabel('Frequency', fontsize=14)

# 添加图例
plt.legend()

# 设置网格线
plt.grid(axis='y', linestyle='--', alpha=0.7)

# 显示图形
plt.show()

# 打印异众比率
print(f"异众比率为: {ratio:.2f}")
```

![image-20250117234819536](assets\image-20250117234819536.png)

异众比率为: 0.69

### 2.3.3分布形态度量

数据的分布形态度量主要包括对称性、偏斜与扁平程度等。

#### 2.3.3.1 偏态及其测度

偏态(Skewness)是对一组数据分布相较于标准正态分布的斜方向和程度的度量，计算公式为:

![image-20241222152023394](assets\image-20241222152023394.png)

其中，s为样本的标准差。偏态反映数据分布的非对称程度，偏态系数是衡量偏态的统计量。

偏态系数越接近于0，表明数据集的偏斜程度越小。如图2-9所示，若SK=0则表明数据呈现对称分布。若SK不等于0，则表明数据呈现偏态分布。根据偏移的方向，偏态分布可进一步分为如下两种:

1)右偏分布(Positive skewness):SK>0，众数<中位数<平均值。右偏分布的数据中有一些较大的极端值，使得平均值被拉向右侧。

2)左偏分布(Negative skewness):SK<0，平均值<中位数<众数。左偏分布的数据中有一些较小的极端值，使得平均值被拉向左侧。

偏度的绝对值越大，说明数据分布的偏斜程度越严重。若|SK|>1，则该数据为高度偏态分布;若0.5≤|SK|≤1，则为中等偏态分布。

![image-20241222152138514](assets\image-20241222152138514.png)

#### 2.3.3.2峰态及其测度

峰态(Kurtosis)是对数据分布相较于标准正态分布的平峰或尖峰程度的度量。可以用峰态系数K来衡量峰态，计算公式为:

![image-20241222152312602](assets\image-20241222152312602.png)

峰态是相对于标准正态分布而言的，如果一组数据服从标准正态分布，则K=0:若K>0，则为尖峰分布，表示数据更为集中:若K<0，则为扁平分布数据更为离散。

![image-20241222152333114](assets\image-20241222152333114.png)

本节主要介绍了数据分布特征的各种统计量，如表2-13所示，这些统计量从集中趋势、离散程度、分布形态三个方面帮助我们更好地理解和描述数据。在数据挖掘中，描述统计量是一组常见且重要的指标，用于对数据进行初步的整理、概括和分析，为后续的建模分析提供基础。

![image-20241222152356523](assets\image-20241222152356523.png)

![image-20241222152434164](assets\image-20241222152434164.png)

#### 2.3.3.3示例：偏态、峰态

```
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

# 生成数据
np.random.seed(0)
data = np.random.normal(0, 1, 1000)  # 生成1000个服从正态分布的数据

# 计算偏度和峰度
skewness = stats.skew(data)
kurtosis = stats.kurtosis(data)

# 计算众数、平均数、中位数
mode = stats.mode(data)[0][0]
mean = np.mean(data)
median = np.median(data)

# 打印结果
print("偏度（Skewness）:", skewness)
print("峰度（Kurtosis）:", kurtosis)
print("众数（Mode）:", mode)
print("平均数（Mean）:", mean)
print("中位数（Median）:", median)

# 绘制数据的直方图
plt.hist(data, bins=30, density=True, alpha=0.6, color='g')

# 绘制正态分布曲线
xmin, xmax = plt.xlim()
x = np.linspace(xmin, xmax, 100)
p = stats.norm.pdf(x, np.mean(data), np.std(data))
plt.plot(x, p, 'k', linewidth=2)

# 标注众数、平均数、中位数
plt.axvline(mode, color='r', linestyle='dashed', linewidth=2, label='Mode')
plt.axvline(mean, color='b', linestyle='dashed', linewidth=2, label='Mean')
plt.axvline(median, color='y', linestyle='dashed', linewidth=2, label='Median')

# 添加横纵坐标标注（英文）
plt.xlabel('Data Value')
plt.ylabel('Probability Density')

plt.legend()
plt.show()
```

![image-20250125170825802](assets\image-20250125170825802.png)

### 2.3.4描述性统计常用工具

在 python中，NumPy、Pandas和 SciPy.slats 等库为数据的描述性统计分析提供了极大的便利，它们包含丰富的函数和方法，可以轻松地进行各种计算。

![image-20241222152706689](assets\image-20241222152706689.png)

### 2.3.5实践案例：化妆品销售数据描述性统计分析

数据来自Kaggle 数据集 Sephora Skincare Ingredients(丝芙兰护肤成分)。该数据包含1472条化妆品销售数据，具体字段包括:类型、价格、等级、混合性、干性正常、油性、敏感性(后五列表示产品是否适合此类皮肤)。

![image-20241222153155492](assets\image-20241222153155492.png)

主要使用NumPy库和Pandas库进行描述性统计，具体操作如下:

1.数据分组。本例中使用Pandas库的groupby函数将“价格”数据按照“类型”进行拆分，将相同类别的数据分为一组。

2.描述性统计。对不同类型的化妆品的价格进行描述性统计，包括集中程度度量、离散程度度量以及分布形态度量。大部分指标可直接调用Pandas、NumPy库中的统计函数，少数指标(平均差、异众比率、四分位差和变异系数)需自定义计算。

多函数聚合统计。利用agg函数对groupby的结果同时应用多个函数。如grouped_data.agg(['mean’,'median'])表示同时计算分组后的 grouped data 的平均值和中位数。

输出结果。汇总并输出集中趋势、离散程度和分布形态三个角度的描述性统计结果。

```
import pandas as pd
import numpy as np

pd.set_option('display.width', 300) # 设置字符显示宽度
pd.set_option('display.max_columns', None)  # 设置显示最大列，None为显示所有列
df = pd.read_csv('../data/skincare.csv')  # 读取数据文件
# 1.数据分组
grouped_data = df.groupby('类型')['价格']  # 按Label分组，统计Price列数据
# 2.计算集中趋势度量：平均值、中位数、众数
Central_Tendency = grouped_data.agg(['mean', 'median'])  # 对grouped_data进行聚合操作，计算每组的平均值和中位数
Central_Tendency.columns = ['平均值', '中位数']  # 将结果表的列名重命名为中文的“平均值”和“中位数”
mode = grouped_data.apply(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)  # 使用lambda函数计算每组的众数，如果没有众数则返回NaN
Central_Tendency['众数'] = mode  # 将众数的结果添加为新的列“众数”
print('集中趋势度量结果：')
print(round(Central_Tendency,2))
# 2.计算离散程度度量：极差、平均差、方差、标准差、异众比率、四分位差、变异系数
range = grouped_data.apply(np.ptp)  # 极差
mean_diff = grouped_data.apply(lambda x: np.mean(np.abs(x - x.mean())))  # 平均差
var = grouped_data.apply(np.var)  # 方差
std = grouped_data.apply(np.std)  # 标准差
z_ratio = grouped_data.apply(lambda x: (x.value_counts().max() / len(x)))  # 异众比率
quartile_deviation = grouped_data.apply(lambda x: np.percentile(x, 75)-np.percentile(x, 25))  # 四分位差
coef_of_variation = grouped_data.apply(lambda x: np.std(x) / np.mean(x))  # 变异系数
Dispersion = pd.DataFrame({'极差': range,
               '平均差': mean_diff,
               '方差':var,
               '标准差':std,
               '异众比率': z_ratio,
               '四分位差': quartile_deviation,
               '变异系数': coef_of_variation})
print('离散程度度量结果：')
print(round(Dispersion,2))
# 3.计算分布形态度量：偏度、峰度
Distribution = grouped_data.agg([pd.Series.skew, pd.Series.kurtosis])
Distribution.columns = ['偏度', '峰度']
print('分布形态度量结果：')
print(round(Distribution,2))
```

![image-20241222155533448](assets\image-20241222155533448.png)

## 2.4数据可视化

数据可视化(Data visualization)是一种技术，通过将数据转化为图表、图形或地图等视觉格式，以直观和易于理解的方式呈现复杂数据。

### 2.4.1条形图

### 2.4.2饼图

### 2.4.3箱线图

### 2.4.4直方图

### 2.4.5折线图

### 2.4.6散点图

### 2.4.7气泡图

气泡图(Bubble chart)是用于展示多个变量之间关系的图形，在绘制时可以将变量X1用横轴展示，变量X2用纵轴展示，而第三个变量X3则用气泡的大小来表示，第四个变量X4,则用气泡颜色加以区分，这样便可以展示四维的信息。图2-16展示了二手车交易数据中“发动机功率”(横轴)、“交易价格”(纵轴)、“燃油类型”(气泡颜色)、“汽车已行驶公里”(气泡大小)这四个属性的分布。从气泡图中可以观察到发动机功率与交易价格存在一定的正相关关系，对于三种不同的燃油类型的汽车，其交易价格都随发动机功率的增加而提高。在一些可视化应用中，还可展示“气泡”随时间的变化轨迹，从而达到在二维平面上体现五维特征的效果。

![image-20241222162004801](assets\image-20241222162004801.png)

### 2.4.8示例：条形图、饼图、箱线图、直方图、折线图、散点图、气泡图

```
import numpy as np
import matplotlib.pyplot as plt

# 生成数据
np.random.seed(0)
categories = ['A', 'B', 'C', 'D', 'E']
values = np.random.randint(1, 100, size=len(categories))
sizes = np.random.randint(1, 100, size=len(categories))
x = np.random.normal(0, 1, 100)
y = np.random.normal(0, 1, 100)
sizes_bubble = np.random.rand(100) * 1000  # 生成100个随机大小，范围在0到1000之间

# 条形图
plt.figure(figsize=(8, 6))
plt.bar(categories, values, color='skyblue')
plt.xlabel('Categories')
plt.ylabel('Values')
plt.title('Bar Chart')
plt.show()

# 饼图
plt.figure(figsize=(8, 6))
plt.pie(sizes, labels=categories, autopct='%1.1f%%', startangle=140)
plt.title('Pie Chart')
plt.show()

# 箱线图
plt.figure(figsize=(8, 6))
plt.boxplot(values)
plt.title('Box Plot')
plt.show()

# 直方图
plt.figure(figsize=(8, 6))
plt.hist(x, bins=30, color='green', alpha=0.7)
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.title('Histogram')
plt.show()

# 折线图
plt.figure(figsize=(8, 6))
plt.plot(x, label='Line 1')
plt.plot(y, label='Line 2')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.title('Line Chart')
plt.legend()
plt.show()

# 散点图
plt.figure(figsize=(8, 6))
plt.scatter(x, y, color='red')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.title('Scatter Plot')
plt.show()

# 气泡图
plt.figure(figsize=(8, 6))
plt.scatter(x, y, s=sizes_bubble, alpha=0.5, c='blue')  # 使用sizes_bubble数组控制每个点的大小
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.title('Bubble Chart with Different Sizes')
plt.show()
```

条形图（Bar Chart）：使用plt.bar()函数生成条形图，展示了不同类别的值。categories是类别标签，values是每个类别的值。

饼图（Pie Chart）：使用plt.pie()函数生成饼图，展示了每个类别的比例。sizes是每个类别的值，labels是类别标签，autopct用于显示百分比，startangle用于设置起始角度。


箱线图（Box Plot）：使用plt.boxplot()函数生成箱线图，展示了数据的分布情况。values是数据值。


直方图（Histogram）：使用plt.hist()函数生成直方图，展示了数据的频率分布。x是数据值，bins是直方图的柱数，color是柱的颜色，alpha是透明度。

折线图（Line Chart）：使用plt.plot()函数生成折线图，展示了数据的变化趋势。x和y是数据值，label用于图例标签，plt.legend()用于显示图例。x和y是数据值，label用于图例标签，plt.legend()用于显示图例。


散点图（Scatter Plot）：使用plt.scatter()函数生成散点图，展示了数据点的分布。x和y是数据值，color是点的颜色。


气泡图（Bubble Chart）：使用plt.scatter()函数生成气泡图，展示了数据点的分布和大小。x和y是数据值，s=sizes_bubble参数用于控制每个点的大小，alpha=0.5设置点的透明度，c='blue'设置点的颜色。


每个图表都使用了plt.figure(figsize=(8, 6))来设置图表的大小，plt.xlabel()和plt.ylabel()来设置横纵坐标的标签，plt.title()来设置图表的标题。

![image-20250125171013842](assets\image-20250125171013842.png)

![image-20250125171020089](assets\image-20250125171020089.png)

![image-20250125171027604](assets\image-20250125171027604.png)

![image-20250125171034963](assets\image-20250125171034963.png)

![image-20250125171039744](assets\image-20250125171039744.png)

![image-20250125171044456](assets\image-20250125171044456.png)

![image-20250125171050169](assets\image-20250125171050169.png)



### 2.4.9实践案例：钻石属性数据可视化分析

![image-20241222160610614](assets\image-20241222160610614.png)

```
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt
import collections

# 1.读取数据
df = pd.read_csv('../data/diamonds.csv', encoding="gb2312")  # 读取文件
plt.rcParams['font.sans-serif'] = 'SimSun'  # 设置字体为宋体
plt.rcParams.update({'font.size': 18})
plt.figure(figsize=(12, 8))  # 设置图形的宽和高
# 2.绘制条形图
plt.subplot(2, 3, 1)
color_list = collections.Counter(df['颜色'])  # 使用collections模块的Counter函数统计每个字母的数量
colors = ['D', 'E', 'F', 'G', 'H', 'I', 'J']  # 指定颜色列表的顺序创建字母和对应数量的列表
counts = [color_list[color] for color in colors]  # 按照colors列表中的顺序依次获取各颜色的数量，存储在counts中
plt.bar(colors, counts, facecolor='#F7C4C1', edgecolor='k', alpha=0.8)  # 利用plt.bar()函数绘制条形图
plt.xlabel('颜色')
plt.ylabel('数量')
plt.title('条形图（颜色）')
# 3.绘制饼图
plt.subplot(2, 3, 2)
clarity = df['清晰度'].value_counts()  # 统计各清晰度等级的钻石数量
plt.pie(clarity.values, labels=clarity.index, colors=['#FCDFBE', '#F3DAC0', '#F7C4C1', '#E3C6E0', '#CECCE5', '#C3E2EC', '#BCD1BC', '#DBEDC5'])  # 利用plt.pie()函数绘制饼图
plt.title('饼图（清晰度）')
# 4.绘制箱线图
plt.subplot(2, 3, 3)
plt.grid(True)  # 显示网格
plt.boxplot(df['长'],
            medianprops={'color': 'red', 'linewidth': '1.5'},
            meanline=True,
            showmeans=True,
            meanprops={'color': 'blue', 'ls': '--', 'linewidth': '1.5'},
            flierprops={"marker": "o", "markeredgecolor": "k", "markersize": 5},
            labels=['长'])  # 利用plt.boxplot()函数绘制箱线图
plt.title('箱线图（长）')
# 5.绘制直方图
plt.subplot(2, 3, 4)
plt.hist(df['价格'], bins=10, facecolor='#BCD1BC', edgecolor='k',alpha=0.8)  # 利用plt.hist()函数绘制直方图
plt.title("直方图（价格）")
plt.xlabel("价格")
plt.ylabel("频数")
# 6.绘制折线图
plt.subplot(2, 3, 5)
counts = df['切割质量'].value_counts()  # 统计各切割质量等级的钻石数量
plt.plot(counts, alpha=0.8, marker='*', color='#DB7093')  # 利用plt.plot()函数绘制折线图
plt.title('折线图（切割质量）')
plt.xlabel('切割质量')
plt.ylabel('数量')
# 7.绘制散点图
df_data = df[df['切割质量'] == '好']  # 提取切割质量为“好“的钻石数据
plt.subplot(2, 3, 6)
plt.scatter(df_data['克拉'], df_data['价格'], color='#E3C6E0', alpha=0.5,  s=20)  # 利用plt.scatter()函数绘制散点图
plt.title('散点图（克拉与价格）')
plt.xlabel('克拉')
plt.ylabel('价格')
plt.tight_layout()  # 调整子图位置
plt.show()
```

![image-20241222160654777](assets\image-20241222160654777.png)