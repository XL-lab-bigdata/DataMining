# 1概述

对于数组和Series来说，维度就是功能shape返回的结果，shape中返回了几个数字，就是几维。索引以外的数
据，不分行列的叫一维（此时shape返回唯一的维度上的数据个数），有行列之分叫二维（shape返回行x列），也
称为表。一张表最多二维，复数的表构成了更高的维度。当一个数组中存在2张3行4列的表时，shape返回的是(更
高维，行，列)。当数组中存在2组2张3行4列的表时，数据就是4维，shape返回(2,2,3,4)。

数组中的每一张表，都可以是一个特征矩阵或一个DataFrame，这些结构永远只有一张表，所以一定有行列，其中
行是样本，列是特征。针对每一张表，维度指的是样本的数量或特征的数量，一般无特别说明，指的都是特征的数
量。除了索引之外，一个特征是一维，两个特征是二维，n个特征是n维。

对图像来说，维度就是图像中特征向量的数量。特征向量可以理解为是坐标轴，一个特征向量定义一条直线，是一
维，两个相互垂直的特征向量定义一个平面，即一个直角坐标系，就是二维，三个相互垂直的特征向量定义一个空
间，即一个立体直角坐标系，就是三维。三个以上的特征向量相互垂直，定义人眼无法看见，也无法想象的高维空
间。

降维算法中的”降维“，指的是降低特征矩阵中特征的数量。降维的目的是为了让算法运算更
快，效果更好，但其实还有另一种需求：数据可视化。从上面的图我们其实可以看得出，图像和特征矩阵的维度是
可以相互对应的，即一个特征对应一个特征向量，对应一条坐标轴。所以，三维及以下的特征矩阵，是可以被可视
化的，这可以帮助我们很快地理解数据的分布，而三维以上特征矩阵的则不能被可视化，数据的性质也就比较难理
解。

## 1.2 sklearn中的降维算法

sklearn中降维算法都被包括在模块decomposition中，这个模块本质是一个矩阵分解模块。在过去的十年中，如
果要讨论算法进步的先锋，矩阵分解可以说是独树一帜。矩阵分解可以用在降维，深度学习，聚类分析，数据预处
理，低纬度特征学习，推荐系统，大数据分析等领域。在2006年，Netflix曾经举办了一个奖金为100万美元的推荐
系统算法比赛，最后的获奖者就使用了矩阵分解中的明星：奇异值分解SVD。

| 类 | 说明 |
| --- | --- |
| 主成分分析 | |
| decomposition.PCA | 主成分分析（PCA） |
| decomposition.IncrementalPCA | 增量主成分分析（IPCA） |
| decomposition.KernelPCA | 核主成分分析（KPCA） |
| decomposition.MiniBatchSparsePCA | 小批量稀疏主成分分析 |
| decomposition.SparsePCA | 稀疏主成分分析（SparsePCA） |
| decomposition.TruncatedSVD | 截断的SVD（aka LSA） |
| 因子分析 | |
| decomposition.FactorAnalysis | 因子分析（FA） |
| 独立成分分析 | |
| decomposition.FastICA | 独立成分分析的快速算法 |
| 字典学习 | |
| decomposition.DictionaryLearning | 字典学习 |
| decomposition.MiniBatchDictionaryLearning | 小批量字典学习 |
| decomposition.dict_learning | 字典学习用于矩阵分解 |
| decomposition.dict_learning_online | 在线字典学习用于矩阵分解 |
| 高级矩阵分解 | |
| decomposition.LatentDirichletAllocation | 具有在线变分贝叶斯算法的隐含狄利克雷分布 |
| decomposition.NMF | 非负矩阵分解（NMF） |
| 其他矩阵分解 | |
| decomposition.SparseCoder | 稀疏编码 |

## 2 PCA与SVD

在降维过程中，我们会减少特征的数量，这意味着删除数据，数据量变少则表示模型可以获取的信息会变少，模型
的表现可能会因此受影响。同时，在高维数据中，必然有一些特征是不带有有效的信息的（比如噪音），或者有一
些特征带有的信息和其他一些特征是重复的（比如一些特征可能会线性相关）。我们希望能够找出一种办法来帮助
我们衡量特征上所带的信息量，让我们在降维的过程中，能够即减少特征的数量，又保留大部分有效信息——将那
些带有重复信息的特征合并，并删除那些带无效信息的特征等等——逐渐创造出能够代表原特征矩阵大部分信息
的，特征更少的，新特征矩阵。一种重要的特征选择方法：方差过滤。如果一个特征的方差很小，则意味着这个
特征上很可能有大量取值都相同（比如90%都是1，只有10%是0，甚至100%是1），那这一个特征的取值对样本而
言就没有区分度，这种特征就不带有有效信息。从方差的这种应用就可以推断出，如果一个特征的方差很大，则说
明这个特征上带有大量的信息。因此，在降维中，PCA使用的信息量衡量指标，就是样本方差，又称可解释性方
差，方差越大，特征所带的信息量越多。

$ \text{Var} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2 $

## 2.1 降维实现

PCA作为矩阵分解算法的核心算法，其实没有太多参数，但不幸的是每个参数的意义和运用都很难，因为几乎每个
参数都涉及到高深的数学原理

我们现在有一组简单的数据，有特征x1和x2，三个样本数据的坐标点分别为(1,1)，(2,2)，(3,3)。我们可以让x1和x2分别作为两个特征向量，很轻松地用一个二维平面来描述这组数据。这组数据现在每个特征的均值都为2，方差则等于：

$ x1\_var = x2\_var = \frac{(1 - 2)^2 + (2 - 2)^2 + (3 - 2)^2}{2} = 1 $

每个特征的数据一模一样，因此方差也都为1，数据的方差总和是2。

现在我们的目标是：只用一个特征向量来描述这组数据，即将二维数据降为一维数据，并且尽可能地保留信息量，即让数据的总方差尽量靠近2。于是，我们将原本的直角坐标系逆时针旋转45°，形成了新的特征向量x1*和x2*组成的新平面，在这个新平面中，三个样本数据的坐标点可以表示为($\sqrt{2}$, 0)，(2$\sqrt{2}$, 0)，(3$\sqrt{2}$, 0)。可以注意到，x2*上的数值此时都变成了0，因此x2*明显不带有任何有效信息了（此时x2*的方差也为0了）。此时，x1*特征上的数据均值是2$\sqrt{2}$，而方差则可表示成：

$ x2*\_var = \frac{(\sqrt{2} - 2\sqrt{2})^2 + (2\sqrt{2} - 2\sqrt{2})^2 + (3\sqrt{2} - 2\sqrt{2})^2}{2} = 2 $

x1*上的数据均值为0，方差也为0。

此时，我们根据信息含量的排序，取信息含量最大的一个特征，因为我们想要的是一维数据。所以我们可以将x2*
删除，同时也删除图中的x2*特征向量，剩下的x1*就代表了曾经需要两个特征来代表的三个样本点。通过旋转原
有特征向量组成的坐标轴来找到新特征向量和新坐标平面，我们将三个样本点的信息压缩到了一条直线上，实现了
二维变一维，并且尽量保留原始数据的信息。一个成功的降维，就实现了。

| 过程 | 二维特征矩阵 | n维特征矩阵 |
| --- | --- | --- |
| 1 | 输入原数据，结构为 (3,2)<br>找出原本的2个特征对应的直角坐标系，本质是找出这2个特征构成的2维平面 | 输入原数据，结构为 (m,n)<br>找出原本的n个特征向量构成的n维空间V |
| 2 | 决定降维后的特征数量：1 | 决定降维后的特征数量：k |
| 3 | 旋转，找出一个新坐标系<br>本质是找出2个新的特征向量，以及它们构成的新2维平面<br>新特征向量让数据能够被压缩到少数特征上，并且总信息量不损失太多 | 通过某种变化，找出n个新的特征向量，以及它们构成的新n维空间V |
| 4 | 找出数据点在新坐标轴上，2个新坐标轴上的坐标 | 找出原始数据在新特征空间V中的n个新特征向量上对应的值，即“将数据映射到新空间中” |
| 5 | 选取第1个方差最大的特征向量，删掉没有被选中的特征，成功将2维平面降为1维 | 选取前k个信息量最大的特征，删掉没有被选中的特征，成功将n维空间V降为k维 |

## PCA和特征选择技术的不同

特征选择是从已存在的特征中选取携带信息最多的，选完之后的特征依然具有可解释性，我们依然知道这个特
征在原数据的哪个位置，代表着原数据上的什么含义。
而PCA，是将已存在的特征进行压缩，降维完毕后的特征不是原本的特征矩阵中的任何一个特征，而是通过某
些方式组合起来的新特征。通常来说，在新的特征矩阵生成之前，我们无法知晓PCA都建立了怎样的新特征向
量，新特征矩阵生成之后也不具有可读性，我们无法判断新特征矩阵的特征是从原数据中的什么特征组合而
来，新特征虽然带有原始数据的信息，却已经不是原数据上代表着的含义了。以PCA为代表的降维算法因此是
特征创造（feature creation，或feature construction）的一种。
可以想见，PCA一般不适用于探索特征和标签之间的关系的模型（如线性回归），因为无法解释的新特征和标
签之间的关系不具有意义。在线性回归模型中，我们使用特征选择。

## 2.2 重要参数n_components

n_components是我们降维后需要的维度，即降维后需要保留的特征数量，降维流程中第二步里需要确认的k值，
一般输入[0, min(X.shape)]范围中的整数。一说到K，大家可能都会想到，类似于KNN中的K和随机森林中的
n_estimators，这是一个需要我们人为去确认的超参数，并且我们设定的数字会影响到模型的表现。如果留下的特
征太多，就达不到降维的效果，如果留下的特征太少，那新特征向量可能无法容纳原始数据集中的大部分信息，因
此，n_components既不能太大也不能太小。那怎么办呢？
可以先从我们的降维目标说起：如果我们希望可视化一组数据来观察数据分布，我们往往将数据降到三维以下，很
多时候是二维，即n_components的取值为2。

### 2.2.1 案例：高维数据的可视化


```python
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
```


```python
iris = load_iris()
y = iris.target
X = iris.data
#作为数组，X是几维？
X.shape
#作为数据表或特征矩阵，X是几维？
import pandas as pd
pd.DataFrame(X)
```


```python
#调用PCA
pca = PCA(n_components=2) #实例化
pca = pca.fit(X) #拟合模型
X_dr = pca.transform(X) #获取新矩阵
X_dr
#也可以fit_transform一步到位
#X_dr = PCA(2).fit_transform(X)
```


```python
#要将三种鸢尾花的数据分布显示在二维平面坐标系中，对应的两个坐标（两个特征向量）应该是三种鸢尾花降维后的
x1和x2，怎样才能取出三种鸢尾花下不同的x1和x2呢？
X_dr[y == 0, 0] #这里是布尔索引，看出来了么？
#要展示三中分类的分布，需要对三种鸢尾花分别绘图
#可以写成三行代码，也可以写成for循环
"""
plt.figure()
plt.scatter(X_dr[y==0, 0], X_dr[y==0, 1], c="red", label=iris.target_names[0])
plt.scatter(X_dr[y==1, 0], X_dr[y==1, 1], c="black", label=iris.target_names[1])
plt.scatter(X_dr[y==2, 0], X_dr[y==2, 1], c="orange", label=iris.target_names[2])
plt.legend()
plt.title('PCA of IRIS dataset')
plt.show()
"""
colors = ['red', 'black', 'orange']
iris.target_names
plt.figure()
for i in [0, 1, 2]:
    plt.scatter(X_dr[y == i, 0]
               ,X_dr[y == i, 1]
               ,alpha=.7
               ,c=colors[i]
               ,label=iris.target_names[i]
               )
plt.legend()
plt.title('PCA of IRIS dataset')
plt.show()
```

鸢尾花的分布被展现在我们眼前了，明显这是一个分簇的分布，并且每个簇之间的分布相对比较明显，也许
versicolor和virginia这两种花之间会有一些分类错误，但setosa肯定不会被分错。这样的数据很容易分类，可以遇
见，KNN，随机森林，神经网络，朴素贝叶斯，Adaboost这些分类器在鸢尾花数据集上，未调整的时候都可以有
95%上下的准确率。


```python
#属性explained_variance_，查看降维后每个新特征向量上所带的信息量大小（可解释性方差的大小）
pca.explained_variance_
#属性explained_variance_ratio，查看降维后每个新特征向量所占的信息量占原始数据总信息量的百分比
#又叫做可解释方差贡献率
pca.explained_variance_ratio_
#大部分信息都被有效地集中在了第一个特征上
pca.explained_variance_ratio_
.sum()
```


```python
import numpy as np
pca_line = PCA().fit(X)
plt.plot([1,2,3,4],np.cumsum(pca_line.explained_variance_ratio_))
plt.xticks([1,2,3,4]) #这是为了限制坐标轴显示为整数
plt.xlabel("number of components after dimension reduction")
plt.ylabel("cumulative explained variance ratio")
plt.show()
```

### 2.2.2 最大似然估计自选超参数


```python
pca_mle = PCA(n_components="mle")
pca_mle = pca_mle.fit(X)
X_mle = pca_mle.transform(X)
X_mle
#可以发现，mle为我们自动选择了3个特征
pca_mle.explained_variance_ratio_.sum()
#得到了比设定2个特征时更高的信息含量，对于鸢尾花这个很小的数据集来说，3个特征对应这么高的信息含量，并不
需要去纠结于只保留2个特征，毕竟三个特征也可以可视化
```

### 2.2.3 按信息量占比选超参数


```python
pca_f = PCA(n_components=0.97,svd_solver="full")
pca_f = pca_f.fit(X)
X_f = pca_f.transform(X)
pca_f.explained_variance_ratio_
```

## 2.3 PCA中的SVD

### 2.3.1 PCA中的SVD哪里来？

`svd_solver`是奇异值分解器的意思，为什么PCA算法下面会有有关奇异值分解的参数？不是两种算法么？PCA和SVD涉及了大量的矩阵计算，两者都是运算量很大的模型，但其实，SVD有一种惊人的数学性质，即是它可以跳过数学神秘的宇宙，不计算协方差矩阵，直接找出一个新特征向量组成的n维空间，而这个n维空间就是奇异值分解后的右矩阵$V^T$（所以一开始在讲解降维过程时，我们说“生成新特征向量组成的空间V”，并非巧合，而是特指奇异值分解中的矩阵$V^T$）。

传统印象中的SVD：$X \rightarrow$ 数学神秘的宇宙 $\rightarrow U\Sigma V^T$

其实会开挂的SVD：$X \rightarrow$ 一个比起PCA简化非常多的数学过程 $\rightarrow V^T$

右奇异矩阵$V^T$有着如下性质：

$ X_{dr} = X * V[:, k:]^T $

k就是n_components，是我们降维后希望得到的维度。若X为(m,n)的特征矩阵，$V^T$就是结构为(n,n)的矩阵，取这个矩阵的前k行（进行切片），即将V转换为结构为(k,n)的矩阵。而$SV_{(k,n)}^T$与原特征矩阵X相乘，即可得到降维后的特征矩阵$X_{dr}$。这是说，奇异值分解可以不计算协方差矩阵等等结构复杂计算冗长的矩阵，就直接求出新特征空间和降维后的特征矩阵。

简而言之，SVD在矩阵分解中的过程比PCA简单快速，虽然两个算法都走一样的分解流程，但SVD可以作弊耍赖直
接算出V。但是遗憾的是，SVD的信息量衡量指标比较复杂，要理解”奇异值“远不如理解”方差“来得容易，因此，
sklearn将降维流程拆成了两部分：一部分是计算特征空间V，由奇异值分解完成，另一部分是映射数据和求解新特
征矩阵，由主成分分析完成，实现了用SVD的性质减少计算量，却让信息量的评估指标是方差


```python
PCA(2).fit(X).components_
PCA(2).fit(X).components_.shape
```


    ---------------------------------------------------------------------------

    NameError                                 Traceback (most recent call last)

    Cell In[4], line 1
    ----> 1 PCA(2).fit(X).components_
          2 PCA(2).fit(X).components_.shape
    

    NameError: name 'PCA' is not defined


### 2.3.2 重要参数svd_solver 与 random_state

参数svd_solver是在降维过程中，用来控制矩阵分解的一些细节的参数。有四种模式可选："auto", "full", "arpack", "randomized"，默认"auto"。

- **"auto"**: 基于X.shape和n_components的默认策略来选择分解器：如果输入数据的尺寸大于500x500且要提取的特征数小于数据最小维度min(X.shape)的80%，就启用效率更高的"randomized"方法。否则，精确完整的SVD将被计算，截断将会在矩阵被分解完成后有选择地发生

- **"full"**: 从scipy.linalg.svd中调用标准的LAPACK分解器来生成精确完整的SVD，适合数据量比较适中，计算时间充足的情况，生成的精确完整的SVD的结构为：
  $$
  U_{(m,m)}, \Sigma_{(m,n)}, V^T_{(n,n)}
  $$

- **"arpack"**: 从scipy.sparse.linalg.svds调用ARPACK分解器来运行截断奇异值分解(SVD truncated)，分解时就将特征数量降到n_components中输入的数值k，可以加快运算速度，适合特征矩阵很大的时候，但一般用于特征矩阵为稀疏矩阵的情况，此过程包含一定的随机性。截断后的SVD分解出的结构为：
  $$
  U_{(m,k)}, \Sigma_{(k,k)}, V^T_{(n,n)}
  $$

- **"randomized"**: 通过Halko等人的随机方法进行随机SVD。在"full"方法中，分解器会根据原始数据和输入的n_components值去计算和寻找符合需求的新特征向量，但是在"randomized"方法中，分解器先生成多个随机向量，然后一一去检测这些随机向量中是否有任何一个符合我们的分解需求，如果符合，就保留这个随机向量，并基于这个随机向量来构建后续的向量空间。这个方法已经被Halko等人证明，比"full"模式下计算快很多，并且还能够保证模型运行效果。适合特征矩阵巨大，计算量庞大的情况。

而参数random_state在参数svd_solver的值为"arpack"或"randomized"的时候生效，可以控制这两种SVD模式中的随机模式。通常我们就选用"auto"，不必对这个参数纠结太多。

### 2.3.3 重要属性components_

现在我们了解了，V(k,n)是新特征空间，是我们要将原始数据进行映射的那些新特征向量组成的矩阵。我们用它来
计算新的特征矩阵，但我们希望获取的毕竟是X_dr，为什么我们要把V(k,n)这个矩阵保存在n_components这个属
性当中来让大家调取查看呢？
我们之前谈到过PCA与特征选择的区别，即特征选择后的特征矩阵是可解读的，而PCA降维后的特征矩阵式不可解
读的：PCA是将已存在的特征进行压缩，降维完毕后的特征不是原本的特征矩阵中的任何一个特征，而是通过某些
方式组合起来的新特征。通常来说，在新的特征矩阵生成之前，我们无法知晓PCA都建立了怎样的新特征向量，新
特征矩阵生成之后也不具有可读性，我们无法判断新特征矩阵的特征是从原数据中的什么特征组合而来，新特征虽
然带有原始数据的信息，却已经不是原数据上代表着的含义了。
但是其实，在矩阵分解时，PCA是有目标的：在原有特征的基础上，找出能够让信息尽量聚集的新特征向量。在
sklearn使用的PCA和SVD联合的降维方法中，这些新特征向量组成的新特征空间其实就是V(k,n)。当V(k,n)是数字
时，我们无法判断V(k,n)和原有的特征究竟有着怎样千丝万缕的数学联系。但是，如果原特征矩阵是图像，V(k,n)这
个空间矩阵也可以被可视化的话，我们就可以通过两张图来比较，就可以看出新特征空间究竟从原始数据里提取了
什么重要的信息。


```python
from sklearn.datasets import fetch_lfw_people
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np
```


```python
faces = fetch_lfw_people(min_faces_per_person=60)
faces.images.shape
#怎样理解这个数据的维度？
faces.data.shape
#换成特征矩阵之后，这个矩阵是什么样？
X = faces.data
```


```python
#数据本身是图像，和数据本身只是数字，使用的可视化方法不同
#创建画布和子图对象
fig, axes = plt.subplots(4,5
                       ,figsize=(8,4)
                       ,subplot_kw = {"xticks":[],"yticks":[]} #不要显示坐标轴
                       )
fig
axes
#不难发现，axes中的一个对象对应fig中的一个空格
#我们希望，在每一个子图对象中填充图像（共24张图），因此我们需要写一个在子图对象中遍历的循环
axes.shape
#二维结构，可以有两种循环方式，一种是使用索引，循环一次同时生成一列上的三个图
#另一种是把数据拉成一维，循环一次只生成一个图
#在这里，究竟使用哪一种循环方式，是要看我们要画的图的信息，储存在一个怎样的结构里
#我们使用 子图对象.imshow 来将图像填充到空白画布上
#而imshow要求的数据格式必须是一个(m,n)格式的矩阵，即每个数据都是一张单独的图
#因此我们需要遍历的是faces.images，其结构是(1277, 62, 47)
#要从一个数据集中取出24个图，明显是一次性的循环切片[i,:,:]来得便利
#因此我们要把axes的结构拉成一维来循环
axes.flat
enumerate(axes.flat)
#填充图像
for i, ax in enumerate(axes.flat):
    ax.imshow(faces.images[i,:,:] 
             ,cmap="gray" #选择色彩的模式
               )
#https://matplotlib.org/tutorials/colors/colormaps.html
```


```python
#原本有2900维，我们现在来降到150维
pca = PCA(150).fit(X)
V = pca.components_
V.shape

```


```python
fig, axes = plt.subplots(3,8,figsize=(8,4),subplot_kw = {"xticks":[],"yticks":[]})
for i, ax in enumerate(axes.flat):
    ax.imshow(V[i,:].reshape(62,47),cmap="gray")
```

## 2.4 重要接口inverse_transform

### 2.4.1 案例：用人脸识别看PCA降维后的信息保存量

人脸识别是最容易的，用来探索inverse_transform功能的数据。我们先调用一组人脸数据X(m,n)，对人脸图像进
行绘制，然后我们对人脸数据进行降维得到X_dr，之后再使用inverse_transform(X_dr)返回一个X_inverse(m,n)，
并对这个新矩阵中的人脸图像也进行绘制。如果PCA的降维过程是可逆的，我们应当期待X(m,n)和X_inverse(m,n)
返回一模一样的图像，即携带一模一样的信息


```python
from sklearn.datasets import fetch_lfw_people
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np
```


```python
faces = fetch_lfw_people(min_faces_per_person=60)
faces.images.shape
#怎样理解这个数据的维度？
faces.data.shape
#换成特征矩阵之后，这个矩阵是什么样？
X = faces.data
```


```python
pca = PCA(150)
X_dr = pca.fit_transform(X)
X_dr.shape
```


```python
X_inverse = pca.inverse_transform(X_dr)
X_inverse.shape

```


```python
fig, ax = plt.subplots(2,10,figsize=(10,2.5)
                     ,subplot_kw={"xticks":[],"yticks":[]}
                     )
#和2.3.3节中的案例一样，我们需要对子图对象进行遍历的循环，来将图像填入子图中
#那在这里，我们使用怎样的循环？
#现在我们的ax中是2行10列，第一行是原数据，第二行是inverse_transform后返回的数据
#所以我们需要同时循环两份数据，即一次循环画一列上的两张图，而不是把ax拉平
for i in range(10):
    ax[0,i].imshow(face.image[i,:,:],cmap="binary_r")
    ax[1,i].imshow(X_inverse[i].reshape(62,47),cmap="binary_r")
```

### 2.4.2 案例：用PCA做噪音过滤

降维的目的之一就是希望抛弃掉对模型带来负面影响的特征，而我们相信，带有效信息的特征的方差应该是远大于
噪音的，所以相比噪音，有效的特征所带的信息应该不会在PCA过程中被大量抛弃。inverse_transform能够在不
恢复原始数据的情况下，将降维后的数据返回到原本的高维空间，即是说能够实现”保证维度，但去掉方差很小特
征所带的信息“。利用inverse_transform的这个性质，我们能够实现噪音过滤。


```python
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np
```


```python
igits = load_digits()
digits.data.shape
```


```python
def plot_digits(data):
    fig, axes = plt.subplots(4,10,figsize=(10,4)
                           ,subplot_kw = {"xticks":[],"yticks":[]}
                           )
    for i, ax in enumerate(axes.flat):
        ax.imshow(data[i].reshape(8,8),cmap="binary")
        
plot_digits(digits.data)
```


```python
np.random.RandomState(42)
#在指定的数据集中，随机抽取服从正态分布的数据
#两个参数，分别是指定的数据集，和抽取出来的正太分布的方差
noisy = np.random.normal(digits.data,2)
plot_digits(noisy)
```


```python
pca = PCA(0.5).fit(noisy)
X_dr = pca.transform(noisy)
X_dr.shape
```


```python
without_noise = pca.inverse_transform(X_dr)
plot_digits(without_noise)
```

# 3 案例：PCA对手写数字数据集的降维


```python
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
```


```python
data = pd.read_csv(r"C:\work\learnbetter\micro-class\week 3 Preprocessing\digitrecognizor.csv")
X = data.iloc[:,1:]
y = data.iloc[:,0]
X.shape
```


```python
pca_line = PCA().fit(X)
plt.figure(figsize=[20,5])
plt.plot(np.cumsum(pca_line.explained_variance_ratio_))
plt.xlabel("number of components after dimension reduction")
plt.ylabel("cumulative explained variance ratio")
plt.show()
```


```python
#======【TIME WARNING：2mins 30s】======#
score = []
for i in range(1,101,10):
    X_dr = PCA(i).fit_transform(X)
    once = cross_val_score(RFC(n_estimators=10,random_state=0)
                           ,X_dr,y,cv=5).mean()
    score.append(once)
plt.figure(figsize=[20,5])
plt.plot(range(1,101,10),score)
plt.show()

```


```python
#======【TIME WARNING：2mins 30s】======#
score = []
for i in range(10,25):
    X_dr = PCA(i).fit_transform(X)
    once = cross_val_score(RFC(n_estimators=10,random_state=0),X_dr,y,cv=5).mean()
    score.append(once)
plt.figure(figsize=[20,5])
plt.plot(range(10,25),score)
plt.show()
```


```python
X_dr = PCA(23).fit_transform(X)
#======【TIME WARNING:1mins 30s】======#
cross_val_score(RFC(n_estimators=100,random_state=0),X_dr,y,cv=5).mean()
```


```python
from sklearn.neighbors import KNeighborsClassifier as KNN
cross_val_score(KNN(),X_dr,y,cv=5).mean()
```


```python
#======【TIME WARNING: 】======#
score = []
for i in range(10):
    X_dr = PCA(23).fit_transform(X)
    once = cross_val_score(KNN(i+1),X_dr,y,cv=5).mean()
    score.append(once)
plt.figure(figsize=[20,5])
plt.plot(range(10),score)
plt.show()

```


```python
cross_val_score(KNN(4),X_dr,y,cv=5).mean()
#=======【TIME WARNING: 3mins】======#
%%timeit
cross_val_score(KNN(4),X_dr,y,cv=5).mean()
```
