# **关联规则挖掘**

## **一、背景导入**

​	 在大数据时代，我们每天都会生成和处理大量的数据。从这些数据中提取有用的信息和模式变得越来越重要。关联规则挖掘是一种强大的数据分析技术，它能够揭示数据中的隐藏关联关系，帮助我们在各种应用场景中做出更明智的决策。通过相关研究和收集的资料，我们可以发现关联规则挖掘在多个现实领域都有广泛的应用。 

<img src="./img/image-20250311201254136-1741695181549-6.png" alt="image-20250311201254136" style="zoom:67%;" />



#### **1.1 市场购物篮分析** 

​	日常购物时，我们可以注意到超市里某些商品总是被放在一起销售。例如，经典的“啤酒和尿布”案例就非常有名。这个案例是通过分析顾客的购物数据发现的：一些年轻的父亲在购买尿布的同时也会购买啤酒。这种关联关系的发现不仅帮助超市优化了货架布局，还设计了更有效的促销策略，从而提高了销售额。此外，我们还可以发现其他有趣的关联，比如购买烘焙食材的顾客往往也会购买烹饪器具，或者购买儿童玩具的顾客倾向于同时购买儿童图书。 

#### 1.2 购物推荐系统 

​	我经常使用各种电商平台购物。在这些平台上，推荐系统为我提供了个性化的商品推荐。通过分析我的购买历史和浏览行为，平台可以发现我可能感兴趣的商品。例如，我在购买了一本武侠小说后，平台会推荐同类型的其他书籍或相关电影。这种基于关联规则的推荐系统不仅提高了我的购物体验，也增加了平台的销售额。 

#### 1.3 健康与健身领域 

​	作为一名健身爱好者，我常使用各种健康和健身应用APP。这些应用程序通过分析我的运动记录和饮食日志，可以发现某些运动与特定饮食之间的关联。例如，我发现喜欢跑步的人通常也会选择高蛋白饮食。这种关联关系可以帮助我制定更有效的健身计划和饮食建议，从而达到更好的健康效果。 

​	通过这些具体的案例，我们可以深刻地认识到关联规则挖掘在各个领域的广泛应用和巨大潜力。这项技术不仅能够揭示数据中的隐藏模式，还能为决策提供有力的支持。接下来的部分我将详细介绍关联规则挖掘的技术原理、常用算法和研究应用情景。



## 二、关联规则挖掘概述

​	关联规则挖掘是一种数据分析技术，旨在**从大量数据集中发现物品（或事件）之间的关联性**。其核心是找到“如果出现A事物时，很可能同时出现B事物”的规律（包括两个及以上事物的关系）。**关联规则挖掘常用于知识发现，而非预测，属于无监督的机器学习算法。**

#### 2.1相关基础概念

​	学习关联规则需要先了解以下4个基本概念：**项、项集、事务、关联规则**。我们可用使用一个简单的购物篮例子来理解上述概念。

<img src="./img/image-20250311201338988-1741695220019-9.png" alt="image-20250311201338988" style="zoom:67%;" />

​	•**项（Item）：项通常指数据集中可单独识别和操作的基本元素或对象。**

​	eg：在上述购物篮中，"牛奶"、"鸡蛋"、"面包"等都是单个的项。

​	•**项集（Itemset）：项集是指项的集合，它可以包含一个或多个项。**

​	eg： {"牛奶"，"鸡蛋"，"面包"，"薯片"} 是四项集； {"鸡蛋"，"薯片"，"面包"} 是三项集。

​	**•事物（Transaction）：一个事务就是一个项集。不同的事务一起组成了事务集。**

​	eg： {"牛奶"，"鸡蛋"，"面包"，"薯片"} 是一个事务，所有事务（项集）的集合就是一个事务集。

​	**•关联规则（Association rule）：关联规则是指形如 X→Y 的蕴含表达式，其中X是前项，而Y是后项。其含义是前项X发生时，	后项Y也可能发生。常使用支持度、置信度、提升度进行评价。**

​	eg：{牛奶}→{面包}这一关联规则表示当一个人购买牛奶时，可能也会购买面包。

#### 2.2三大常用评估公式

##### **2.2.1 支持度**

​	支持度（Support）：支持度可用来衡量一个关联规则在所有事务中出现的频繁程度。使用包含XY的事务数/总事务数计算。通常位于0-1之间，越接近1，支持度越大。
$$
Support(X,Y)=P(XY)=  \frac {number(XY)}{num(AllSamples)}
$$
​	 eg：{牛奶}→{面包}这一关联规则的支持度=牛奶、面包同时出现的次数 / 事务集事务总数=5/9

##### **2.2.2 置信度**

​	置信度（Confidence）：置信度表示包含项集X的所有事务中，也包含项集Y的事务的概率。使用包含XY的事务数/包含X的事务数计算。通常位于0-1之间，越接近1，置信度越大。
$$
Confidence(X \Rightarrow Y)=  \frac {Support(XY)}{Support(X)}
$$
​	eg：{牛奶}→{面包}这一关联规则的置信度=牛奶、面包同时出现的支持度 / 牛奶的支持度=5/6

##### **2.2.3 提升度**

> [!NOTE]
>
> **除提升度外还有很多其他度量可以用来进行关联规则评估，具体见2.5节。**

​	提升度（Lift）：提升度可用来衡量X的出现对Y的出现概率产生多大变化。使用XY的置信度/Y的支持度计算。提升度越大越好，当提升度等于1时，X与Y相互独立；当提升度大于1时，X与Y有正相关性，反之有负相关性。
$$
Lift(X  \Rightarrow  Y)= \frac {Confidence(X\Rightarrow Y)}{Support(Y)}
$$
​	 eg：{牛奶}→{面包}这一关联规则的提升度=牛奶、面包关联规则的置信度 / 面包的支持度=5/4

##### **2.2.4强关联规则**

​	强关联规则（Strong association rule）：满足最小支持度和最小置信度的关联规则称为强关联规则。一般认为支持度和置信度较高的关联规则是强关联规则，且强关联规则提升度一般大于1。

​	eg：设置最小支持度和最小置信度为0.5，可用发现{牛奶}→{面包}这一关联规则的支持度和置信度都满足最小阈值，且提升度大于1，属于强关联规则。

> [!NOTE]
>
> **为什么常使用最小支持度和最小置信度来筛选强关联规则?**
> 	支持度表示关联规则发生的概率，通常支持度低的关联规则，很难在商业上推广运用并产生可观的收益。使用支持度过滤低频率的规则，可以发现更显著，适用性更强的规则。
>
> ​	置信度表示给定条件下A事物发生时B事物发生的概率，可用来评价A、B事物关联规则的可靠程度。规则A→B置信度越高，B在A发生时出现的可能性就越大。利用置信度，可以将关联性较弱的关联规则排除。
>
> ​	通常而言我们需要结合支持度和置信度两者共同筛选关联规则。
>
> ​	**例子：**关联规则{衣柜，书桌}=>{电脑椅}的支持度为 14%，置信度为 15%，关联规则{台灯，电脑}->{电脑椅}的支持度为 8%，置信度为 35%。如果仅依据支持度，可得出人们购买衣柜、书桌、电脑椅的行为相比购买台灯、电脑、电脑椅的行为频繁的结论；但根据置信度，购买衣柜、书桌的人中仅有 15%购买了电脑椅，其对电脑椅的购买意愿远低于购买台灯、电脑的人，关联规则{台灯，电脑]->{电脑椅}的关联关系更强，在现实中的应用推广能取得更好的效果。

#### 2.3频繁项集、超集、闭频繁项集、极大频繁项集

##### 2.3.1频繁项集

​	频繁项集是指事务集中出现的频次大于最小支持度的项集，含K项的频繁项集称为频繁K项集。

<img src="./img/image-20250311201410425-1741695253601-12-1741695258825-14.png" alt="image-20250311201410425" style="zoom:67%;" />

​	以上述事务数据库为例，若最小支持度为0.7，则只有{C}项集支持度为0.75大于0.77，说明仅{C}项集是频繁项集，且为频繁1项集。

​	若最小支持度为0.5，有{a}、{b}、{c}、{a，c}项集的支持度大于0.5，这些都为频繁项集。

##### 2.3.2超集

​	若项集A中的每一个元素都在项集B中，且项集B中可能包含项集A中没有的元素，则项集B就是项集A的一个超集，项集A是项集B的真子集（类似集合的概念）。

<img src="./img/image-20250311201450136-1741695292522-16.png" alt="image-20250311201450136" style="zoom:67%;" />

​	以上表为例，其中项集{a，b}是项集{a}和项集{b}的超集；项集{a，b}是项集{a,b,c}、{a,b,d}、{a,b,c,d}的真子集。

##### 2.3.3闭频繁项集

​	闭频繁项集满足两个条件：①项集是闭合的。即项集的真超集的支持度≠该项集的支持度。

​						      ②项集是频繁的。即项集的支持度大于最小支持度。

<img src="./img/image-20250311201458357-1741695300629-18.png" alt="image-20250311201458357" style="zoom:67%;" />

​	以上表为例，项集{a,b,c,d}是项集{a,b,c}的唯一真超集。项集{a,b,c,d}的支持度为0.125，项集{a,b,c}的支持度为0.25，两者不相等，说明项集{a,b,c}是闭合的。弱最小支持度为0.2，项集{a,b,c}的支持度大于最小支持度，则项集{a,b,c}是频繁的。综上，项集{a,b,c}是闭合频繁项集。

​	再举一个例子，项集{a,b,c}是项集{a,b}l的真超集，但项集{a,b,c}和项集{a,b}的支持度都为0.25，这种情况下，项集{a,b}就不是闭合项集，也就不可能是闭合频繁项集。

##### 2.3.4极大频繁项集

​	如果一个频繁项集的所有超集都是非频繁项集，那么称该频繁项集为**极大频繁项集**或**最大频繁模式**。

<img src="./img/image-20250311201505059-1741695307592-20.png" alt="image-20250311201505059" style="zoom:67%;" />

​	以上表为例，项集{a,c}的超集有{a,b,c}、{a,d,c}、{a,b,c,d}。若最小支持度阈值为0.5，项集{a,c}的支持度为0.5，属于频繁项集。但项集{a,b,c}、{a,d,c}、{a,b,c,d}的支持度分别为0.25，0.25，0.125，都小于最小支持度0.5，说明项集{a,c}的超集都不是频繁项集，则项集{a,c}为极大频繁项集。

#### 2.4挖掘关联规则基本逻辑

​	关联规则挖掘算法的基本逻辑是由挖掘频繁项集到产生强关联规则。它可划分为以下两个主要任务：

##### 	**①生成全部频繁项集。**

​	其主要目的是发现所有满足最小支持度阈值的项集，这些项集称作频繁项集。

​	参考上一小节的事务数据库，设置最小支持度为0.25，则可筛选出频繁项集如上表所示。

<img src="./img/image-20250311201517337.png" alt="image-20250311201517337" style="zoom:67%;" />

##### 	②产生强关联规则

​	其主要目的是从上一步发现的频繁项集中提取所有高置信度的关联规则，这些规则称作强关联规则。

​	任意项数大于2的频繁项集都可以拆分为至少两个非空子集，其非空子集构成的关联规则的支持度必等于该频繁项集的支持度，大于最小支持度阈值。因此我们可以使用出6频繁项集的子集间构造关联规则来寻找满足最小置信度的强关联规则。

​	如频繁项集{a,c}可才拆分为{a}和{c}子项集，则子项集构成的关联规则{a}→{c}和{c}→{a}的支持度都为0.5，置信度分别为1和0.667。设置最小置信度为0.7，则{a}→{c}为强关联规则，{c}→{a}为弱关联规则。依照上述步骤可得部分关联规则如下表所示。

<img src="./img/image-20250311201529735.png" alt="image-20250311201529735" style="zoom:67%;" />

#### 2.5关联模式评估

> [!TIP]
>
> **支持度-置信度框架的局限性**
>
> 经典的关联规则挖掘算法依赖支持度和置信度来去除没有意义的模式。
>
> 支持度的缺点在于大量潜在的有意义的模式由于包含了支持度较小的项而被筛去（即使该模式对于该项具有显著意义，仍然会因为项自身的支持度过低而删去。但一些稀少的项可能也具有显著意义）。
>
> 置信度的缺点在于忽略了后项的支持度，比如在置信度阈值为40%的情况下，喜欢二次元的人自然占了70%，此时考虑计算机专业→喜欢二次元计算机专业→喜欢二次元这一关联规则，由于我们都是从计算机专业中采样，所以得出的置信度水平为60%。但是我们发现原本就喜欢二次元的人占了70%，加入前件的限制后反而下降到60%，说明计算机专业和二次元间存在逆关系，解释了支持度-置信度框架的另一个不足。

##### 2.5.1常见度量指标

​	正是因为支持度-置信度框架有着局限性, 学者们又提出了许多客观度量来进一步提升关联规则的质量。其中最常用的就是**提升度（Lift）**。**提升度弥补了置信度忽略后项的缺陷，使用规则置信度和后件的支持度计算。**此外，还有一些常见度量指标如下：

<img src="./img/image-20250311201546198.png" alt="image-20250311201546198" style="zoom:67%;" />

##### 2.5.2评估指标适用属性

​	关联规则评估度量对规则的评估可能存在差异甚至得出相反的结果，因此，在实际应用时需要分析评估度量的性质，选择合适的度量，从而准确反映数据中的关联关系，为决策提供有价值的支持。各指标适用性质如下表所示。

<img src="./img/image-20250311201557413.png" alt="image-20250311201557413" style="zoom:67%;" />

​	**对称性**指的是度量在变量置换下保持不变的性质。对称性度量**适用于不需要区分方向性的场合**。在这些场合，度量的对称性确保了分析结果不受表达关系时，只关心基因是否一起表达，如评估基因之间的关联强度。

​	**行/列缩放不变性**指的是度量在对列联表M中的某一行或列进行等比例缩放时保持不变。这种不变性的度量能够**确保分析结果在不同样本比例下的稳定性和一致性**，如比较不同国籍学生的教育成果时，样本中各国籍学生比例变化不应影响度量结果。

​	**行/列排列下的反对称性**指的是度量在对列联表的行或列进行排列变换后变为原来的负值。反对称性度量**适用于区分正负关联的场合**，例如在医疗数据分析中，确定某个因素是增加还是减少某种疾病的风险，反对称性的度量能有效地识别这些关联的方向。

​	**反转不变性**指的是度量在数据的正负翻转(即0和1的互换)下保持不变的性质。它**适用于需要在数据特征变化时保持度量稳定和一致的情境**，例如推荐系统中分析用户对商品的喜好(如喜欢与不喜欢)，即使数据翻转，度量结果仍应保持不变，以确保推荐结果的一致性。

​	**空不变性度量**不考虑不包含A和B的事务数量的影响，一些零不变性度量包括余弦度量和 Jaccard 系数。零不变性对于市场篮数据和文本文档等稀疏型数据集非常有用，这些**数据集更关注项的共存，而非项共缺席**。



### 三、常用关联规则挖掘算法

#### 3.1 Apriori算法原理及应用

##### 3.1.1 Apriori算法基本原理

​	Apriori算法是关联规则挖掘中最经典的算法之一，其核心思想是通过递归迭代地生成候选频繁项集，并基于一定的支持度阈值筛选出频繁项集，进而生成关联规则。

​	获得频繁项集，最简单直接的方法就是穷举法，但是这种方法计算量过于庞大，如下图所示，k项的数据集可能生成 2^𝑘−1 个项集。

<img src="./img/image-20250311201608432.png" alt="image-20250311201608432" style="zoom: 50%;" />

​	可见穷举法在实际中并不可取。必须设法降低产生频繁项集的计算复杂度。此时可以利用支持度对候选项集进行剪枝，这是Apriori算法所利用的先验原理：

1. **先验性质1**：如果一个项集是频繁项集，则它的所有子项集都是频繁项集。

​	例如：假设一个集合{A,B}是频繁项集，即A、B同时出现在一条记录的次数大于等于最小支持度，则它的子集{A},{B}出现次数必定大于等于最小支持度，即它的子集都是频繁项集。

2. **先验性质2：**如果一个集合不是频繁项集，则它的所有超集都不是频繁项集。

​	例如：假设集合{A}不是频繁项集，即A出现的次数小于最小支持度，则它的任何超集如{A,B}出现的次数必定小于最小支持度，因此其超集必定也不是频繁项集。利用下图我们也能发现，当{A,B}是非频繁集时，就代表所有包含它的超集也是非频繁的，即可以将它们都剪除。

<img src="./img/image-20250311201623672.png" alt="image-20250311201623672" style="zoom:67%;" />



> [!IMPORTANT]
>
> **Apriori算法的基本思路**
>
> Apriori算法的目标是找到最大的K项频繁集。这里有两层意思，首先，我们要找到符合支持度标准的频繁集。但是这样的频繁集可能有很多。第二层意思就是我们要找到最大个数的频繁集。Apriori算法采用了迭代的方法，先搜索出候选1项集及对应的支持度，剪枝去掉低于支持度的1项集，得到频繁1项集。然后对剩下的频繁1项集进行连接，得到候选的频繁2项集，筛选去掉低于支持度的候选频繁2项集，得到真正的频繁二项集，以此类推，迭代下去，直到无法找到频繁k+1项集为止，对应的频繁k项集的集合即为算法的输出结果。
>
> **Apriori算法的基本步骤**
>
> （1）扫描事务集中所有数据，找点所有1项集并进行计数，依据最小支持度筛选出频繁1项集的集合L（1）；
>
> （2）对L（K）执行**连接和剪枝**操作产生候选（k+1）项集的集合C（k+1）；其中，**连接**指的是两个项集对应的项按从小到大顺序排列好，有且仅有最后一项不相同时这两个项集可连接。例如：{a,b}和{a,c}仅有最后一项不等，可进行连接操作，连接后变成{a,b,c}。**剪枝**指的是把不满足最小支持度阈值的项集删除。
>
> （3）对集合C（k+1）计数，并根据最小支持度，筛选出频繁集合L（k+1）；
>
> （4）重复2、3步骤，直至没有新的候选集产生；
>
> （5）根据最小置信度，由频繁项集产生强关联规则，程序结束。

​	以一个简短的事务进行示例。

<img src="./img/image-20250311201638226.png" alt="image-20250311201638226" style="zoom:67%;" />

​	在该事务数据库中，设置**最小支持度为0.25，最小支持度数量为2**进行Apriori算法模拟生成频繁项集，模拟过程见下图所示。

<img src="./img/image-20250311201646223.png" alt="image-20250311201646223" style="zoom:67%;" />

​	生成频繁1-项集 (L1)：根据预设的最小支持度阈值，筛选出满足条件的频繁1-项集。
​	生成候选2-项集 (C2)：将频繁1-项集中的项两两组合，生成候选2-项集，并进行剪枝操作以去除不满足Apriori性质的项集。
​	生成频繁2-项集 (L2)：第二次扫描事务数据库，统计候选2-项集的支持度，并筛选出满足最小支持度阈值的频繁2-项集。
​	生成候选3-项集 (C3)：将频繁2-项集中的项两两组合，生成候选3-项集，并进行剪枝操作。
​	生成频繁3-项集 (L3)：第三次扫描事务数据库，统计候选3-项集的支持度，并筛选出满足最小支持度阈值的频繁3-项集。
​	重复上述过程：继续生成更高阶的候选项集和频繁项集，直到不再有新的频繁项集产生。

##### 3.1.2 Apriori算法优缺点及适用场景

​	**>>Apriori算法优点**

- 简单易懂: Apriori算法基于直观的原理，并且计算过程简单。
- 可扩展性强: 算法可以应用于大规模的数据集。
- 使用先验性质，大大提高了频繁项集逐层产生的效率

​	**>>Apriori算法缺点**

- 候选集计算量大: 在大数据集上，可能需要生成大量的候选项集。
- 需要多次扫描数据: 算法需要多次扫描数据集以计算项集的支持度，这在数据集很大时可能是低效的。
- 不适用于密集数据集：对于非常密集的数据集，Apriori算法可能表现不佳，因为它依赖于逐层搜索策略，可能导致大量的冗余计算。

​	**>>Apriori算法适用场景**

- 事务数据库：Apriori算法最初设计就是为了处理交易数据，如市场篮子分析。这类数据通常表现为事务数据库的形式，每个事务代表一次购买行为，包含一组商品（项）。Apriori算法能够有效地识别出哪些商品经常一起被购买，这对于优化库存管理和营销策略非常有用。
- 稀疏型数据集：由于Apriori算法基于频繁项集的概念，它能有效应对高维稀疏数据，即大部分项集在单个事务中不会出现的情况。这种特性使得它特别适合用于电子商务、社交网络分析等领域，其中用户活动或产品组合往往表现出高度的稀疏性12。
- 短模式的关联规则挖掘：当关注的是较短长度的频繁项集时，Apriori算法可以很好地工作。然而，对于较长的模式，由于其产生的候选项集数量庞大，算法效率会显著降低。因此，在挖掘相对较短的模式时，Apriori算法是一个合适的选择。

##### 3.1.3 Apriori算法的优化

​	**①基于散列的技术**

​	基于散列的技术是通过压缩候选集，减少支持度计数的时间开销。散列表(Hash table)是一种通过映射函数f将每个键映射到一个索引，进而实现高效数据査找和插入的数据结构。映射函数f被称为哈希函数，它将键 key 转换成散列值 f(key)，散列值用于索引表中的存储位置。在散列表中进行査找和插入操作的时间复杂度为 O(1)，相比事务数据库遍历速度更快。

​	**步骤：**

​	1.遍历事务数据库对候选 k项集进行计数，同时生成单个事务中所有的 K+1 项集。

​	2.每个项集进行哈希函数运算，生成相应散列值，按照散列值将其分配到不同桶中。

​	3.从候选k+1项集中删除所有桶计数低于最小支持度的项集，从而减少寻找频繁 k+1项集的计算量。

​	**②事务压缩技术**

​	事务压缩技术通过减少事务的数量，来减少计算候选集支持度的开销，从而提高算法运行效率。如果一个事务不包含任何频繁k项集，那么这个事务也不包含任何频繁k+1项集。在扫描数据库寻找频繁k项集L时，如果识别出一条事务不包含任何频繁 k-1项集，可对这类事务进行标记或删除，后续搜寻L(>k)时也不需要对这类事务进行扫描。

​	**③划分技术**

​	划分技术通过将事务数据库划分为多个分区进行并行计算，避免大规模数据的运算，从而提高计算效率。划分技术先将数据划分为多个分区，若项集A满足sup(A)>min_sup，则在至少一个分区中，项集是频繁的。划分技术基于此原理在各个分区中寻找分区中的频繁项集作为候选项集，并在总数据集上进一步验证候选项集是否频繁。

​	**步骤:**

​	1.将事务数据库划分为n个互不相交的分区，即各个分区不存在相同的事务;

​	2.对各个分区使用Apriori 算法，生成分区中的频繁项集，作为事务数据库中频繁项集的候选项集;

​	3.对事务数据库中事务进行全局遍历扫描，对步骤2中候选项集进行支持度计数，找到事务数据库中所有频繁项集。

​	**④抽样技术**

​	抽样方法是从数据库中随机抽样，并对抽样数据使用Apriori算法得出局部数据中的频繁项集，再使用数据库数据来验证这些频繁项集在全局上是否是频繁的Apriori 的计算效率受事务数据库的规模影响，通过抽样算法能降低挖掘频繁项集过程中的 I/O开销。

​	**步骤：**

​	1.从事务数据库中抽取随机样本S，在S中使用Apriori算法搜索局部频繁项集（一般使用比最小支持度更小的支持度阈值来找出S中的局部频繁项集），作为事务数据库中频繁项集的候选项集，得到局部频繁项集的集合。

​	2.使用事务数据库抽样后余下的数据，对步骤一得到的局部频繁项集的集合中的项集进行支持度计数，得到频繁项集的集合L。

​	**⑤动态项集计数**

​	动态项集计数将数据库划分为若干个区段，逐个扫描区段，并动态添加频项集和生成候选项集，从而减少Apriori 算法过程中的数据库遍历次数。动态项集计数在生成候选项集后，逐个扫描区段来对候选项集进行支持度计数，扫括数个区段后，若候选项集在这些区段的支持度计数已大于最小支持度计数阈值，则添加为频繁项集，并利用新的频繁项集生成新的候选项集，继续扫描下个区段对候选项集进行支持度计数。

##### 3.1.4 基于Apriori算法的购物篮关联记录分析

**>>案例数据**

​	使用Apriori算法对欧洲某超市一个月的销售数据进行购物篮分析，该超市在当月一共售类109种商品，完成9835笔订单，每笔订单包含若干种商品，购物篮部分数据如下表所示。

<img src="./img/image-20250311201657752.png" alt="image-20250311201657752" style="zoom:67%;" />

**>>Apriori算法生成频繁项集**

​	为了从超市商品购买记录中挖掘关联规则，本例首先利用Apriori算法得到商品频繁项集，然后从得到的频繁项集生成关联规则。在实验中，利用mxiend库中封装好的 apriori函数来挖掘频繁项集。apriori函数的使用方法如下:

```
apriori(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0, low_memory=False)
```

​	其中，df是指布尔型数据集。min_support为最小支持度，默认是0.5。 use_colnames是布尔型列名，若为真值则使用列名表示项，反之使用列索引作为列名。max_len是生成频繁项集的最大长度，“None”为默认值，表示不设置频繁项集长度上限，也可取值为大于0的数值作为频繁项集的长度上限。verbose为布尔型变量，默认值“0”表示不输出详细的频繁项集信息。low_memory为布尔型变量，默认值“False”表示不以低内存占用的模式运行算法。

​	设置最小支持度为0.06，运行下面代码可得到所有的频繁项集。

```
import pandas as pd
from mlxtend.frequent_patterns import apriori  # 生成频繁项集
from mlxtend.frequent_patterns import association_rules  # 生成强关联规则

# 1.导入数据
dataSet = pd.read_csv('../data/groceries.csv')

# 2.调用apriori函数，生成频繁项集
if __name__ == '__main__':
    frequent_itemsets = apriori(dataSet, min_support=0.06, use_colnames=True)  # apriori算法生成频繁项集
 
# 3.结果展示
print(frequent_itemsets)
```

​	得到的部分频繁项集如下表所示。

​						<img src="./img/image-20250311201707136.png" alt="image-20250311201707136" style="zoom:67%;" />



**>>Apriori算法生成强关联规则**

​	生成频繁项集后，可以使用 mlxtend库中封装好的 association_rules 函数来生成关联规则:

```
association_rules(df, metric='confidence', min_threshold=0.8, support_only=False)
```

​	其中，df是指数据框形式的数据集。metric示用于筛选关联规则的评估指标，，默认值为“confidence”，常用的度量指标有“support"、“confidence”、“lift”、“leverage”、和“conviction”。min_threshold表示选取的“metric”参数的阈值，默认评估指标“confidence”的默认值为 0.8。support_only为布尔型变量，“True”表示仅计算支持度，其他度量设置为空值，默认值为“False”。

​	设置最小置信度为0.35，运行下面代码可得到所有的强关联规则。

```
import pandas as pd
from mlxtend.frequent_patterns import apriori  # 生成频繁项集
from mlxtend.frequent_patterns import association_rules  # 生成强关联规则

# 1.导入数据
dataSet = pd.read_csv('../data/groceries.csv')

# 2.调用apriori函数，生成频繁项集与关联规则
if __name__ == '__main__':
    frequent_itemsets = apriori(dataSet, min_support=0.02, use_colnames=True) # apriori算法生成频繁项集
    rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.35, num_itemsets='num_itemsets')  # 使用频繁项集生成强关联规则
    
# 3.结果展示
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])
```

​	得到的部分强关联规则如下表所示。

<img src="./img/image-20250311201718104.png" alt="image-20250311201718104" style="zoom:67%;" />

#### 3.2 FP-growth算法原理及应用

##### **3.2.1 FP-growth算法基本原理**

​	  FP-growth算法是一种用于数据挖掘的频繁项集挖掘算法，它比传统的Apriori算法更高效，因为它只需要对数据库进行两次扫描即可完成频繁模式的发现。FP-growth算法的核心在于它使用了一种称为FP树（Frequent Pattern Tree）的数据结构来存储压缩的数据集信息。

**FP树到底长啥样呢？**

​	FP树有五大基本结构：**根节点、项结点、项头表、结点链、父节点**。

- 根结点(Root node)：FP树的根结点代表一个空集。它没有关联的项但是有指向树中每个事务的第一个结点的指针。
- 项结点(Itemnode)：FP树中的每个项结点代表数据集中的一个项。它存储项 ID、数据集中到达该项结点的事务计数、指向相同项ID的下一个项结点(按照结点添加顺序)的结点链。
- 项头表(Header table)：项头表列出数据集中所有频繁项，以及它们的支持度计数，并存储指向该频繁项在 FP树中第一个结点的结点链。
- 结点链(Node link)：结点链是一个指针，连接项头表的每个项与FP树中该项的第一个结点，以及相同D的结点。结点链有助于方便快速地访问树中的项。
- 父结点(Parentnode)与子结点(Child node)：FP树每个项结点为从该点出发的实线箭头的结尾点的父节点，结尾点为该项结点的子结点。父子结点表示两个项在数据集中的至少一条事务中共同出现，用于存储项集在数据库中的关联信息。

​	下面是一个简单的FP树示意图：

<img src="./img/image-20250311201725416.png" alt="image-20250311201725416" style="zoom:67%;" />

> [!IMPORTANT]
>
> **FP-growth算法主要包括两个步骤：构建FP树和从FP树中挖掘频繁项集。**
>
> **构建FP树步骤：**
>
> 1. 统计原始事务集中1项集出现的频率：扫描整个事务数据库，对1项集进行支持度计数。
>
> 2. 支持度过滤：移除那些不满足最小支持度阈值的项，因为不频繁的元素项不会出现在频繁项集中。
>
> 3. 排序：根据项的频率对每个事务中的元素进行排序。这是为了确保FP树的构建过程中，相同的元素项可以共享树的节点。
>
> 4. 构建FP树：使用排序后的事务来构建FP树。如果树中已经有相同的元素项路径，则增加其计数；如果没有，则创建新的节点。
> 5. 重复上述过程，直到读取所有事务。
>
> **从FP树中挖掘频繁项集步骤：**
>
> 1. 抽取条件模式基：:通过项头表和结点链遍历Tree，找到其中每个频繁项的条件模式基;。
>
> 2. 构建条件FP树：使用这些条件模式基来构建条件FP树。
>
> 3. 递归挖掘：递归地重复上述过程，直到每棵树只包含一个元素项为止。在这个过程中，算法会发现所有的频繁项集。

​	以一个简单例子示范，事务数据库如下图所示。

<img src="./img/image-20250311201731887.png" alt="image-20250311201731887" style="zoom:67%;" />

​	设置min sup=25%，min_sup_count=2，扫描数据库，计算每个项的支持度计数，得到sup(c)>sup(a)sup(b)>sup(d)>min_sup，删除事务中的非频繁项，将事务的项按照支持度递减顺序进行重排序，得到下表：

<img src="./img/image-20250311201738809.png" alt="image-20250311201738809" style="zoom:67%;" />

​	为FP树构建名为null的起始结点。第二次扫描数据库，读入 {c,a,b}，创建c、a、和b三个结点,且三个结点的频度计数都为1，构建路径null→(c:1)→(a:1)→(b:1)。而后读入{b}，该项集在FP树中不存在前缀路径，直接创建新结点(b:1)，形成路径null→(b:1)，创建结点链从上一个b结点指向新b结点。再读入{c,a,d}，该项集在FP树中存在前缀路径null→(c:1)→(a:1)，前缀路径上结点的支持度计数增加1，即nu→(c:2)→(a:2)，并创建结点(d:1)作为前缀路径上(a:2)的子结点。读入{c,a}，已存在该路径，直接增加前缀路径null→(c:2)→(a:2)上支持度计数，即null→(c:3)→(a:3)，其他结点的支持度计数不变。最后参照前述步骤依次读入后续项集，直到读完所有事务。

<img src="./img/image-20250311201745089.png" alt="image-20250311201745089" style="zoom:67%;" />

​	由于FP树实质上是由事务生成的路径集合，可以使用FP树中的结点链来遍所包含各个频繁项的路径。通过遍历图结点链而构建的包含各结点的路径如下图所示。

<img src="./img/image-20250311201753761.png" alt="image-20250311201753761" style="zoom:67%;" />

​	FP-growth 算法通过这种方式实现对数据库的分区，并在每个分区中构建不同频繁项的条件模式基，即在 FP树中以某个频繁项为后缀(末结点)的路径集合，如下表所示。

<img src="./img/image-20250311201801083.png" alt="image-20250311201801083" style="zoom:67%;" />

​	上述表中，null→c→a→b:1表示路径的支持度计数为1，从项d的条件模式基构建条件FP树时，结点b的计数为1，不满足最小支持度计数，于是将结点b从该条件FP-树中删除。FP-growth算法在条件模式基上，构造条件数据库(又称条件子树)，每个条件子树关联一个频繁模式，仅需分别对这些条件数据库进行挖掘，将挖掘出的频繁项与“模式段”组合，可以构成长模式的频繁项集。各个项的条件模式基与关联的条件FP树如下表所示。

<img src="./img/image-20250311201812991.png" alt="image-20250311201812991" style="zoom:67%;" />

​	以d点为例，设置最小支持度数量为2，从项 d关联的条件 FP-树挖掘频繁项集的过程如下:

​	(1) 从结点d的条件模式基构造条件FP-树，将不能满足最小支持度值要求的结点删除，结点c、a的支持度计数为2，结点b的支持度计数为1，所以结点c、a都保留，结点b将直接被删除；

​	(2) 项d关联的条件 FP-树中频繁项为c:2、a:2，与后缀d构成频繁项集{c,d}:2、{a,d}:2;

​	(3) {c,d} 的条件模式基为空，以{c,d}为后缀的频繁项集已全部挖掘;

​	(4) 挖掘以{a,d}为后缀的频繁项集。项集{a,d}的条件模式基为null→c:2，关联的条件FP-树为null→c:2，条件FP-树中有仅有一个项c且满足最小支持度阈值，所以{c,a,d}为频繁项集;

​	(5) 此时以 d为后缀的频繁项集已全部找到。其他节点重复上述过程即可。

##### **3.2.2 FP-growth优缺点及适用场景**

​	**>>FP-growth算法优点**

- 挖掘效率高:仅需进行两次数据库遍历，避免了产生长候选项集，能够减小搜索空间。
- 适应性强: FP-growth 对不同长度的项集都有很好的适应性。

​	**>>FP-growth算法缺点**

- 不适用于稀疏数据: 当数据集较为稀疏时，FP树的压缩效果较差，算法效率会大幅度下降。
- 不适用于大型数据集：在大型数据库中要生成数量庞大的条件FP树，会耗费大量的时间和空间。

​	**>>FP-growth算法适用场景**

- 密集型数据库：FP-Growth算法通过构建FP树，能够有效地压缩频繁项集的数据结构。对于那些具有高密度、高重复性的交易数据集，FP-Growth能够显著减少内存使用和计算时间。
- 单维布尔关联规则挖掘：在需要从二值化数据中挖掘关联规则的场景下，例如市场篮子分析，FP-Growth表现尤为出色。在这种情况下，每个商品的存在与否可以用布尔值表示，简化了数据结构，提高了算法效率。
- 大型但相对简单的事务数据库：对于那些包含大量简单事务的数据库，即每个事务涉及的商品或项目的数量有限，FP-Growth可以通过其独特的树结构优化频繁模式的发现过程。
- 需要快速响应的实时系统：如果应用场景要求对频繁模式进行快速检测并及时响应，比如推荐系统中的实时个性化推荐，FP-Growth因其高效的模式匹配能力而成为理想选择。

##### **3.2.3 基于FP-growth算法的**消费者购物时间偏好关联分析

**>>案例数据**

​	利用某购物网站的消费者购物时间数据为例，使用FP-growth 算法发现消费者的偏好购物时间之间的关联规律。该数据集共1572599条数据，记录了206209位消费者分别在0~23点的购物次数。在实验过程中，为每位消费者构造一条数据，记录用户的购物时间偏好(购物次数超过总购物次数的10%的时刻集合)，并编码成布尔型数据集见下表所示。

<img src="./img/image-20250311201820485.png" alt="image-20250311201820485" style="zoom:67%;" />

**>>FP-growth算法生成频繁项集**

​	利用 mlxtend库中的fp-growth 函数来挖掘频繁项集，设置最小支持度为0.03，具体代码如下：

```
from mlxtend.frequent_patterns import fpgrowth
from mlxtend.frequent_patterns import association_rules
import pandas as pd

# 1.导入数据
data = pd.read_csv('../data/user_orders_hourofday.csv')

# 2.调用fpgrowth函数, 生成频繁项集与关联规则
frequent_itemsets = fpgrowth(data, min_support=0.03, use_colnames= True)  # 生成频繁项集

# 3.结果展示
print(frequent_itemsets)
```

​	运行上述代码，可得到部分频繁项集如下：

<img src="./img/image-20250311201827157.png" alt="image-20250311201827157" style="zoom:67%;" />

**>>FP-growth算法生成强关联规则**

​	设置最小置信度为0.35，提升度为1.2，从消费者购物时间数据中挖掘的频繁项集生成满足阈值的所有强关联规则及对应的支持度、置信度、提升度信息，代码如下：

```
from mlxtend.frequent_patterns import fpgrowth
from mlxtend.frequent_patterns import association_rules
import pandas as pd

# 1.导入数据
data = pd.read_csv('../data/user_orders_hourofday.csv')

# 2.调用fpgrowth函数, 生成频繁项集与关联规则
frequent_itemsets = fpgrowth(data, min_support=0.03, use_colnames= True)  # 生成频繁项集
rules = association_rules(frequent_itemsets,metric = 'confidence', min_threshold = 0.35,num_itemsets='num_itemsets')  # 生成关联规则
rules = rules[rules['lift']>=1.2]  #设置提升度阈值，筛选关联规则

# 3.结果展示
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])
```

​	生成的关联规则如下表所示。

<img src="./img/image-20250311201832990.png" alt="image-20250311201832990" style="zoom:67%;" />

​	这些关联规则表明消费者购物时间与相邻时间之间存在较强的关联关系，常在上午某个时刻购物的消费者往往也倾向于在其他上午时刻购物，常在下午某个时刻购物的消费者也表现出常在其他下午时刻购物的行为。同时，上午和下午的购物时刻之间并没有表现出较强的关联关系。上述分析结果表明消费者购物时间在不同时间段存在个人偏好，这对于进行用户细分，以及利用用户偏好来设置广告推送、分时营销等具有非常有意义的应用。

#### 3.3 Eclat算法原理及应用

##### 3.3.1 Eclat算法基本原理

​	Eclat算法加入了**倒排**的思想，将事务数据集中的项作为索引，每个项对应的事务ID作为索引值，**采用垂直数据格式**。只需对数据进行一次扫描，算法的运行效率很高。

<img src="./img/image-20250311201839137.png" alt="image-20250311201839137" style="zoom:67%;" />

> [!IMPORTANT]
>
>  **Eclat算法生成频繁项集的基本步骤**
>
> 1. 扫描数据库，将水平数据格式转化为垂直数据格式;
> 2. 使用项的TID集计算项的支持度，过滤掉不满足最小支持度的项集，得到频繁1项集和对应的TID集，即L1;
> 3. 频繁k项集及其对应的TID集为Lk，对Lk中两个前k-1个项相同的项集求并集，对应的TID集求交集，得到候选k+1项集及其对应的TID集。例如，{a,b}:{1,3,5,6}与{a,c}:{1,3,4,6,7} 中项集合并产生候选项集{a,b,c}，TID集的交集为{ 1,3,6}，根据TID集可以得到{a,b,c}的支持度计数为 3；
> 4. 对候选k+1项集的TID集中的事务数进行计数，如果大于最小支持度，则该候选项集为频繁项集，得到L（k+1）；
> 5. 重复步骤2和步骤 3，直到找不到其他的频繁项集。

​	以上一个图为例，设置最小支持度为2，扫描数据库，将事务转化为垂直数据格式，得到的候选1项集与对应的TID集，如上图(b)所示。

​	计算候选1项集的支持度。根据候选1项集对应的TID集中的元素个数计算项集支持度，消除不满足最小支持度数量的候选1项集，得到频繁1项集的集合L1={{a},{b},{c},{d}};

​	对频繁1项集及其TID集分别执行并交集操作生成候选2项集与对应的TID集。如下表所示，候选2项集(b,d}对应的TID集只含有1个事务，不满足最小支持度数量，因此不是频繁项集，其他项集为频繁项集;

<img src="./img/image-20250311201848739.png" alt="image-20250311201848739" style="zoom:67%;" />

​	频繁2项集及其TID集分别执行并交集操作生成候选3项集与对应的TID集。如下表所示，候选3项集{a,b,d}为非频繁项集，其他候选项集为频繁项集;

<img src="./img/image-20250311201857547.png" alt="image-20250311201857547" style="zoom:67%;" />

​	使用频繁3项集生成候选4项集。由于候选集为空，Eclat算法终止:

##### 3.3.2 Elact算法的优缺点及适用场景

Eclat算法使用垂直数据格式，由于每个候选k项集的TIDset携带了计算支持度所需的完整信息，Eclat 算法通过对候选项集的TIDset中的元素直接计数来确定候选项集是否为频繁项集，无需像 Aprion算法执行多次扫描事务数据库的操作但 Eclat 算法生成候选项集时，需要先判断两个项集是否满足连接条件，当项集的长度较长时，连接条件判断的时间也会增长，两个项集执行并集操作后，对应的两个TID 集需要进行取交集操作，增加了时间消耗，如果生成的不是频繁项集，长集合的交运算仍然运算时间较长，这使得Bclat算法更适合挖掘短模式	**>>Elact算法优点**

- 高效性：通过垂直数据表示和逐层遍历，Eclat算法能够显著降低时间复杂度，提高频繁项集挖掘的效率。
- 可扩展性：基于前缀的等价关系将搜索空间划分为较小的子空间，使得算法能够处理大规模数据集。
- 灵活性：算法支持不同的支持度阈值设置，可以根据实际需求进行调整。

​	**>>Elact算法缺点**

- 不适用于稀疏数据：由于需要频繁进行事务ID列表的交集操作，如果数据集中的项目分布非常稀疏，这种交集操作会变得非常耗时。
- 不适用于长模式数据集：生成候选项集时，要先判断两个项集是否满足连接条件，并对对应的两个TID集需要进行取交集操作，增加了时间消耗。

​	**>>Elact算法适用场景**

- 中小型数据集：对于那些事务数量相对较少且项目分布较为密集的数据集，Eclat算法能够有效地发现频繁项集，并且相比Apriori算法，它可以减少生成候选集的开销。
- 需要快速响应的小规模应用：在一些实时性要求较高的小规模应用场景中，Eclat算法可以提供较快的响应时间，因为它避免了像Apriori那样生成大量候选集的过程。
- 垂直数据格式：Eclat算法采用垂直数据格式（即每个项目对应一个事务ID列表），这使得它在处理某些特定类型的数据集时更加高效，特别是当项目的支持度较高时。

##### 3.3.3 基于Eclat算法糖尿病症状关联分析

**>>案例数据**

​	糖尿病患者症状数据集，对患者的疾病症状数据进行关联规则挖掘，研究哪些症状与糖尿病有密切联系，以及症状间是否存在并发现象，具体数据集见下表所示：

<img src="./img/image-20250311201905053.png" alt="image-20250311201905053" style="zoom:67%;" />

​	上表字段含义如下表所示：

<img src="./img/image-20250311201912604.png" alt="image-20250311201912604" style="zoom:67%;" />

**>>Eclat算法生成频繁项集**

​	验调用 pyEclat包中的 Eclat函数生成频繁项集，代码如下：

```
eclat instance =ECLAT(data=dataframe, verbose=True)
get_ECLAT_indexes, get_ECLAT_supports =eclat_instance.fit(min_support=8.08, min_combination=1,max_combination=3,separator='&',verbose=True)
```

​	其中，“data”是 DataFrame格式的数据集，每行表示1条事务。“verbose”表示是否显示具体进度，默认值“True”表示显示频繁项集挖掘的具体进度，“False”表示不显示具体进度。“min support”为最小支持度。“min_combination”和“max_combination”分别表示生成项集的最小和最大长度，默认值分别为“1”和“3”。“separator”表示项集中项的连接符号，为字符串格式，默认值为“&”。“get_ECLAT _supports ”是以字典格式存储的频繁项集及其对应支持度，键为频繁项集，值为对应的支持度。“get_ECLAT_indexes”是以字典格式存储的频繁项集及其对应 TID集，键为频繁项集，值为对应的 TID 集。

​	设置最小支持度为0.25，对糖尿病特征数据使用Eclat 算法，具体代码如下

```
import pandas as pd
from pyECLAT import ECLAT 
from mlxtend.frequent_patterns import association_rules

# 1.导入数据，并将其处理为特定格式
Data = pd.read_csv('../data/symptoms_of_diabetes_patients.csv')
Data.columns = range(len(Data.columns))
eclat_instance = ECLAT(data = Data, verbose=True) 

# 2.调用eclat_instance.fit生成频繁项集
get_ECLAT_indexes, get_ECLAT_supports = eclat_instance.fit(min_support=0.25, min_combination=1, max_combination=4, separator=' & ',verbose=True)

eclat_instance.df_bin # 数据展示

# 3.生成关联规则
items_list = [frozenset(element.split(' & ')) for element in list(get_ECLAT_supports.keys())]
frequentsets = pd.DataFrame(list(zip(list(get_ECLAT_supports.values()), items_list)), columns=['support', 'itemsets']) # 将频繁项集处理成特定格式

# 4.结果展示
print(frequentsets)
```

​	运行上述代码，得到部分频繁项集如下：

<img src="./img/image-20250311201920390.png" alt="image-20250311201920390" style="zoom:67%;" />

**>>Eclat算法生成强关联规则**

​	设定关联规则的置信度阈值为0.8，运行下面的代码，可得到关联规则及对应的支持度、置信度、提升度信息：

```
import pandas as pd
from pyECLAT import ECLAT 
from mlxtend.frequent_patterns import association_rules

# 1.导入数据，并将其处理为特定格式
Data = pd.read_csv('../data/symptoms_of_diabetes_patients.csv')
Data.columns = range(len(Data.columns))
eclat_instance = ECLAT(data = Data, verbose=True) 

# 2.调用eclat_instance.fit生成频繁项集
get_ECLAT_indexes, get_ECLAT_supports = eclat_instance.fit(min_support=0.25, min_combination=1, max_combination=4, separator=' & ',verbose=True)
eclat_instance.df_bin # 数据展示

# 3.生成关联规则
items_list = [frozenset(element.split(' & ')) for element in list(get_ECLAT_supports.keys())]
frequentsets = pd.DataFrame(list(zip(list(get_ECLAT_supports.values()), items_list)), columns=['support', 'itemsets']) # 将频繁项集处理成特定格式
rules = association_rules(frequentsets, metric="confidence", min_threshold=0.8)

# 4.结果展示
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])
```

​	运行上述代码，可得如下部分强关联规则：

<img src="./img/image-20250311201926506.png" alt="image-20250311201926506" style="zoom:67%;" />



#### 3.4 H-mine算法原理及应用  

##### 3.4.1 H-mine算法基本原理

​	H-mine算法的核心思想是利用一种新的数据结构——**超链接数据结构(H-struct )**，来替代传统的FP-tree或事务ID列表。H-struct 能够有效地压缩数据，并通过递归的方式发现频繁项集。

​	**H-struct 由项头表、频繁项投影、超链接**等组成。

- 项头表(Header table)：项头表列出数据集中所有频繁项，以及它们的支持度计数，并存储指向该频繁项在 H-struct中第一个结点的超链接。
- 频繁项投影(Frequent-item projections oftansaction)：由一系列频繁项组成，对应数据集中的一条事务，由事务剔除非频繁项得到。
- 超链接(Hyper-link)：用于链接首项相同的频繁项投影使其形成队列，项头表中的对应项作为队列头。

​	以下表事务数据库为例构建的H-struct。

​										<img src="./img/image-20250311201933227.png" alt="image-20250311201933227" style="zoom:67%;" />		

​	(1)第一次扫描数据库，找到所有频繁1项集，并生成事务的频繁项集投影。最小支持度数量分别设定为25%和50%时，事务的频繁项集投影见下表第三列和第四列，频繁项与对应的支持度计数为{a:4,b:4,c:6,d:3}，a:4表示a的支持度计数为 4。

<img src="./img/image-20250311201941103.png" alt="image-20250311201941103" style="zoom:67%;" />

​	(2)设定min_sup为 25%，继续构建 H-struct。按照频繁项的某种**固定顺序(F-list)**将事务中项重新排序。例如，在本例中，将事务中的项按照字典顺序a-b-c-d排序。

​	(3)第二次扫描事务数据库，根据事务的频繁项集投影建立 H-struct。首项相同的频繁项集投影通过超链接形成队列，而这个队列的头结点就是项头表H中的项。例如，在头表H中的项a是a-队列的头结点，它链接了事务1、3、4和7的频繁项投影，因为这三个投影的前缀均为a-队。类似地，事务2、5被链接到b-队。事务6被链接到c-队列;事务8被链接到d-队列。

​	(4)最终生成的H-struct见下图所示。

<img src="./img/image-20250311201947330.png" alt="image-20250311201947330" style="zoom:67%;" />

> [!IMPORTANT]
>
> **H-mine 算法生成频繁项集的步骤**
>
> 1. 初始化处理顺序：根据预设的“F-list”顺序（依次处理每个频繁项。对每个项构成的初始项集，执行后续步骤以生成更复杂的组合。
> 2. 生成扩展频繁项集：以当前项集为基础，从项头表（H）中按F-list顺序筛选位于当前项右侧的后续项。将这些后续项依次与当前项集合并，形成更大的频繁项集，确保新项的顺序符合F-list规则，避免重复组合。
> 3. 构建动态索引结构：为当前项集创建一个索引表（HI），用于记录相关数据的位置。扫描所有包含当前项集的事务记录，若某条记录中首次出现后续项，则通过指针将该记录链接到该项对应的队列中，实现快速定位。
> 4. 递归扩展与索引更新：对步骤2生成的新项集递归执行步骤2~4：扩展：继续寻找后续项，生成更大项集。索引维护：更新索引表（HI），将新记录动态链接到对应项的位置，确保后续操作能高效访问相关数据。通过递归不断“生长”项集，直到无法生成更大的频繁组合。
> 5. 回溯与全局遍历：完成当前项集的所有扩展后，回溯到最初的索引表（H），遍历剩余的未处理记录。将这些记录链接到下一顺序项的组合中，确保算法覆盖所有可能的频繁项集，避免遗漏。
>

​	以上图为例，设定最小支持度为25%，最小支持度数量为2，使用 H-mine 算法从 H-stnuct上挖掘所有包含a的频繁项集的过程如下:

​	(1)扫描 a-队列，对H,中各项进行支持度计数，将频繁项与a组合成频繁2项集。遍历队列，得到项及其支持度计数为(b:2，c:4，d:2}都超过了最小支持度数量，因此输出频繁项集{a,b}、{a,c}、{a,d}，此时已输出所有包含a的频繁2项集;

​	(2)为项a构建项头表H，并将a-队列中频繁项投影链接到H中的队列项头表Ha，如下图所示，用于存储 a-队列中的项及支持度信息，Ha中项b链接事务1、7，项c链接事务 3;

<img src="./img/image-20250311201955142.png" alt="image-20250311201955142" style="zoom:67%;" />

​	(3)挖掘所有包含 a、b的频繁项集。建立一个项头表Hab。，如下图 所示。在ab-队列中只有c是频繁项，输出频繁项集(a,b,c}。H中c之后的项都不是频繁项，这条路径上的频繁项集已全部被输出，无需继续挖掘:

<img src="./img/image-20250311202001445.png" alt="image-20250311202001445" style="zoom:67%;" />

​	(4)调整 ab-队列中事务的超链接，ab-队列中的每个投影加入该投影中项b的下一个项的队列中。事务1、7都链接到ac-队列，如下图所示;

<img src="./img/image-20250311202007949.png" alt="image-20250311202007949" style="zoom:67%;" />

​	(5)挖掘包含 a、c，且不包含6的频繁项集。扫描 ac-队列，d 为频繁项，输出频繁项集{a,c,d}。d没有任何下一级项，所以无需挖掘 ad-队列。此时认为所有包含a的频繁项集已经被找到;
​	(6)调整超链接，更新队列。遍历一次a-队列，队列中的每个事务都被添加到投影中项a的下一个项的队列中。例如，频繁项投影1:{a,b,c}与7:{a,b,c,d}被插入到 b-队列中，3:{a,c,d}与 4:{a,c}被插入到 c-队列中，得到更新超链接的 H-struct 如下图所示。

<img src="./img/image-20250311202013557.png" alt="image-20250311202013557" style="zoom:67%;" />

##### 3.4.2 H-mine算法的优缺点及适用场景

​	**>>H-mine算法优点**

- 空间效率高：H-mine算法具有多项式空间复杂度，尤其在挖掘稀疏数据库时比FP-growth等方法更节省空间。它通过构建项头表来存储频繁项、支持度信息和超链接信息，占用空间更少。
- 快速运行速度：由于采用了动态调整链接的方法，H-mine能够实现小而可精确预测的空间开销，并且在执行过程中运行速度快。
- 低内存需求：只需少量内存即可构建项头表，对于资源有限的环境特别有利。
- 良好的扩展性：面对大型数据库时，通过数据库分区的方法解决内存需求高的问题，在挖掘大型数据库时展现出良好的可扩展性。

​	**>>H-mine算法缺点**

- 密集数据处理性能下降：当处理非常密集的数据库时，由于H-struct中频繁项之间缺乏直接纵向链接，必须从头到尾遍历事务来查找项间的关联信息，这会导致性能大幅下降。
- 内存要求较高：尽管针对大型数据库提出了分区解决方案，但面对极其庞大的数据集时，仍需要较高的内存来读取事务至内存中进行处理。

​	**>>H-mine算法适用场景**

- 稀疏数据库挖掘：鉴于其高效的空间利用和处理能力，H-mine算法非常适合用于稀疏数据库的关联规则挖掘。
- 大规模数据集：对于拥有大量事务的数据集，尤其是那些事务数量庞大但每笔交易中的项目数相对较少的情况，H-mine表现出色。

##### 3.4.3 基于H-mine算法的新闻推荐

**>>案例数据**

​	利用匈牙利在线新闻门户的点击流数据集来挖掘频繁项集，该数据集共包含990002条数据，每条数据是一个用户每次浏览网站点击的一系列新闻编号，部分数据如下表所示。

<img src="./img/image-20250311202021524.png" alt="image-20250311202021524" style="zoom:67%;" />

**>>H-mine算法生成频繁项集**

​	利用 mlxtend 库中封装好的 hmine 函数来挖掘频繁项集，具体代码如下：

```
hmine(df,min_support=0.5,use_colnames=False, max_len=None, verbose=0)
```

​	设置最小支持度为1%，运行下面代码，可得所有频繁项集：

```
import pandas as pd
import ast
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import hmine
from mlxtend.frequent_patterns import association_rules

# 1.数据导入
data = pd.read_csv('../data/kosarak.csv')
retrieved_data = [ast.literal_eval(row) for row in data['data']]

# 2.数据热编码，并使用稀疏表示
te = TransactionEncoder()
oht_ary = te.fit(retrieved_data).transform(retrieved_data, sparse=True)
sparse_df = pd.DataFrame.sparse.from_spmatrix(oht_ary, columns=te.columns_)

# 3.调用hmine函数，生成频繁项集和关联规则
frequent_pattern = hmine(sparse_df, min_support=0.01, use_colnames=True, verbose=0)

# 4.结果展示
print(frequent_pattern)
```

​	得到的部分频繁项集如下表所示：

<img src="./img/image-20250311202027542.png" alt="image-20250311202027542" style="zoom:67%;" />

**>>H-mine算法生成强关联规则**

​	设置置信度阈值为0.85，提升度值为1，运行下述代码，可得强关联规则：

```
import pandas as pd
import ast
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import hmine
from mlxtend.frequent_patterns import association_rules

# 1.数据导入
data = pd.read_csv('../data/kosarak.csv')
retrieved_data = [ast.literal_eval(row) for row in data['data']]

# 2.数据热编码，并使用稀疏表示
te = TransactionEncoder()
oht_ary = te.fit(retrieved_data).transform(retrieved_data, sparse=True)
sparse_df = pd.DataFrame.sparse.from_spmatrix(oht_ary, columns=te.columns_)

# 3.调用hmine函数，生成频繁项集和关联规则
frequent_pattern = hmine(sparse_df, min_support=0.01, use_colnames=True, verbose=0)
rules = association_rules(frequent_pattern ,metric = 'confidence',min_threshold = 0.85)
rules = rules[rules['lift'] >= 1]

# 4.结果展示
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])
```

​	得到部分强关联规则如下表所示：

<img src="./img/image-20250311202033124.png" alt="image-20250311202033124" style="zoom:67%;" />

​	关联规则 0:(1，148)→(11)的置信度达到了0.867，提升度为2.360，说明点击新闻 1、148 的用户中有 86.7%也点击了新闻11，比其他用户点击新闻11的可能性高出136.0%，新闻(1,148)与新闻(11)表现出较强的关联关系；关联规则 271:(87)→(7)的置信度为 0.927，提升度 10.169，这个关联规则表明如果用户点击新闻(87)，则其点击新闻(7)的概率页显著提升，因此在进行新闻推荐时，当用户已有新闻(87)的点击记录时，即可向用户推荐新闻(7)。利用关联规则挖掘的 273条关联规则，网站可对用户进行有效的新闻推荐。

#### 3.5 四类算法综合对比

​                       <img src="./img/image-20250311202103235.png" alt="image-20250311202103235" style="zoom:67%;" /> 

### 四、关联规则挖掘应用拓展

#### 4.1 关联规则可视化

​	关联规则经常可视化为**网络图，散点图，热力图**三类格式。

- 网络图：网络图是一种常用的可视化方法，用于展示项集之间的关联关系。在网络图中，每个项集表示为一个节点，节点之间的连线表示关联规则。连线的粗细可以表示关联规则的强度（如支持度、置信度或提升度）。
- 散点图：将支持度、置信度作为行和列，使用颜色或大小来表示关系的强度。

- 热力图：热力图是一种矩阵形式的可视化方法，用于展示项集之间的关联强度。在热力图中，每个单元格表示两个项集之间的关联强度，单元格的颜色深浅表示关联强度的大小。类似于矩阵图。

​	以3.1.4节案例数据为例，设置最小支持度为0.02，最小置信度为0.25，利用Apriori算法进行关联规则挖掘可得如下部分强关联规则。

<img src="./img/image-20250311202122536.png" alt="image-20250311202122536" style="zoom:67%;" />

​	将其可视化为网络图，设置连线粗细表示提升度，运行下述代码：

```
plt.figure(figsize=(12, 8))
G = nx.DiGraph()

for i in range(len(rules)):
    antecedent = ','.join(list(rules['antecedents'][i]))
    consequent = ','.join(list(rules['consequents'][i]))
    lift = round(rules['lift'][i], 2)
    G.add_edge(antecedent, consequent, weight=lift)

pos = nx.spring_layout(G, k=0.3)  # positions for all nodes
nx.draw_networkx_nodes(G, pos, node_size=700)
nx.draw_networkx_edges(G, pos, width=2, edge_color='b')
nx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')

# 设置边上的标签并保留两位小数
edge_labels = {(u, v): d['weight'] for u, v, d in G.edges(data=True)}
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)

plt.title('Association Rules Network')
plt.show()
```

<img src="./img/image-20250311202132889.png" alt="image-20250311202132889" style="zoom:67%;" />

​	将其可视化为散点图，设置点大小表示提升度，运行下面的代码：

```
plt.figure(figsize=(12, 8))
sns.scatterplot(x='support', y='confidence', size='lift', data=rules)
plt.title('Support vs Confidence with Lift Size')
plt.xlabel('Support')
plt.ylabel('Confidence')
plt.legend(title='Lift')
plt.show()
```

<img src="./img/image-20250311202140494.png" alt="image-20250311202140494" style="zoom:67%;" />

​	将其可视化为热力图，设置颜色表示提升度，运行下面的代码：

```
# 准备数据：构造一个矩阵，其中前件和后件作为行列，提升度作为值
pivot_rules = rules.pivot(index='antecedents', columns='consequents', values='lift')

# 将frozenset转换为字符串
pivot_rules.index = [', '.join(list(x)) for x in pivot_rules.index]
pivot_rules.columns = [', '.join(list(x)) for x in pivot_rules.columns]

plt.figure(figsize=(15, 10))
sns.heatmap(pivot_rules, annot=True, cmap="YlGnBu", linewidths=.5)
plt.title('Lift Heatmap of Association Rules')
plt.show()
```

<img src="./img/image-20250311202147932.png" alt="image-20250311202147932" style="zoom:67%;" />

​	关联规则可视化全部代码如下：

```
from mlxtend.preprocessing import TransactionEncoder
import seaborn as sns
import networkx as nx
import pandas as pd
from mlxtend.frequent_patterns import apriori  # 生成频繁项集
from mlxtend.frequent_patterns import association_rules  # 生成强关联规则
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体作为默认无衬线字体（支持中文）
plt.rcParams['axes.unicode_minus'] = False    # 解决保存图像时负号'-'显示为方块的问题

# 1.导入数据
dataSet = pd.read_csv('../data/groceries.csv')

# 2.调用apriori函数，生成频繁项集与关联规则
if __name__ == '__main__':
    frequent_itemsets = apriori(dataSet, min_support=0.02, use_colnames=True) # apriori算法生成频繁项集
    rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.25, num_itemsets='num_itemsets')  # 使用频繁项集生成强关联规则

# 3.结果展示
print(frequent_itemsets)
print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

# 网络图
plt.figure(figsize=(12, 8))
G = nx.DiGraph()

for i in range(len(rules)):
    antecedent = ','.join(list(rules['antecedents'][i]))
    consequent = ','.join(list(rules['consequents'][i]))
    lift = round(rules['lift'][i], 2)
    G.add_edge(antecedent, consequent, weight=lift)

pos = nx.spring_layout(G, k=0.3)  # positions for all nodes
nx.draw_networkx_nodes(G, pos, node_size=700)
nx.draw_networkx_edges(G, pos, width=2, edge_color='b')
nx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')

# 设置边上的标签并保留两位小数
edge_labels = {(u, v): d['weight'] for u, v, d in G.edges(data=True)}
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)

plt.title('Association Rules Network')
plt.show()

# 散点图
plt.figure(figsize=(12, 8))
sns.scatterplot(x='support', y='confidence', size='lift', data=rules)
plt.title('Support vs Confidence with Lift Size')
plt.xlabel('Support')
plt.ylabel('Confidence')
plt.legend(title='Lift')
plt.show()

# 热力图
# 准备数据：构造一个矩阵，其中前件和后件作为行列，提升度作为值
pivot_rules = rules.pivot(index='antecedents', columns='consequents', values='lift')

# 将frozenset转换为字符串
pivot_rules.index = [', '.join(list(x)) for x in pivot_rules.index]
pivot_rules.columns = [', '.join(list(x)) for x in pivot_rules.columns]

plt.figure(figsize=(15, 10))
sns.heatmap(pivot_rules, annot=True, cmap="YlGnBu", linewidths=.5)
plt.title('Lift Heatmap of Association Rules')
plt.show()
```

#### 4.2 关联规则与网络分析

​	关联规则结合网络分析的应用前沿在多个领域展现出了强大的潜力，特别是在文本挖掘、社交网络分析、生物信息学等领域。其中，利用高频关键词构建关联规则并形成关键词网络是其中的一个具体应用方向。下面将详细介绍这一领域的几个关键点：

1. **高频关键词提取**

​        首先需要从大量文本数据中提取出高频关键词。这通常涉及到自然语言处理技术，如分词、词频统计、主题模型等方法来识别文档集合中的重要术语或短语。

​	以下面一个简单的新闻数据集为例：

<img src="./img/image-20250311202156233.png" alt="image-20250311202156233" style="zoom:67%;" />

​	首先，进行分词和去停用词及词频统计，筛选出每篇文档出现频次大于5的高频词:

```
# 读取Excel文件
file_path = '../data/ces.xlsx'
df = pd.read_excel(file_path)

# 加载停用词表
stopwords_path = 'stopwords.txt'
stopwords = set(pd.read_csv(stopwords_path, header=None, names=['word'])['word'])

# 分词并去除停用词
def preprocess_text(text, stopwords):
    words = jieba.lcut(str(text))
    filtered_words = [word for word in words if word not in stopwords and len(word) > 1]
    return filtered_words

# 提取第三列“内容”
texts = df['内容'].tolist()

# 构建交易数据集
transactions = []
for idx, text in enumerate(texts):
    transaction = preprocess_text(text, stopwords)
    transactions.append(transaction)

    # 输出每个文本的高频词列表
    word_freq = Counter(transaction)
    high_freq_words = {word: freq for word, freq in word_freq.items() if freq > 5}
    print(f"Text {idx + 1} High Frequency Words: {high_freq_words}")

# 统计所有词汇的频率并筛选高频词
all_words = [word for transaction in transactions for word in transaction]
word_freq = Counter(all_words)
high_freq_words = {word: freq for word, freq in word_freq.items() if freq > 5}
```

​	输出部分文本高频词见下图所示：

<img src="./img/image-20250311202203926.png" alt="image-20250311202203926" style="zoom:67%;" />

2.**关联规则挖掘**

​	一旦确定了高频关键词，就可以运用关联规则挖掘算法（如Apriori、FP-Growth、Eclat等）来发现这些关键词之间的潜在关系。例如，在学术文献中，可能会发现某些关键词经常一起出现，这可能暗示了特定的研究趋势或者理论联系。

​	使用apriori算法进行关联规则挖掘，设置最小支持度和置信度为0.5和0.6的强关联规则：

```
# 使用TransactionEncoder编码交易数据
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df_te = pd.DataFrame(te_ary, columns=te.columns_)

# 应用Apriori算法
frequent_itemsets = apriori(df_te, min_support=0.5, use_colnames=True)
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=0.6,num_itemsets='num_itemsets')

# 输出构建的关联规则
print("\nAssociation Rules:")
for idx, row in rules.iterrows():
    antecedents = list(row['antecedents'])
    consequents = list(row['consequents'])
    support = row['support']
    confidence = row['confidence']
    lift = row['lift']
    print(
        f"Rule {idx + 1}: {antecedents} -> {consequents}, Support: {support:.4f}, Confidence: {confidence:.4f}, Lift: {lift:.4f}"
    )
```

​	得到部分关联规则见下图所示：

<img src="./img/image-20250311202211322.png" alt="image-20250311202211322" style="zoom:67%;" />

3.**高频词关联规则网络分析**

​	可视化挖掘得到的关联规则为网络图。在这个网络中，节点代表关键词，边表示关键词间的关联强度（例如支持度、提升度等）。通过分析这个网络，研究者可以获得关于关键词之间关系的直观理解，并且能够识别出核心概念和新兴趋势。

​	以提升度为边，边粗细代表提升度大小，可视化上述关联规则，可得到下面的高频词关联规则网络图：

```
# 可视化高频词关联规则网络
G = nx.DiGraph()

# 添加节点和边，并将提升度作为边的权重
edge_weights = {}  # 存储边及其对应的提升度
for _, row in rules.iterrows():
    antecedents = list(row['antecedents'])
    consequents = list(row['consequents'])
    support = row['support']
    confidence = row['confidence']
    lift = row['lift']

    for antecedent in antecedents:
        G.add_node(antecedent)
    for consequent in consequents:
        G.add_node(consequent)

    for antecedent in antecedents:
        for consequent in consequents:
            G.add_edge(antecedent, consequent, weight=lift)
            edge_weights[(antecedent, consequent)] = lift

# 设置中文字体
plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体
plt.rcParams['axes.unicode_minus'] = False  # 解决负号'-'显示为方块的问题

# 调整布局以减少重叠
pos = nx.spring_layout(G, k=0.8, iterations=50)  # 增加k值以增大节点间距，增加迭代次数以优化布局

# 计算边的宽度，使提升度越大，边越粗
min_lift = min(edge_weights.values())
max_lift = max(edge_weights.values())
edge_widths = {edge: (lift - min_lift) / (max_lift - min_lift) * 5 + 1 for edge, lift in edge_weights.items()}  # 归一化处理后乘以系数

# 绘制网络图
plt.figure(figsize=(18, 12))  # 增大图表尺寸
nx.draw_networkx_nodes(G, pos, node_size=20)  # 增大节点大小

# 根据提升度设置边的宽度
edges = G.edges()
weights = [edge_widths[edge] for edge in edges]
nx.draw_networkx_edges(G, pos, edgelist=edges, width=weights, edge_color='b')

# 增大字体大小
nx.draw_networkx_labels(G, pos, font_size=18, font_family="SimHei")  # 使用黑体并增大字体

# edge_labels = {(u, v): f"{d['weight']:.2f}" for u, v, d in G.edges(data=True)}
# nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_family="SimHei", font_size=12)  # 使用黑体并增大字体

plt.title("高频词关联规则网络", fontsize=20)  # 增大标题字体大小
plt.show()
```

<img src="./img/image-20250311202220152.png" alt="image-20250311202220152" style="zoom:67%;" />

​	完整代码如下：

```
import pandas as pd
import jieba
from collections import Counter
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import networkx as nx
import matplotlib.pyplot as plt
from matplotlib.font_manager import FontProperties

# 读取Excel文件
file_path = '../data/ces.xlsx'
df = pd.read_excel(file_path)

# 加载停用词表
stopwords_path = 'stopwords.txt'
stopwords = set(pd.read_csv(stopwords_path, header=None, names=['word'])['word'])

# 分词并去除停用词
def preprocess_text(text, stopwords):
    words = jieba.lcut(str(text))
    filtered_words = [word for word in words if word not in stopwords and len(word) > 1]
    return filtered_words

# 提取第三列“内容”
texts = df['内容'].tolist()

# 构建交易数据集
transactions = []
for idx, text in enumerate(texts):
    transaction = preprocess_text(text, stopwords)
    transactions.append(transaction)

    # 输出每个文本的高频词列表
    word_freq = Counter(transaction)
    high_freq_words = {word: freq for word, freq in word_freq.items() if freq > 5}
    print(f"Text {idx + 1} High Frequency Words: {high_freq_words}")

# 统计所有词汇的频率并筛选高频词
all_words = [word for transaction in transactions for word in transaction]
word_freq = Counter(all_words)
high_freq_words = {word: freq for word, freq in word_freq.items() if freq > 5}

# 使用TransactionEncoder编码交易数据
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df_te = pd.DataFrame(te_ary, columns=te.columns_)

# 应用Apriori算法
frequent_itemsets = apriori(df_te, min_support=0.5, use_colnames=True)
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=0.7,num_itemsets='num_itemsets')

# 输出构建的关联规则
print("\nAssociation Rules:")
for idx, row in rules.iterrows():
    antecedents = list(row['antecedents'])
    consequents = list(row['consequents'])
    support = row['support']
    confidence = row['confidence']
    lift = row['lift']
    print(
        f"Rule {idx + 1}: {antecedents} -> {consequents}, Support: {support:.4f}, Confidence: {confidence:.4f}, Lift: {lift:.4f}"
    )

# 可视化高频词关联规则网络
G = nx.DiGraph()

# 添加节点和边，并将提升度作为边的权重
edge_weights = {}  # 存储边及其对应的提升度
for _, row in rules.iterrows():
    antecedents = list(row['antecedents'])
    consequents = list(row['consequents'])
    support = row['support']
    confidence = row['confidence']
    lift = row['lift']

    for antecedent in antecedents:
        G.add_node(antecedent)
    for consequent in consequents:
        G.add_node(consequent)

    for antecedent in antecedents:
        for consequent in consequents:
            G.add_edge(antecedent, consequent, weight=lift)
            edge_weights[(antecedent, consequent)] = lift

# 设置中文字体
plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体
plt.rcParams['axes.unicode_minus'] = False  # 解决负号'-'显示为方块的问题

# 调整布局以减少重叠
pos = nx.spring_layout(G, k=0.8, iterations=50)  # 增加k值以增大节点间距，增加迭代次数以优化布局

# 计算边的宽度，使提升度越大，边越粗
min_lift = min(edge_weights.values())
max_lift = max(edge_weights.values())
edge_widths = {edge: (lift - min_lift) / (max_lift - min_lift) * 5 + 1 for edge, lift in edge_weights.items()}  # 归一化处理后乘以系数

# 绘制网络图
plt.figure(figsize=(18, 12))  # 增大图表尺寸
nx.draw_networkx_nodes(G, pos, node_size=20)  # 增大节点大小

# 根据提升度设置边的宽度
edges = G.edges()
weights = [edge_widths[edge] for edge in edges]
nx.draw_networkx_edges(G, pos, edgelist=edges, width=weights, edge_color='b')

# 增大字体大小
nx.draw_networkx_labels(G, pos, font_size=18, font_family="SimHei")  # 使用黑体并增大字体

# edge_labels = {(u, v): f"{d['weight']:.2f}" for u, v, d in G.edges(data=True)}
# nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_family="SimHei", font_size=12)  # 使用黑体并增大字体

plt.title("高频词关联规则网络", fontsize=20)  # 增大标题字体大小
plt.show()
```

#### 4.3 关联规则与风险耦合

​	关联规则和风险分析看似两个不相关的领域，其实也可以结合起来研究。《Risk coupling analysis under accident scenario evolution: A methodological construct and application》提出了一种综合框架，用于在事故情景演变过程中进行风险耦合分析。该框架结合了多种方法，包括加权关联规则挖掘算法、社会网络分析和随机Petri网。下面我们详细介绍如何结合关联规则和社会网络分析来实现风险分析。

**加权关联规则挖掘算法（Weighted Eclat Algorithm）**

**目的**：从风险信息数据中提取关键风险因素。

**步骤：**

- 数据预处理：对原始数据进行清洗、分词、去除停用词等操作，得到常见风险列表。下面是一个简单的文本预处理算法。

  ```
  import jieba
  
  # 示例文本数据可替换
  documents = ["这是一个用于测试的文本数据，雄安郊野公园是一个规模极大的城市郊野公园，我们一路逛着，来到定州园的非遗集市。一排排红灯笼簇拥的展位上，各地特色美食和手工艺品琳琅满目。集市前广场上，锣鼓声起，一只大红狮子跃入场中，扑、跌、翻、滚、跳跃、擦痒，动作层出不穷。鼓点声越来越密，狮子的动作也越来越急。鼓声乍停，狮子就地一滚，高高立起，向围观人群抱拳敬礼。众人纷纷喝彩，狮子一个转身，摇摇摆摆走下场去。我与儿子都感觉还没看够，突然间鼓声又起，只见一人手拿绣球在前，四只狮子紧跟在后，有大有小，有红有黄，摇头甩尾，左顾右盼地跑上场来。锣鼓声变得紧凑起来，四只狮子各显神通，“两面抖”“大荷花”“群狮舞”“狮子滚绣球”等绝技一一亮出，全场顿时掌声雷动。"]
  
  # 自定义分词函数，使用jieba进行分词
  def segment_text(text):
      # 使用精确模式进行分词
      return jieba.lcut(text)
  
  # 停用词列表，可以根据需要自定义添加或减少或引入官方停用词表
  stop_words = set(['的', '这', '是', '一个', '用于', '我们', '将', '会', '其', '进行', '包括', '和'])
  
  # 清洗、分词并去除停用词
  def preprocess_documents(docs):
      processed_docs = []
      for doc in docs:
          words = segment_text(doc)  # 分词
          filtered_words = [word for word in words if word not in stop_words]  # 去除停用词
          processed_docs.append(filtered_words)
      return processed_docs
  
  processed_documents = preprocess_documents(documents)
  
  # 输出处理后的结果
  for i, words in enumerate(processed_documents):
      print(f"文档 {i+1} 处理后的词语列表: {words}")
  ```

- 引入信息熵：作为风险因素的权重，以确保数字化原则。

  ​	信息熵越高，表示该风险因素的不确定性越大，因此在风险耦合分析中，高信息熵的风险因素可能需要更多的关注和控制。下面是一个简单的信息熵计算公式。
  $$
  E_{j}=-\sum_{X\in X_{j}}P_{j}(X)\log P_{j}(X)
  $$
  ​	其中，Ej : 表示第 j 个风险因素的信息熵。𝑋𝑗 : 表示第 𝑗个风险因素的集合。𝑃𝑗(𝑋)： 表示第 𝑗个风险因素中事件 𝑋 发生的概率，也可以理解为事件 𝑋在数据集中出现的频率。而后对其归一化作为权重即可。
  $$
  W_{j}=\frac{E_{j}}{\sum_{j=1}^{m}E_{j}}.
  $$

- 设定阈值：通过迭代试验(间隔0.1)比较确定最小支持度（Min-sup）和最小置信度（Min_conf）。

  <img src="./img/image-20250311202231002.png" alt="image-20250311202231002" style="zoom:67%;" />

  ​	观测可发现最小支持度为0.5和最小置信度为0.6时，关联规则分布更均匀，数量也不过多，因此设置最小支持度为0.5和最小置信度为0.6。

- 挖掘关联规则：使用加权Eclat算法挖掘强关联规则，并用R语言可视化关联规则网络，以识别重要风险因素。

<img src="./img/image-20250311202237522.png" alt="image-20250311202237522" style="zoom:67%;" />

​	其中，R4、R21、R22、R19等都在关联规则网络中占据核心位置，属于重要风险。

**社会网络分析（SNA）**

**目的**：识别风险耦合网络中的关键风险因素及其传播影响。

**步骤**：

- 构建风险因素矩阵：利用提升度将风险因素之间的关系连接为矩阵。**提升度**能够衡量前项和后项之间（风险之间）的关联强度，反映两个因素之间的耦合关系强度，并将其可视化为热图，如下图所示。

  <img src="./img/image-20250311202246084.png" alt="image-20250311202246084" style="zoom:67%;" />

- 中心性分析：计算节点的度中心性和中介中心性，以评估风险节点其在网络中的重要性。其中度中心性可以衡量节点的连接强度。中介中心性能够衡量节点在网络中控制其他节点的能力。

  计算度中心性：

$$
C_D=\frac{\sum_{i=1}^m(C_{D_{\max}}-C_{D_i})}{\max\sum_{i=1}^m(C_{D_{\max}}-C_{D_i})},
$$

​	𝐶𝐷𝑚𝑎𝑥代表网络中度中心性最高的节点的度中心性值，而𝐶𝐷𝑖是节点𝑖的度中心性。这个公式通过计算每个节点的度中心性与最大度中心性之差的总和，并将其归一化到0到1之间，来提供一个相对的比较标准。

​	计算中介中心性：
$$
C_{B}\left(m_{i}\right)=\sum_{j<k}\frac{N_{jk}(m_{i})}{N_{jk}}
$$
​	CB(mi)代表节点𝑖的中介中心性，𝑁𝑗𝑘是从节点𝑗到节点𝑘的所有最短路径数，而𝑁𝑗𝑘(𝑚𝑖)是这些路径中经过节点𝑖的数量。因此，中介中心性测量了节点𝑖作为其他节点对之间桥梁的重要性。如果一个节点具有高中介中心性，则意味着它在网络的信息流或资源分配中扮演着关键角色。

- 可视化风险耦合社会网络：将上图的因素矩阵也被加载到Netdraw软件中，以创建风险因素的耦合网络。

  <img src="./img/image-20250311202254674.png" alt="image-20250311202254674" style="zoom:67%;" />

  ​	从该网络中可以初步看出，R1（社区人员安全意识差，缺乏安全知识），R6（可燃物过度积聚，可燃物自燃），R7（建筑消防设计不合理、违法装修的建筑）、R19（未定期对社区人员进行消防教育），R21（社区火灾隐患调查不按规定进行）、R22（没有合理的消防管理制度）都在关联规则分布网络中处于关键位置，属于关键风险因素。



### 五、参考资料

| https://blog.csdn.net/answer3lin/article/details/84501984    |
| :----------------------------------------------------------- |
| https://blog.csdn.net/wtyuong/article/details/124769657      |
| https://developer.aliyun.com/article/1363430                 |
| https://zhuanlan.zhihu.com/p/370133983                       |
| https://blog.csdn.net/m0_73752612/article/details/143841369  |
| https://blog.csdn.net/weixin_51545953/article/details/128609392 |
| https://developer.baidu.com/article/details/3146008          |
| https://zhuanlan.zhihu.com/p/607861824                       |
| https://blog.51cto.com/u_92655/6636387                       |
| https://blog.csdn.net/m0_64198455/article/details/135741785  |
| https://blog.csdn.net/m0_61789994/article/details/131837391  |
| https://zhuanlan.zhihu.com/p/668316974                       |
| https://blog.csdn.net/baixiangxue/article/details/80335469   |
| https://blog.csdn.net/zaishuiyifangxym/article/details/97782705 |
| https://blog.csdn.net/guangzhanblog/article/details/105066881 |
| https://blog.csdn.net/u013571432/article/details/142187493   |
| https://blog.csdn.net/ARPOSPF/article/details/86632802       |
| https://blog.csdn.net/2401_84750872/article/details/143466830 |
| https://blog.csdn.net/qq_45047246/article/details/107632067  |
| https://blog.csdn.net/universsky2015/article/details/137312605 |
| Yao , J. , Zhang , B. , Wang , D. , Lei , D. ,& Tong , R.(2023).Risk coupling analysis under accident scenario evolution: A methodological construct and application. Risk Analysis, 44(6), 1482-1497. |

