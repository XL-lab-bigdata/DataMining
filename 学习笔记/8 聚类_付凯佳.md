<font face="宋体">

# 聚类分析

## 1. 聚类分析的概念/种类/场景

### 1.1 聚类分析的概念

聚类分析是指将物理或抽象对象的集合分组为由类似对象组成的多个类的分析过程。其目标是在相似性基础上收集数据并分类 。它起源于分类学，早在 1932 年，德里弗和克罗格在人类学中首次应用，随后被引入心理学、人格心理学等领域。1963 年，《数值分类学原理》推动了聚类方法的研究。

聚类分析适用于各类事物缺乏可靠历史资料，无法确定类别数量，却需要将性质相近事物归为一类的情况。在各指标之间存在一定相关关系时，聚类分析能有效发挥作用。其适用范围广泛，涵盖数学、计算机科学、统计学、生物学和经济学等多个领域。

我们之所以需要聚类分析，是因为它能在数据没有明确标签或类别时，帮助我们发现隐藏模式。比如在探索性数据分析中，当我们对数据集了解不足时，聚类分析可帮助识别数据中的模式或组，从而获得初步见解。并且在更复杂的分析或预测模型之前，聚类还可用于数据的分段或降维，提高后续步骤的效率和准确性。虽然在某些特定场景下，其他技术如分类分析等也能处理数据分类问题，但聚类分析独特之处在于，它是无监督学习，不需要事先给定分类标准，能自动从样本数据出发进行分类 ，这是其他技术难以替代的。

### 1.2 聚类分析的种类

#### 按照聚类的技术分类

- **基于划分的方法**：这类方法的核心思想是将整个数据集分割为若干个互不重叠的子集，每个子集即为一个簇。K-means 是其中最具代表性的算法之一，它通过最小化簇内方差来寻找最佳的簇分配方案。这种方法简单高效，但是对初始质心的选择非常敏感，并且只能识别球形簇。

| 代表算法 | 输入数据结构 | 输出数据结构 | 度量指标 | 基本思想与使用场景 |
| --- | --- | --- | --- | --- |
| K-means         | 数值型特征向量       | 簇标签             | 欧式距离、曼哈顿距离等           | 通过迭代优化簇内点的距离平方和来寻找最优解。适用于大型数据集，但对初始中心敏感 |
| K-medoids       | 数值型特征向量       | 簇标签             | 曼哈顿距离等                    | 使用实际数据点作为簇中心，对噪声更鲁棒，但计算成本较高 |

---

- **基于层次的方法**：该方法构建了一个树形结构，可以是自底向上的凝聚层次聚类或者是自顶向下的分裂层次聚类。这种类型的聚类非常适合用于探索性数据分析，因为它提供了关于数据如何分组的详细视图。然而，它的计算复杂度较高，特别是对于大数据集来说。

| 代表算法 | 输入数据结构 | 输出数据结构 | 度量指标 | 基本思想与使用场景 |
| --- | --- | --- | --- | --- |
| Agglomerative   | 数值型特征向量       | 树状图（Dendrogram）| 最近邻距离、最远邻距离等         | 构建一个树形结构，自底向上或自顶向下合并/分裂簇，适合探索性数据分析 |
| Divisive        | 数值型特征向量       | 树状图（Dendrogram）| 最近邻距离、最远邻距离等         | 从所有数据开始，逐步分裂成子簇，适用于需要明确层级关系的情况 |

---

- **基于密度的方法**：DBSCAN 和 Mean Shift 属于这一类，它们依赖于数据点的密度来定义簇。这意味着即使形状复杂的簇也能被正确地识别出来。这些算法对于含有噪声的数据特别有用，因为它们能够区分出核心点、边界点和噪声点。

| 代表算法 | 输入数据结构 | 输出数据结构 | 度量指标 | 基本思想与使用场景 |
| --- | --- | --- | --- | --- |
| DBSCAN          | 数值型特征向量       | 簇标签             | 密度可达、核心对象               | 基于密度定义簇，能够发现任意形状的簇，适用于具有噪声的数据 |
| Mean Shift      | 数值型特征向量       | 簇标签             | 密度峰值                        | 通过滑动窗口找到密度最高的区域，适用于图像处理等领域 |

---

- **基于模型的方法**：Gaussian Mixture Model (GMM) 是一种典型的基于模型的聚类算法，它假设数据是由多个高斯分布混合而成的。通过期望最大化（Expectation-Maximization, EM）算法，GMM 可以估计出各个高斯分布的参数。这种方法在处理混合类型的数据时非常有效，但它要求用户预先知道或者猜测数据中的成分数量。

| 代表算法 | 输入数据结构 | 输出数据结构 | 度量指标 | 基本思想与使用场景 |
| --- | --- | --- | --- | --- |
| Gaussian Mixture| 数值型特征向量       | 高斯分布参数       | 对数似然                       | 假设数据由若干高斯分布生成，通过EM算法估计参数，适用于混合模型的数据 |

---

- **基于网格的方法**：STING 是一个例子，这类算法首先将空间划分成若干网格单元，然后根据单元内部的统计特性来进行聚类。这种方法的优点在于它可以处理非常大的数据集，并且计算效率高。不过，它的性能高度依赖于网格大小的选择。

| 代表算法 | 输入数据结构 | 输出数据结构 | 度量指标 | 基本思想与使用场景 |
| --- | --- | --- | --- | --- |
| STING           | 网格单元 | 簇标签 | 单元间的统计信息 | 将空间划分为网格，然后根据网格内的统计信息进行聚类，适用于大规模数据 |

---

### 1.3 聚类分析的使用场景

- **市场细分** ：在营销和商业智能中，聚类分析可将客户分组。例如电商平台通过分析客户的购买行为、浏览记录、消费金额等数据，将客户聚类。某电商发现一类客户经常购买母婴产品，且消费金额较高，就可以针对这部分客户推送高端母婴用品促销信息、育儿知识等，提高营销效果。

- **异常检测**：在工业生产中，聚类分析可用于检测设备运行状态是否异常。比如在化工生产中，对设备的温度、压力、流量等参数进行监测，通过聚类分析，将正常运行状态下的数据聚为一类，当出现与该类数据差异显著的数据点时，就可判断设备可能出现故障，及时进行维护。

- **图像分割**：在图像识别领域，聚类可将图像分割成不同区域。以医学图像为例，通过聚类分析，将 X 光片、CT 影像等中的人体组织、器官、病变部位等分割出来，辅助医生进行疾病诊断。

- **社交网络分析**：在社交平台中，聚类分析可以识别具有相似属性或连接的社区或群组。例如微博通过分析用户的关注关系、互动行为、兴趣标签等，将用户划分成不同的兴趣群组，如体育爱好者群组、影视爱好者群组等，方便平台进行精准内容推荐和广告投放 。

- **基因数据分析**：在生物信息学中，用于分析和分类基因或蛋白质的表达模式。通过聚类，研究人员可以发现具有相似功能的基因或蛋白质，从而深入了解生物的遗传机制和生命过程，为疾病研究和药物研发提供依据。

---

## 2. 聚类分析的算法

### 2.1 聚类分析的步骤

在执行聚类分析之前，需要对数据进行一系列预处理步骤，以确保数据适合聚类算法的要求，并提高聚类结果的质量。以下是详细的步骤：

>1.数据准备与清洗：

- **降噪**：去除异常值和噪声点，因为它们可能会严重影响聚类结果。可以使用统计方法（如箱线图）或基于密度的方法（如DBSCAN）来识别并处理这些点。

- **缺失值处理**：检查数据集中是否存在缺失值，并根据实际情况选择填充、删除或插值等方法处理。

- **归一化/标准化**：由于不同的特征可能具有不同的量纲和范围，因此通常需要对数据进行归一化或标准化处理，以便所有特征都在相同的尺度上。常见的方法有最小-最大缩放、Z-score标准化等。

---

>2.特征选择与提取：

- **特征选择**：从原始特征中挑选出最能代表数据特性的特征，避免冗余信息干扰聚类过程。可以基于相关性分析、主成分分析（PCA）、Lasso回归等技术进行特征选择。

- **特征提取**：当原始特征不足以描述数据结构时，可以通过特征提取生成新的特征。例如，使用PCA降维或者利用自动编码器（Autoencoder）学习低维表示。

---

>3.确定聚类数目：

根据研究目的和领域知识确定合适的簇数。如果不确定，可以使用肘部法则、轮廓系数、Gap statistic等方法来估计最佳簇数。

---

>4.选择聚类算法：

- **基于划分的方法**：（如K-means）适用于大规模数据集，但对初始质心敏感，且只能发现球形簇。

- **基于层次的方法**：（如Agglomerative Clustering）能够揭示数据的层级结构，但计算复杂度较高。

- **基于密度的方法**：（如DBSCAN）对于非凸形状的数据和噪声具有较好的鲁棒性。

- **基于模型的方法**：（如Gaussian Mixture Model, GMM）假设数据由多个概率分布组成，适用于混合模型的数据。

---

>5.执行聚类算法：

应用选定的聚类算法到数据集上，并设置必要的参数（如K-means中的K值）。

---

>6.结果评估与解释：

使用内部指标（如Silhouette Score、Calinski-Harabasz Index）或外部指标（如果有真实标签，如Adjusted Rand Index, ARI）评估聚类效果。分析每个簇的特点，理解其背后的业务含义或模式。

---

>7.当聚类效果不佳时的修改策略：

- **调整参数**：尝试不同的参数设置，比如改变K-means中的K值，或是DBSCAN中的eps和min_samples参数。

- **改进数据预处理**：重新考虑数据清洗和标准化的方法，确保没有遗漏任何关键步骤。

- **尝试不同的算法**：如果当前算法无法提供满意的结果，可以尝试其他类型的聚类算法。

- **增加特征信息**：有时候，添加更多的特征或使用更复杂的特征提取方法可以帮助提升聚类的效果。

---

### 2.2 常见的聚类算法

### 2.2.1 K-Means 算法

K-Means 是一种常用的无监督学习算法，主要用于将数据集划分为 $ k $ 个不同的簇。其基本思路是通过迭代优化来最小化所有样本点到各自最近的簇中心的距离平方和。

**算法步骤如下：**

1. **初始化**：随机选择 $ k $ 个初始簇中心（质心）。

2. **分配簇**：对于每个样本点，计算它到各个簇中心的距离，并将其分配到距离最近的簇中。

3. **更新簇中心**：对于每个簇，重新计算其新的簇中心，即该簇中所有样本点的均值。

4. **重复**：重复执行步骤 2 和 3，直到簇中心不再发生变化或达到最大迭代次数。

5. **输出结果**：输出最终的簇中心和每个样本所属的簇标签。

**数学公式:**

假设我们有一个数据集 $ X = \{x_1, x_2, ..., x_n\} $，其中每个 $ x_i \in \mathbb{R}^d $ 表示一个 $ d $-维的数据点。K-Means的目标是最小化以下损失函数：

$$ J(C) = \sum_{i=1}^{k} \sum_{x_j \in C_i} \| x_j - \mu_i \|^2 $$

其中，

- $ C = \{C_1, C_2, ..., C_k\} $ 是 $ k $ 个簇。
- $ \mu_i $ 是第 $ i $ 个簇的中心。
- $ \| x_j - \mu_i \|^2 $ 是样本点 $ x_j $ 到簇中心 $ \mu_i $ 的欧氏距离的平方。

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans

# 生成一些示例数据
n_samples = 1500
random_state = 170
X, y = make_blobs(n_samples=n_samples, random_state=random_state)

# 使用K-Means进行聚类
kmeans = KMeans(n_clusters=3, random_state=random_state)
cluster_labels = kmeans.fit_predict(X)
centers = kmeans.cluster_centers_

# 绘制散点图
plt.figure(figsize=(8, 6))
colors = ['r', 'g', 'b']
for i in range(3):
    points = X[cluster_labels == i]
    plt.scatter(points[:, 0], points[:, 1], s=50, c=colors[i], label=f'Cluster {i+1}')
plt.scatter(centers[:, 0], centers[:, 1], s=200, c='yellow', marker='X', label='Centroids')
plt.title('K-Means Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True)
plt.show()

```

---

### 2.2.2 GMM算法

高斯混合模型（Gaussian Mixture Model, GMM）是一种概率模型，用于表示由多个高斯分布混合而成的数据集。GMM通过假设数据是由多个高斯分布生成的，并使用期望最大化（Expectation-Maximization, EM）算法来估计这些高斯分布的参数。

**算法步骤如下：**

1. **初始化**：随机选择 $ k $ 个初始高斯分布的均值、协方差矩阵和权重。

2. **E步（Expectation Step）**：计算每个样本点属于每个高斯分布的概率（后验概率），即责任度（responsibility）。
   $$
   r_{ij} = \frac{\pi_j \mathcal{N}(x_i | \mu_j, \Sigma_j)}{\sum_{l=1}^{k} \pi_l \mathcal{N}(x_i | \mu_l, \Sigma_l)}
   $$
   其中，
   - $ r_{ij} $ 是样本 $ x_i $ 属于第 $ j $ 个高斯分布的概率。
   - $ \pi_j $ 是第 $ j $ 个高斯分布的权重。
   - $ \mathcal{N}(x_i | \mu_j, \Sigma_j) $ 是高斯分布的概率密度函数。

3. **M步（Maximization Step）**：更新每个高斯分布的参数，包括均值、协方差矩阵和权重。
   $$
   N_j = \sum_{i=1}^{n} r_{ij}
   $$
   $$
   \mu_j = \frac{1}{N_j} \sum_{i=1}^{n} r_{ij} x_i
   $$
   $$
   \Sigma_j = \frac{1}{N_j} \sum_{i=1}^{n} r_{ij} (x_i - \mu_j)(x_i - \mu_j)^T
   $$
   $$
   \pi_j = \frac{N_j}{n}
   $$

4. **重复**：重复执行步骤 2 和 3，直到参数收敛或达到最大迭代次数。

5. **输出结果**：输出最终的高斯分布参数和每个样本所属的簇标签。

**数学公式:**

假设我们有一个数据集 $ X = \{x_1, x_2, ..., x_n\} $，其中每个 $ x_i \in \mathbb{R}^d $ 表示一个 $ d $-维的数据点。GMM的目标是找到一组参数 $ \theta = (\pi, \mu, \Sigma) $ 来最大化数据集的对数似然函数：

$$
\log L(\theta | X) = \sum_{i=1}^{n} \log \left( \sum_{j=1}^{k} \pi_j \mathcal{N}(x_i | \mu_j, \Sigma_j) \right) 
$$

其中,

- $ \pi_j $ 是第 $ j $ 个高斯分布的权重，满足 $ \sum_{j=1}^{k} \pi_j = 1 $。
- $ \mathcal{N}(x_i | \mu_j, \Sigma_j) $ 是高斯分布的概率密度函数。

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.mixture import GaussianMixture

# 生成一些示例数据
n_samples = 1500
random_state = 170
X, y = make_blobs(n_samples=n_samples, random_state=random_state)

# 使用GMM进行聚类
gmm = GaussianMixture(n_components=3, random_state=random_state)
cluster_labels = gmm.fit_predict(X)
means = gmm.means_
covariances = gmm.covariances_

# 绘制散点图
plt.figure(figsize=(8, 6))
colors = ['r', 'g', 'b']
for i in range(3):
    points = X[cluster_labels == i]
    plt.scatter(points[:, 0], points[:, 1], s=50, c=colors[i], label=f'Cluster {i+1}')
    
    # 绘制椭圆边界
    v, w = np.linalg.eigh(covariances[i])
    u = w[0] / np.linalg.norm(w[0])
    angle = np.arctan(u[1] / u[0])
    angle = 180 * angle / np.pi  
    ell = plt.matplotlib.patches.Ellipse(means[i], 2 * np.sqrt(v[0]), 2 * np.sqrt(v[1]),
                                         angle=angle, color=colors[i], fill=False)
    ell.set_clip_box(plt.gca().bbox)
    ell.set_alpha(0.5)
    plt.gca().add_artist(ell)

plt.title('GMM Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True)
plt.show()

```

![](./img/GMM.png)

---

### 2.2.3 DBSCAN算法

DBSCAN（Density-Based Spatial Clustering of Applications with Noise）是一种基于密度的聚类算法。它能够发现任意形状的簇，并且对噪声数据具有鲁棒性。DBSCAN的核心思想是将密集区域划分为簇，而稀疏区域被视为噪声。

**算法步骤如下：**

1. **选择一个未访问的样本点**：随机选择一个未被访问过的样本点 $ p $。

2. **查找邻域内的样本点**：
   - 查找所有距离 $ p $ 小于或等于半径 $ \epsilon $ 的邻居点。
   - 如果这些邻居点的数量小于最小点数 $ \text{MinPts} $，则将 $ p $ 标记为噪声点。
   - 否则，创建一个新的簇，并将 $ p $ 及其所有密度可达的点加入该簇。

3. **扩展簇**：
   - 对于每个新加入簇的点 $ q $，检查其邻域内的点。
   - 如果 $ q $ 的邻域内有足够的点，则将其邻域内的所有密度可达的点也加入当前簇。

4. **重复**：重复上述过程，直到所有样本点都被处理过。

5. **输出结果**：输出最终的簇和噪声点。

**数学公式:**

DBSCAN 使用两个主要参数来定义簇：

- $ \epsilon $：邻域半径。
- $ \text{MinPts} $：最小点数阈值。

对于一个样本点 $ p $，其邻域 $ N_\epsilon(p) $ 定义为：

$$ N_\epsilon(p) = \{ x \in D \mid \| x - p \| \leq \epsilon \} $$

其中，$ D $ 是数据集，$ \| x - p \| $ 是样本点 $ x $ 和 $ p $ 之间的欧氏距离。如果：

$$
|N_\epsilon(p)| \geq \text{MinPts}
$$

则 $ p $ 是核心点（core point）。否则，它是边界点（border point）或噪声点（noise point）。

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.cluster import DBSCAN

# 生成一些示例数据
n_samples = 1500
random_state = 170
X, _ = make_moons(n_samples=n_samples, noise=0.05, random_state=random_state)

# 使用DBSCAN进行聚类
dbscan = DBSCAN(eps=0.2, min_samples=10)
cluster_labels = dbscan.fit_predict(X)

# 绘制散点图
plt.figure(figsize=(8, 6))
unique_labels = set(cluster_labels)
colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]

for k, col in zip(unique_labels, colors):
    if k == -1:
        # 黑色用于噪声点
        col = [0, 0, 0, 1]

    class_member_mask = (cluster_labels == k)

    xy = X[class_member_mask]
    plt.scatter(xy[:, 0], xy[:, 1], s=50, c=[col], label=f'Cluster {k}' if k != -1 else 'Noise')

plt.title('DBSCAN Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True)
plt.show()

```

![](./img/dbscn.png)

---

### 2.2.4 OPTICS算法

**思路与流程:**

OPTICS（Ordering Points To Identify the Clustering Structure）是一种基于密度的聚类算法，类似于DBSCAN，但它能够识别不同密度的簇，并且不需要预先指定簇的数量。OPTICS通过构建一个可达距离图来确定簇结构。

**算法步骤如下：**

1. **初始化**：初始化所有样本点的状态为“未访问”。

2. **选择一个未访问的样本点**：随机选择一个未被访问过的样本点 $ p $。

3. **计算可达距离**：
   - 对于每个样本点 $ q $，计算其从 $ p $ 出发的可达距离。
   - 可达距离定义为：
     $$
     \text{reachability\_distance}(q, p) = \max(\epsilon\_neighborhood\_radius(p), \| x_q - x_p \| )
     $$
     其中，
     - $ \epsilon\_neighborhood\_radius(p) $ 是 $ p $ 的邻域半径。
     - $ \| x_q - x_p \| $ 是样本点 $ q $ 和 $ p $ 之间的欧氏距离。

4. **更新可达距离图**：将样本点 $ q $ 添加到有序队列中，并按可达距离排序。

5. **扩展簇**：
   - 对于队列中的每个点 $ q $，如果它的可达距离小于某个阈值，则将其加入当前簇。
   - 如果 $ q $ 是核心点，则继续扩展簇；否则，停止扩展。

6. **重复**：重复上述过程，直到所有样本点都被处理过。

7. **输出结果**：输出最终的簇和噪声点。

**数学公式:**

OPTICS 使用两个主要参数来定义簇：

- $ \epsilon $：邻域半径。
- $ \text{MinPts} $：最小点数阈值。

对于一个样本点 $ p $，其邻域 $ N_\epsilon(p) $ 定义为：

$$ N_\epsilon(p) = \{ x \in D \mid \| x - p \| \leq \epsilon \} $$

其中，$ D $ 是数据集，$ \| x - p \| $ 是样本点 $ x $ 和 $ p $ 之间的欧氏距离。

核心点（core point）定义为：

$$ |N_\epsilon(p)| \geq \text{MinPts} $$

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.cluster import OPTICS

# 生成一些示例数据
n_samples = 1500
random_state = 170
X, _ = make_moons(n_samples=n_samples, noise=0.05, random_state=random_state)

# 使用OPTICS进行聚类
optics = OPTICS(min_samples=10, xi=0.05, min_cluster_size=0.1)
cluster_labels = optics.fit_predict(X)

# 绘制散点图
plt.figure(figsize=(8, 6))
unique_labels = set(cluster_labels)
colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]

for k, col in zip(unique_labels, colors):
    if k == -1:
        # 黑色用于噪声点
        col = [0, 0, 0, 1]

    class_member_mask = (cluster_labels == k)

    xy = X[class_member_mask]
    plt.scatter(xy[:, 0], xy[:, 1], s=50, c=[col], label=f'Cluster {k}' if k != -1 else 'Noise')

plt.title('OPTICS Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True)
plt.show()

```

![](./img/optical.png)

---

### 2.2.4 Agglomerative聚类

#### 思路与流程

凝聚层次聚类（Agglomerative Hierarchical Clustering）是一种自底向上的层次聚类方法。它从每个样本点作为一个单独的簇开始，逐步将最接近的簇合并，直到形成一个单一的簇或达到预定的簇数量。

**算法步骤如下：**

1. **初始化**：
   - 将每个样本点视为一个单独的簇。
   - 计算所有簇之间的相似度（通常是距离）矩阵。

2. **合并最近的簇**：
   - 找到距离最近的两个簇 $ C_i $ 和 $ C_j $，并将其合并为一个新的簇 $ C_{ij} = C_i \cup C_j $。
   - 更新距离矩阵，计算新簇 $ C_{ij} $ 与其他簇之间的距离。

3. **重复**：
   - 重复执行步骤 2，直到所有的簇合并成一个单一的簇，或者达到预定的簇数量。

4. **输出结果**：
   - 输出最终的簇结构树（Dendrogram），以及每个样本所属的簇标签。

**数学公式:**

在凝聚层次聚类中，常用的距离度量方法包括单链距离（Single Linkage）、全链距离（Complete Linkage）、组平均距离（Average Linkage）和重心距离（Ward's Method）。以下是这些距离度量的公式：

- **单链距离（Single Linkage）**：
  $$
  d_{\text{single}}(C_i, C_j) = \min_{x \in C_i, y \in C_j} \| x - y \|
  $$

- **全链距离（Complete Linkage）**：
  $$
  d_{\text{complete}}(C_i, C_j) = \max_{x \in C_i, y \in C_j} \| x - y \|
  $$

- **组平均距离（Average Linkage）**：
  $$
  d_{\text{average}}(C_i, C_j) = \frac{1}{|C_i||C_j|} \sum_{x \in C_i} \sum_{y \in C_j} \| x - y \|
  $$

- **重心距离（Ward's Method）**：
  $$
  d_{\text{ward}}(C_i, C_j) = \frac{|C_i| + |C_j|}{|C_i| + |C_j| + |C_{ij}|} \| \mu_i - \mu_j \|^2
  $$
  其中，
  - $ \mu_i $ 和 $ \mu_j $ 分别是簇 $ C_i $ 和 $ C_j $ 的均值。
  - $ |C_i| $ 和 $ |C_j| $ 分别是簇 $ C_i $ 和 $ C_j $ 中的样本数量。

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

# 生成一些示例数据
n_samples = 1500
random_state = 170
X, _ = make_blobs(n_samples=n_samples, random_state=random_state)

# 使用Agglomerative Clustering进行聚类
agg_clustering = AgglomerativeClustering(n_clusters=3, linkage='ward')
cluster_labels = agg_clustering.fit_predict(X)

# 绘制散点图
plt.figure(figsize=(12, 6))

# 左侧子图：散点图
plt.subplot(1, 2, 1)
colors = ['r', 'g', 'b']
for i in range(3):
    points = X[cluster_labels == i]
    plt.scatter(points[:, 0], points[:, 1], s=50, c=colors[i], label=f'Cluster {i+1}')
plt.title('Agglomerative Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True)

# 右侧子图：Dendrogram
plt.subplot(1, 2, 2)
Z = linkage(X, method='ward')
dendrogram(Z, truncate_mode='level', p=3)
plt.title('Dendrogram')
plt.xlabel('Sample index')
plt.ylabel('Distance')

plt.tight_layout()
plt.show()

```

![](./img/层次聚类.png)

---

### 2.2.5 谱聚类算法

**思路与流程:**

谱聚类（Spectral Clustering）是一种基于图论的方法，利用图的拉普拉斯矩阵的特征向量来进行数据聚类。它特别适用于处理非凸形状的数据集，并且能够有效地发现数据中的潜在结构。

**算法步骤如下：**

1. **构建相似度图**：
   - 将数据点视为图的顶点。
   - 构建一个加权邻接矩阵 $ W $，其中 $ W_{ij} $ 表示样本点 $ x_i $ 和 $ x_j $ 之间的相似度。
   - 常用的相似度度量方法包括高斯核函数：
     $$
     W_{ij} = \exp\left(-\frac{\| x_i - x_j \|^2}{2\sigma^2}\right)
     $$
     其中，$ \sigma $ 是高斯核的标准差。

2. **构建对角度矩阵**：
   - 计算度矩阵 $ D $，其中 $ D_{ii} = \sum_{j=1}^{n} W_{ij} $。

3. **构建归一化拉普拉斯矩阵**：
   - 计算归一化拉普拉斯矩阵 $ L $：
     $$
     L = I - D^{-1/2} W D^{-1/2}
     $$
     其中，$ I $ 是单位矩阵。

4. **求解特征值问题**：
   - 求解归一化拉普拉斯矩阵 $ L $ 的前 $ k $ 个最小特征值对应的特征向量。
   - 这些特征向量组成一个 $ n \times k $ 的矩阵 $ U $。

5. **进行K-Means聚类**：
   - 在特征向量矩阵 $ U $ 上应用K-Means算法，得到最终的簇标签。

6. **输出结果**：
   - 输出每个样本所属的簇标签。

**数学公式:**

- **相似度矩阵 $ W $**：
  $$
  W_{ij} = \exp\left(-\frac{\| x_i - x_j \|^2}{2\sigma^2}\right)
  $$

- **度矩阵 $ D $**：
  $$
  D_{ii} = \sum_{j=1}^{n} W_{ij}
  $$

- **归一化拉普拉斯矩阵 $ L $**：
  $$
  L = I - D^{-1/2} W D^{-1/2}
  $$

- **特征值分解**：
  $$
  L u_i = \lambda_i u_i
  $$
  其中，$ \lambda_i $ 和 $ u_i $ 分别是第 $ i $ 个特征值和特征向量。

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_circles
from sklearn.cluster import SpectralClustering
from sklearn.preprocessing import StandardScaler

# 生成一些示例数据
n_samples = 1500
random_state = 170
X, _ = make_circles(n_samples=n_samples, factor=0.5, noise=0.05, random_state=random_state)

# 标准化数据
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 使用Spectral Clustering进行聚类
spectral_clustering = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', assign_labels='kmeans', random_state=random_state)
cluster_labels = spectral_clustering.fit_predict(X_scaled)

# 绘制散点图
plt.figure(figsize=(8, 6))
colors = ['r', 'g']
for i in range(2):
    points = X[cluster_labels == i]
    plt.scatter(points[:, 0], points[:, 1], s=50, c=colors[i], label=f'Cluster {i+1}')

plt.title('Spectral Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True)
plt.show()

```

![](./img/谱聚类.png)

---

## 3. 聚类分析结果评价与可视化

### 3.1 聚类分析的评价指标

#### 3.1.1 外部指标

以下是聚类分析中11种外部指标的名称、描述、含义、取值范围、作用及适用场景：

>1.列联矩阵

- **描述**：无特定单一公式，一般用$C_{ij}$表示矩阵中第$i$行第$j$列元素，$i$对应真实类别，$j$对应聚类类别
- **含义**：展示真实类别与聚类类别之间的交叉统计，$C_{ij}$表示属于真实类别$i$且被聚类到类别$j$的样本数量
- **取值范围**：元素为非负整数，具体取值取决于样本数据和聚类结果
- **作用**：直观呈现不同真实类别在各聚类中的分布，便于了解错分情况
- **适用场景**：在各种聚类算法结果评估中，初步分析聚类与真实类别的对应关系。

```python
from sklearn import metrics
from sklearn.datasets import make_blobs
X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)
kmeans = KMeans(n_clusters=4, random_state=0)
y_kmeans = kmeans.fit_predict(X)
''' shape of y_true & y_kmeans is (300,)'''
metrics.cluster.contingency_matrix(y_true, y_kmeans)
```

---

>2.对称混淆矩阵

- **描述**：一般表示为$\begin{bmatrix}a&b\\c&d\end{bmatrix}$，其中$a$表示在真实标签和聚类结果中都属于同一类的样本对数量，$b$表示在真实标签中属于同一类但在聚类结果中不属于同一类的样本对数量，$c$表示在真实标签中不属于同一类但在聚类结果中属于同一类的样本对数量，$d$表示在真实标签和聚类结果中都不属于同一类的样本对数量
- **含义**：从样本对角度比较真实标签和聚类结果的分类情况
- **取值范围**：元素为非负整数
- **作用**：评估聚类结果与真实标签在样本对层面的一致性
- **适用场景**：用于比较不同聚类算法在样本对分类上的表现。

```python
''' shape of y_true & y_kmeans is (300,)'''
metrics.cluster.pair_confusion_matrix(y_true, y_kmeans)
```

---

>3.Fowlkes - Mallows指数

- **描述**：$FM=\sqrt{\frac{TP}{TP+FP}\times\frac{TP}{TP+FN}}$，其中$TP$是真正例，$FP$是假正例，$FN$是假反例
- **含义**：基于真实类别和聚类类别间的真正例、假正例等计算，是精确率和召回率的几何平均值
- **取值范围**：$[0,1]$
- **作用**：综合评估聚类结果的准确性和完整性，值越接近1一致性越好
- **适用场景**：在有监督的聚类评估中，衡量聚类结果与真实标签的相似性。

```python
''' shape of y_true & y_kmeans is (300,)'''
metrics.fowlkes_mallows_score(y_true, y_kmeans)
```

---

>4.兰德系数

- **描述**：$RI=\frac{a + d}{a + b + c + d}$，其中$a$、$b$、$c$、$d$含义同对称混淆矩阵中元素含义
- **含义**：计算在真实标签和聚类结果中分类一致的样本对占总样本对的比例
- **取值范围**：$[0,1]$
- **作用**：衡量聚类结果与真实标签的一致性，值越大一致性越好
- **适用场景**：广泛用于评估各种聚类算法结果与真实分类的符合程度。

```python
''' shape of y_true & y_kmeans is (300,)'''
metrics.rand_score(y_true, y_kmeans)
```

---

>5.调整兰德系数

- **描述**：$ARI=\frac{RI - E(RI)}{max(RI)-E(RI)}$，其中$RI$是兰德系数，$E(RI)$是随机期望值，$max(RI)$是兰德指数的最大值
- **含义**：对兰德系数修正，考虑随机分类影响
- **取值范围**：$[-1,1]$
- **作用**：消除随机聚类影响，更准确评估聚类与真实标签的一致性
- **适用场景**：当需要准确判断聚类结果是否优于随机分类时使用。

```python
''' shape of y_true & y_kmeans is (300,)'''
metrics.adjusted_rand_score(y_true, y_kmeans)
```

---

>6.互信息

- **描述**：$I(X;Y)=\sum_{x\in X}\sum_{y\in Y}p(x,y)\log\frac{p(x,y)}{p(x)p(y)}$，其中$X$为聚类结果，$Y$为真实标签，$p(x,y)$是$X=x$且$Y=y$的联合概率，$p(x)$、$p(y)$是边缘概率
- **含义**：衡量聚类结果和真实标签之间的信息共享程度
- **取值范围**：$[0,+\infty)$
- **作用**：值越大表明两者相关性越强，聚类结果越接近真实标签
- **适用场景**：用于评估聚类结果对真实标签信息的捕捉能力。

```python
''' shape of y_true & y_kmeans is (300,)'''
metrics.mutual_info_score(y_true, y_kmeans)
```

---

>7.标准化互信息

- **描述**：$NMI(X,Y)=\frac{2I(X,Y)}{H(X)+H(Y)}$，其中$I(X,Y)$是互信息，$H(X)$、$H(Y)$分别是$X$、$Y$的熵
- **含义**：对互信息归一化，消除类别和样本数量影响
- **取值范围**：$[0,1]$
- **作用**：在不同数据集和聚类结果间可比，值越接近1一致性越好
- **适用场景**：用于比较不同聚类算法或不同参数下聚类结果与真实标签的相似性。

```python
''' shape of y_true & y_kmeans is (300,)'''
metrics.normalized_mutual_info_score(y_true, y_kmeans)
```

---

>8.调整互信息

- **描述**：$AMI(X,Y)=I(X,Y)-\frac{H(X)H(Y)-I(X,Y)}{n-1}$，其中$I(X,Y)$是互信息，$H(X)$、$H(Y)$分别是$X$、$Y$的熵，$n$是样本数量
- **含义**：修正互信息，考虑随机分类影响
- **取值范围**：$[-1,1]$
- **作用**：更准确评估聚类与真实标签的依赖关系，排除随机因素干扰
- **适用场景**：在数据存在噪声或聚类结果可能受随机因素影响时使用。

```python
''' shape of y_true & y_kmeans is (300,)'''
metrics.adjusted_mutual_info_score(y_true, y_kmeans)
```

---

>9.同质性

- **描述**：$H=\frac{-\sum_{i=1}^{K}\sum_{j=1}^{C}n_{ij}\log\frac{n_{ij}}{n_{i+}}}{-\sum_{i=1}^{K}n_{i+}\log\frac{n_{i+}}{N}}$，其中$K$是聚类数，$C$是真实类别数，$n_{ij}$是属于真实类别$i$且被聚类到类别$j$的样本数，$n_{i+}=\sum_{j=1}^{C}n_{ij}$，$N$是总样本数
- **含义**：衡量每个聚类是否只包含来自一个真实类别的样本
- **取值范围**：$[0,1]$
- **作用**：评估聚类结果中每个聚类内样本的纯度，值越接近1纯度越高
- **适用场景**：关注聚类结果中每个簇是否只包含单一真实类别样本的场景。

```python
''' shape of y_true & y_kmeans is (300,)'''
metrics.homogeneity_score(y_true, y_kmeans)
```

---

>10.完整性

- **描述**：$C=\frac{-\sum_{i=1}^{K}\sum_{j=1}^{C}n_{ij}\log\frac{n_{ij}}{n_{+j}}}{-\sum_{j=1}^{C}n_{+j}\log\frac{n_{+j}}{N}}$，其中符号含义同同质性公式
- **含义**：衡量每个真实类别是否都被分配到同一个聚类中
- **取值范围**：$[0,1]$
- **作用**：评估真实类别在聚类结果中的完整性，值越接近1完整性越好
- **适用场景**：需要确保每个真实类别在聚类中不被分散的情况。

```python
''' shape of y_true & y_kmeans is (300,)'''
metrics.homogeneity_score(y_true, y_kmeans)
```

---

>11.V - measure

- **描述**：$V=\frac{(1+\beta^2)HC}{\beta^2H + C}$，一般取$\beta = 1$时，$V=\frac{2HC}{H + C}$，其中$H$是同质性，$C$是完整性
- **含义**：同质性和完整性的调和平均值
- **取值范围**：$[0,1]$
- **作用**：综合考量聚类的同质性和完整性，值越接近1整体效果越好
- **适用场景**：全面评估聚类结果在同质性和完整性两方面表现的场景。

```python
''' shape of y_true & y_kmeans is (300,)'''
metrics.v_measure_score(y_true, y_kmeans)
```

---

#### 3.1.2 内部指标

>1.轮廓系数（Silhouette Score）

- **描述**：对于样本集中的每个样本 $i$，其轮廓系数 $s(i)$ 定义为：
  $$s(i)=\frac{b(i)-a(i)}{\max\{a(i),b(i)\}}$$
  其中，$a(i)$ 是样本 $i$ 到同簇内其他样本的平均距离，$b(i)$ 是样本 $i$ 到最近的不同簇中所有样本的平均距离。整个数据集的轮廓系数是所有样本轮廓系数的平均值。
- **含义**：轮廓系数衡量了样本与其所在簇的紧密程度以及与其他簇的分离程度。$a(i)$ 反映了样本 $i$ 与同簇内其他样本的相似性，$b(i)$ 反映了样本 $i$ 与其他簇样本的差异性。
- **取值范围**：$[-1, 1]$。值接近 1 表示样本与所在簇的紧密程度高且与其他簇分离程度高；值接近 -1 表示样本可能被错误地分配到了当前簇；值接近 0 表示样本可能位于两个簇的边界上。
- **作用**：用于评估聚类结果的质量，值越高说明聚类效果越好，能够帮助选择合适的聚类数量。
- **适用场景**：适用于各种聚类算法，尤其是在不知道真实标签的情况下，用于比较不同聚类数量或不同聚类算法的效果。

>2.Calinski - Harabasz 指数（Calinski - Harabasz Index）

- **描述**：
  $$CH=\frac{\mathrm{tr}(B_k)/(k - 1)}{\mathrm{tr}(W_k)/(n - k)}$$
  其中，$n$ 是样本数量，$k$ 是聚类的数量，$\mathrm{tr}(B_k)$ 是类间离散矩阵的迹，$\mathrm{tr}(W_k)$ 是类内离散矩阵的迹。类间离散矩阵 $B_k$ 衡量了不同簇之间的分离程度，类内离散矩阵 $W_k$ 衡量了每个簇内样本的紧密程度。
- **含义**：该指数通过计算类间离散度与类内离散度的比值来评估聚类效果。比值越大，说明类间离散度相对类内离散度越大，即聚类结果中不同簇之间的分离程度越好，同一簇内的样本越紧密。
- **取值范围**：$[0, +\infty)$。值越大表示聚类效果越好。
- **作用**：用于评估聚类的质量，帮助选择最优的聚类数量，指数值最大时对应的聚类数量通常被认为是较优的聚类方案。
- **适用场景**：适用于各种聚类算法，特别是在数据集规模较大时，计算效率相对较高，可用于快速评估不同聚类结果的优劣。

>3.Davies - Bouldin 指数（Davies - Bouldin Index）

- **描述**：
  $$DB=\frac{1}{k}\sum_{i = 1}^{k}\max_{j\neq i}\left(\frac{s_i + s_j}{d_{ij}}\right)$$
  其中，$k$ 是聚类的数量，$s_i$ 是第 $i$ 个簇内样本的平均距离，用于衡量第 $i$ 个簇的紧密程度；$d_{ij}$ 是第 $i$ 个簇和第 $j$ 个簇的质心之间的距离，用于衡量簇间的分离程度。
- **含义**：该指数计算了每个簇与其他簇之间的“相似度”（通过簇内紧密程度和簇间分离程度的比值来衡量），并取所有簇的最大值的平均值。值越小，说明每个簇与其他簇的区分度越高，聚类效果越好。
- **取值范围**：$[0, +\infty)$。值越小表示聚类效果越好。
- **作用**：用于评估聚类的质量，帮助选择合适的聚类数量，指数值最小时对应的聚类数量通常被认为是较优的聚类方案。
- **适用场景**：适用于各种聚类算法，尤其在需要综合考虑簇内紧密性和簇间分离性的场景下，可用于比较不同聚类结果的优劣。 

#### 3.1.3 多指标综合评价

#### (1) 综合使用内部和外部指标进行聚类结果评价

>**1.一致性分析**：

当内部指标和外部指标都显示出较好的结果时，例如轮廓系数接近1，同时调整兰德系数也接近1，说明聚类结果在内部紧密性、外部分离性以及与真实标签的一致性上都表现良好，聚类效果较为可靠。

若内部指标表现好但外部指标不佳，可能是聚类算法在挖掘数据内部结构上有一定能力，但与实际的真实类别划分存在偏差，需要检查数据的特征选择或真实标签的准确性等。

反之，若外部指标好而内部指标一般，可能聚类结果只是在与已知标签的匹配上有巧合，内部的簇结构并不理想，需要进一步分析簇内和簇间的关系。

>**2.稳定性分析**：

通过改变聚类算法的参数或使用不同的聚类算法，观察内部和外部指标的变化情况。

如果在不同设置下，内部和外部指标都相对稳定，说明聚类结果具有较好的稳定性和可靠性；若指标波动较大，则说明聚类结果可能对参数或算法敏感，需要进一步优化。

>**3.互补分析**：

内部指标可以帮助理解聚类结果本身的结构特性，如簇的紧密程度和分离程度；外部指标则从与真实情况的对比角度来评估聚类的准确性。

例如，Calinski - Harabasz指数高说明类间分离度好、类内紧密性高，但不知道是否与真实标签一致，此时结合调整兰德系数等外部指标，若也较高，就可以更全面地说明聚类结果既在内部结构上合理，又与真实情况相符。

#### (2) 指标选用方法

>**1.根据数据有无标签**

- **无标签数据**：优先选择内部指标，如轮廓系数能综合反映簇内紧密性和簇间分离性；Calinski - Harabasz指数可衡量类间和类内的离散程度；Davies - Bouldin指数能体现簇内紧密性与簇间分离性的比值。这些指标可以帮助在无监督的情况下评估聚类结果的质量，选择合适的聚类数量和算法。

- **有标签数据**：使用外部指标，如调整兰德系数、互信息等可以直接衡量聚类结果与真实标签的一致性，还可结合准确率、F1分数等，以全面评估聚类结果的准确性。同时也可以辅助使用内部指标，来进一步了解聚类结果的内部结构。

>**2.根据数据特点**

- **高维数据**：可以选择Dunn指数等相对更适合高维空间的指标，其衡量簇间最小距离与簇内最大距离的比值，受维度影响相对较小。还可以考虑基于密度的指标，因为在高维数据中，密度的概念可能比距离更能反映数据点之间的关系。

- **数据分布复杂**：若数据分布呈现出复杂的形状和结构，可能需要综合使用多种指标。例如，对于存在不同密度区域的数据，轮廓系数可以帮助识别那些可能处于不同密度簇边界的数据点；而Calinski - Harabasz指数可以从整体上评估不同密度簇之间的分离情况。

>**3.根据聚类目的**

- **探索性分析**：主要是为了发现数据中的潜在结构和模式，此时内部指标更为重要，通过分析轮廓系数、Davies - Bouldin指数等，可以了解聚类结果是否合理地将数据划分为不同的簇，以及簇的质量如何。

- **分类或预测任务**：如果聚类的目的是为了后续的分类或预测，那么外部指标就显得尤为重要，如准确率、F1分数等可以直接反映聚类结果对真实类别的预测能力。

### 3.2 聚类分析结果可视化

>使用Python中的`sklearn`库来进行聚类分析，并使用`matplotlib`库进行可视化。具体来说，我们会展示以下几种图形：

- **散点图**：显示数据点及其所属的簇。
- **轮廓图（Silhouette Plot）**：评估每个样本与其最近邻簇的距离。
- **PCA降维后的二维散点图**：通过主成分分析将高维数据降到二维后进行可视化。

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.decomposition import PCA

# 生成一些示例数据
n_samples = 1500
random_state = 170
X, y = make_blobs(n_samples=n_samples, random_state=random_state)

# 使用K-Means进行聚类
range_n_clusters = [2, 3, 4, 5, 6]
clusterer = KMeans(n_clusters=3, random_state=random_state)
cluster_labels = clusterer.fit_predict(X)

# 计算轮廓系数
silhouette_avg = silhouette_score(X, cluster_labels)
sample_silhouette_values = silhouette_samples(X, cluster_labels)

# 绘制散点图
plt.figure(figsize=(12, 8))

ax1 = plt.subplot(1, 2, 1)
colors = plt.cm.Spectral(cluster_labels.astype(float) / 3)
ax1.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,
            c=colors, edgecolor='k')

centers = clusterer.cluster_centers_
ax1.scatter(centers[:, 0], centers[:, 1], marker='o',
            c="white", alpha=1, s=200, edgecolor='k')

for i, c in enumerate(centers):
    ax1.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                s=50, edgecolor='k')

ax1.set_title("The visualization of the clustered data.")
ax1.set_xlabel("Feature space for the 1st feature")
ax1.set_ylabel("Feature space for the 2nd feature")

# 绘制轮廓图
ax2 = plt.subplot(1, 2, 2)
y_lower = 10
for i in range(3):
    ith_cluster_silhouette_values = \
        sample_silhouette_values[cluster_labels == i]

    ith_cluster_silhouette_values.sort()

    size_cluster_i = ith_cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i

    color = plt.cm.Spectral(float(i) / 3)
    ax2.fill_betweenx(np.arange(y_lower, y_upper),
                      0, ith_cluster_silhouette_values,
                      facecolor=color, edgecolor=color, alpha=0.7)

    ax2.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

    y_lower = y_upper + 10  

ax2.set_title("The silhouette plot for the various samples.")
ax2.set_xlabel("The silhouette coefficient values")
ax2.set_ylabel("Cluster label")

ax2.axvline(x=silhouette_avg, color="red", linestyle="--")

ax2.set_yticks([]) 
ax2.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
              "with n_clusters = %d" % 3),
             fontsize=14, fontweight='bold')
plt.show()

# 使用PCA进行降维并绘制二维散点图
pca = PCA(n_components=2).fit_transform(X)
plt.figure(figsize=(8, 6))
scatter = plt.scatter(pca[:, 0], pca[:, 1], c=cluster_labels, cmap=plt.cm.Spectral)
plt.title('PCA-reduced data clustered by KMeans')
plt.colorbar(scatter)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

```

>散点图+轮廓图
![](./img/散点轮廓.png)

>PCA降维度后的二维散点
![](./img/pca二维散点.png)

## 4. 课后思考

### 4.1 聚类分析的选择与数据分布之间的关系

选择合适的聚类算法通常取决于数据的分布、格式以及研究的具体目标。

例如，如果数据呈现明显的球形分布且没有噪声干扰，K-means 算法可能是最佳选择，因为它简单且高效。

然而，当面对非凸形状的数据或含有大量噪声时，基于密度的方法如 DBSCAN 则更为合适，因为它能够识别任意形状的簇并且对噪声具有较高的鲁棒性。

**<span style="color: yellow;">例子：同一组数据因为研究目标的不同可以采用两种不同的算法</span>**

假设我们有一个客户购买行为的数据集，包括购买频率和平均消费金额两个维度。

如果我们希望根据客户的消费模式来细分市场，那么 K-means 可能是一个不错的选择，因为它可以有效地将客户分为几个明确的群体，比如高价值客户、中等价值客户和低价值客户。但是。

如果我们想要识别那些异常的购买行为（比如欺诈行为），那么使用 DBSCAN 更加合适，因为它能够区分出核心客户群、边缘客户群和潜在的异常点。

### 4.2 聚类分析和图像分割

简单的聚类算法如 K-means 可以提供直观的理解，因为它们基于距离度量进行分组。然而，随着模型复杂度的增加，尤其是采用深度学习方法时，模型变得越来越难以解释。

以图像分割为例，YOLO 算法虽然主要用于目标检测，但其架构设计也可以应用于图像分割任务。YOLO 通过单次前向传播预测边界框和类别概率，这种方法非常适合实时应用。

然而，YOLO 并不直接依赖于聚类算法来进行图像分割，而是通过卷积神经网络提取特征并预测每个网格单元的目标位置和类别。

尽管如此，聚类算法可以在图像分割中发挥作用，比如用于预处理步骤中的超像素生成，或者作为后处理步骤的一部分来细化分割结果。

</font>
