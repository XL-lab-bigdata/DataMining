**分类（classification）**：分类任务就是通过学习得到一个目标函数（target function）**f，把每个属性集**x**映射到一个余弦定义的类标号y。目标函数也称为**分类模型（classification model）。

属性可以是离散的或者连续的，但类标号必须是离散的，这正是分类与回归（regression）的关键特征。回归是一种预测建模任务，其中目标属性y是连续的。

分类计数非常适合预测或描述二元或标称类型的数据集，对于序数分类，分类技术不太有效，因为分类技术不考虑隐含在目标类中的序关系。其他形式的联系，如子类和超累的关系也被忽略。本章余下的部分只考虑二元的或标称类型的类标号。

分类技术（或分类法）是一种根据数据集建立分类模型的系统方法。分类法的例子包括**决策树分类法**、**基于规则的分类法**、**神经网络**、**支持向量机**和**朴素贝叶斯分类法**。

分类模型的性能根据模型正确和错误预测的检验记录计数进行评估，这些计数存放在称作混淆矩阵（confusion matrix）的表格中。表中每个表项fij表示实际类标号为i但被预测为类j的记录数。

<img src="file:///D:/下载/241603054刘辰雲/241603054_刘辰雲_学习笔记/imp/1272129-20171214201457888-559314829.png" title="" alt="" data-align="center">

对在混淆矩阵中涉及到的字符进行解释：

TP：真实为 1 且预测正确的样本个数

FN：真实为 1 但预测错误的样本个数

FP：真实为 0 但预测错误的样本个数

TN：真实为 0 且预测正确的样本个数

综上所述，在利用混淆矩阵来评估分类模型时，观察混淆矩阵的对角线数据，即 TP 和 TN 的数值。对角线数据越大，分类模型效率越好；反之模型效率不好。

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-12-30-image.png" title="" alt="" data-align="center">

### 决策树

决策树是一种由结点和有向边组成的层次结构。树中包含三种结点。

● **根结点（root node）**，它没有入边，但有零条或多条出边。

● **内部结点（internal node）**，恰有一条入边和两条或多条出边。

● **叶结点（leaf node）或终结点（terminal node）**，恰有一条入边，但没有出边

在决策树中，每个叶结点都赋予一个类标号。**非终结点（non-terminal node）**（包括根结点和内部结点）包含属性测试条件，用以分开具有不同特性的记录。

**如何建立决策树**

原则上讲，对于给定的属性集，可以构造的决策树的数目达指数级。尽管某些决策树比其他决策树更准确，但是由于搜索空间是指数规模的，招出最佳决策树在计算上是不可行的。尽管如此，人们还是开发了一些有效的算法，能够在合理的时间内构造出具有一定准确率的次最优决策树。这些算法通常都采用贪心策略，在选择划分数据的属性时，采取一系列局部最优策略来构造决策树，**Hunt算法**就是一种这样的算法。Hunt算法是许多决策树算法的基础，包括**ID3**、**C4.5**和**CART**。

**1、Hunt算法**

在Hunt算法中，通过将训练记录相继划分成较纯的子集，以递归方式建立决策树。设Dt是与结点t相关联的训练记录集，而y={y1，y2，…，yc}是类标号，Hunt算法的递归定义如下。

（1）如果Dt中所有记录都属于同一个类yt，则t是叶结点，用yt标记。

（2）如果Dt中包含属于多个类的记录，则选择一个**属性测试条件（attribute test condition）**，将记录划分成较小的子集。对于测试条件的每个输出，创建一个子女结点，并根据测试结果将Dt中的记录分布到子女结点中。然后，对于每个子女结点，递归地调用该算法。

如果属性值的每种组合都在训练数据中出现，并且每种组合都具有唯一的类标号，则Hunt算法是有效的。但是对于大多数实际情况，这些假设太苛刻了，因此，需要附加的条件来处理以下的情况。

（1）算法的第二步所创建的子女结点可能为空，即不存在与这些结点相关联的记录。如果没有一个训练记录包含与这一的结点相关联的属性值集合，这种情形就可能发生。这时，该结点成为叶结点，类标号为其父节点上训练记录中的多数类。

（2）在第二步，如果与Dt相关联的所有记录都具有相同的属性值（目标属性除外），则不可能进一步划分这些记录。在这种情况下，该结点为叶结点，其标号为与该结点相关联的训练记录中的多数类。

**2、决策树归纳的设计问题**

决策树归纳的学习算法必须解决下面两个问题。

（1）如何分裂训练记录？ 

树增长过程的每个递归步都必须选择一个属性测试条件，将记录划分成较小的子集。为了实现这个步骤，算法必须提供为不同类型的属性指定测试条件的方法，并且提供评估每种测试条件的客观度量。

（2）如何停止分裂过程？

需要有结束条件，以终止决策树的生长过程。一个可能的策略是分裂结点，直到所有的记录都属于同一个类，或者所有的记录都具有相同的属性值。尽管两个结束条件对于结束决策树归纳算法都是充分的，但是还可以使用其他的标准提前终止树的生长过程。提前终止的优点将在下文中讨论。

### 表示属性测试条件的方法

二元属性：二元属性的测试条件产生两个可能的输出

标称属性：由于标称属性有多个属性值，它的测试条件可以用两种方法表示。对于多路划分，其输出数取决于该属性不同属性值的个数。另一方面，某些决策树算法（如CART）只产生二元划分，它们考虑创建k个属性值的二元划分的所有2k−1−1种方法。

序数属性：序数属性也可以产生二元或多路划分，只要不违背序数属性值的有序性，就可以对属性值进行分组。

连续属性：连续属性需要离散化为二元或多元输出。离散化后，每个离散化区间赋予一个新的序数值，只要保持有序性，相邻的值还可以聚集成较宽的区间。

### **选择最佳划分的度量**

选择最佳划分的度量通常是根据划分后子女结点不纯性的程度。不纯的程度越低，类分布就越倾斜。例如，类分布为（0,1）的结点具有零不纯性，而均衡分布（0.5,0.5）的结点具有最高的不纯性，不纯性度量的例子包括：

熵

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-21-21-image.png" title="" alt="" data-align="center">

Gini

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-21-33-image.png" title="" alt="" data-align="center">

分类误差

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-21-43-image.png" title="" alt="" data-align="center">

 其中，c是类的个数，并且在计算熵时，0log20=0。p表示属于其中一个类的记录所占的比例，如二元分布均匀时，p=0.5，而当所有记录都属于同一个类时，p=1。

为了确定测试条件的效果，我们需要比较父节点（划分前）的不纯程度和子女结点（划分后）的不纯程度，它们的差越大，测试条件的效果就越好。增益δ是一种可以用来确定划分效果的标准：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-21-54-image.png" title="" alt="" data-align="center">

其中，I(x)是给定结点的不纯性度量，N是父结点上的记录数，k是属性值的个数，N(vj)是与子女结点vj相关联的记录个数。决策树归纳算法通常选择最大化增益δ的测试条件，因为对所有的测试条件来说，I(parent)是一个不变的值，所以最大化增益等价于最小化子女结点的不纯性度量的加权平均值。最后，当选择**熵（entiopy）**作为公式的不纯性度量时，熵的差就是所谓**信息增益（information gain）**δinfo

**1、二元属性的的划分**

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-22-21-image.png" title="" alt="" data-align="center">

比较不同属性的和，取和最小的属性作为属性划分

**2、标称属性的划分**

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-22-40-image.png" title="" alt="" data-align="center">

比较不同属性的值，取值最小的属性作为属性划分

**3、连续属性的划分**

首先需要离散化为二元属性，如“年收入≤v”来划分成二元属性，然后可以同二元属性的划分来判断连续属性的划分。如果用穷举法来确定v的值，计算代价是昂贵的。为了降低计算复杂度，按照年收入将训练记录排序，从两个相邻的排过序的属性值中选择中间值作为候选划分点。该过程还可以进一步优化：仅考虑位于**具有不同类标号**的两个相邻记录之间的候选划分点。

**4、增益率**

测试条件不应产生过多的结点，因为与每个划分相关联的记录太少，以致不能作出可靠的预测。

解决该问题的策略有两种：

（1）限制测试条件只能是二元划分，CART这样的决策树算法采用的就是这种策略。

（2）修改评估划分的标准，把属性测试条件产生的输出数也考虑进去，例如，决策树算法C4.5采用称作增益率（gain ratio）的划分标准来评估划分。

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-22-58-image.png" title="" alt="" data-align="center">

其中，k是划分的总数。如果某个属性产生了大量的划分，它的划分信息将会很大，从而降低了增益率。

### 决策树归纳算法

以下算法给出了称作TreeGrowth的决策树归纳算法的框架。该算法的输入是训练记录集**E**合属性集**F**。算法递归地选择最优的属性来划分数据（步骤7），并扩展树的叶结点（步骤11和步骤12），直到满足结束条件（步骤1）

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\1272129-20171214225128076-1597500285.png" title="" alt="" data-align="center">

（1）函数createNode()为决策树建立新节点。决策树的节点或者是一个测试条件，记作node.testcond，或者是一个类标号，记作node.label

（2）函数find_best_split()确定应当选择哪个属性作为划分训练记录的测试条件。如前所述，测试条件的选择取决于使用哪种不纯性度量来评估划分，一些广泛使用的度量包括熵、Gini指标和χ2

（3）函数Classify()为叶结点确定类标号。对于每个叶结点t，令p(i|t)表示该结点上属于类i的训练记录所占的比例，在大多数情况下，都将叶结点指派到具有多数记录的类：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-23-46-image.png" title="" alt="" data-align="center">

其中，操作argmax返回最大化p(i|t)的参数值i。p(i|t)除了提供确定叶结点类标号所需要的信息之外，还可以用来估计分配到叶结点t的记录属于类i的概率。下文讨论如何使用这种概率估计，在不同的代价函数下，确定决策树的性能。

（4）函数stopping_cond()通过检查是否所有的记录都属于同一个类，或者都具有相同的属性值，决定是否终止决策树的增长。终止递归函数的另一种方法是，检查记录数是否小鱼某个最小阈值。

 建立决策树之后，可以进行**树剪枝（tree-pruning）**，以减小决策树的规模。决策树过大容易受所谓**过分拟合（overfiting）**现象的影响。通过修建初始决策树的分支，剪枝有助于提高决策树的泛化能力。

### 模型的过分拟合

分类模型的误差大致分为两种：**训练误差（training error）和泛化误差（generalization error）**。训练误差也称再**代入误差（resubstitution error）或表现误差（apparent error）****，是在训练记录上误分类样本比例，而泛化误差是模型在位置记录上的期望误差。

**1、泛化误差估计**

虽然过分拟合的主要原因一直是个争辩的话题，大家还是普遍统一模型的复杂度对模型的过分拟合有影响。问题是，如何确定正确的模型复杂度？理想的复杂度是能产生最低泛化误差的模型的复杂度。然而，在建立模型的过程中，学习算法只能访问训练数据集，对检验数据集，它一无所知，因此也不知道所建立的决策树在未知记录上的性能。我们所能做的就是估计决策树的泛化误差。

（1）使用再代入估计

再代入估计方法假设训练数据集可以很好地代表整体数据，因为，可以使用训练误差（又称再打入误差）提供对泛化误差的乐观估计。在这样的前提下，决策树归纳算法简单地选择产生最低训练误差的模型作为最终的模型。然而，训练误差通常是泛化误差的一种很差的估计。

（2）结合模型复杂度

如前所述，模型越是复杂，出现过分拟合的几率就越高，因此，我们更喜欢采用较为简单的模型。这种策略与应用众所周知的**奥卡姆剃刀（Occam's razor）或节俭原则（principle of parsimony）**一致。

**奥卡姆剃刀：**给定两个具有相同泛化误差的模型，较简单的模型比较复杂的模型更可取。

下面介绍两种把模型复杂度与分类模型评估结合在一起的方法：

**a、悲观误差评估：**第一种方法明确使用训练误差与**模型复杂度罚项（penalty term）**的和计算泛化误差。结果泛化误差可以看作模型的**悲观误差估计（pessimistic error estimate**）。例如，设n(t)是结点t分类的训练记录数，e(t)是被误分类的记录数。决策树T的悲观误差估计eg(T)可以用下式计算：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-24-41-image.png" title="" alt="" data-align="center">

其中，k是决策树的叶结点数，e(T)决策树的总训练误差，Nt是训练记录数，Ω(ti)是每个结点ti对应的惩罚。

对于有7个叶结点，有4个误分类的记录数，总训练记录数为24，罚项假设等于0.5的决策树，则eg(T)=4+7×0.524

0.5的罚项意味着每增加一个叶结点，就有0.5个记录错误，除非增加结点能够改善一个记录的分类，否则结点就不应扩展。

**b、最小描述长度原则：**另一种结合模型复杂度的方法是基于称作**最小描述长度（minimum description length,MDL）**原则的信息论方法。假设,A决定建立一个分类模型，概括x和y之间的关系。在传送给B前，模型用压缩形式编码。如果模型的准确率是100%，那么传输的代价就等于模型编码的代价。否则，A还必须传输哪些记录被模型错误分类的信息。传输的总代价是：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-24-54-image.png" title="" alt="" data-align="center">

**c、估计统计上界**

泛化误差也可以用训练误差的统计修正来估计。因为泛化误差倾向于比训练误差大，所以统计修正通常是计算训练误差的上界，考虑到达决策树一个特定叶结点的训练记录数。例如，决策树算法C4.5中，假定每个叶结点上的错误服从二项分布。为了计算泛化误差，我们需要确定训练误差的上限，在下面的例子中解释说明。

TR中的最左叶结点被扩展为TL中的两个子女结点。在划分前，该结点的错误率是2/7=0.286。用正态分布近似二项分布，可以推导出错误率e的上界是：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-25-11-image.png" title="" alt="" data-align="center">

**d、使用确认集**

在该方法中，不是用训练集估计泛化误差，而是把原始的训练数据集分为两个较小的子集，一个子集用于训练，而另一个称作确认集，用于估计泛化误差。典型的做法是，保留2/3的训练集来建立模型，剩余的1/3用作误差估计。

**2、处理决策树归纳中的过分拟合**

在上文中，我们介绍了一些估计分类模型泛化误差的方法。对于泛化误差可靠的估计能让学习算法搜索到准确的模型，而且不会对训练数据过分拟合。本节介绍两种在决策树归纳上避免过分拟合的策略。

**先减枝（提前终止规则）：**在这种方法中，树增长算法在产生完全拟合整个训练数据集的完全增长的决策树之前就停止决策树的生长。为了做到这一点，需要采用更具限制性的结束条件。例如，当观察到的不纯性度量的增益（或估计的泛化误差的改进）低于某个确定的阈值时就停止扩展叶结点。这种方法的优点在于避免产生过分拟合训练数据的过于复杂的子树。然而，很难为提前终止选择正确的阈值。阈值太高将导致拟合不足的模型，而阈值太低就不能充分地解决过分拟合的问题。此外，即便使用已有的属性测试条件得不到显著的增益，接下来的划分也可能产生较好的子树。

**后剪枝：**在该方法中，初始决策树按照最大规模生长，然后进行剪枝的步骤，按照自底向上的方式修剪完全增长的决策树。修剪有两种做法：（1）用新的叶结点替换子树，该叶结点的类标号由子树下记录中的多数类确定；或者（2）用子树中最常使用的分支代替子树。当模型不能再改进时终止剪枝步骤。与先剪枝相比，后剪枝技术倾向于产生更好的结果，因为不像先剪枝，后剪枝是根据完全增长的决策树做出的剪枝决策，先减枝则可能过早终止决策树的生长。然而，对于后剪枝，当子树被剪掉后，生长完全决策树的额外的计算就被浪费了。

### 评估分类器的性能

常用的评估分类器性能的方法：

**1、保持方法**

在**保持（Holdout）方法**中，将被标记的原始数据划分成两个不想交的集合，分别称为训练集合检验集。在训练数据集上归纳分类模型，在检验集上评估模型的性能。训练集和检验集的划分比例通常根据分析家的判断（例如，50-50，或者2/3作为训练集、1/3作为检验集）。分类器的准确率根据模型在检验集上的准确率估计。

**2、随机二次抽样**

可以多次重复保持方法来改进对分类器性能的估计，这种方法称作**随机二次抽样（random subsampling）**。设acci是第i次迭代的模型准确率，总准确率是accsub=∑ki=1acci/k。随机二次抽样也会遇到一些与保持方法同样的问题，因为在训练阶段也没有利用尽可能多的数据。并且，由于它没有控制每个记录用于训练和检验的次数，因此，有些用于训练的记录使用的频率可能比其他记录高很多。

**3、交叉验证**

替代随机二次抽样的一种方法是**交叉验证（cross-validation）**。在该方法中，每个记录用于训练的次数相同，并且恰好检验一次。为了解释该方法，假设把数据分为相同大小的两个子集，首先，我们选择一个子集作训练集，而另一个作检验集，然后交换两个集合的角色，原先作训练集的现在做检验集，反之亦然，这种方法叫做二折交叉验证。总误差通过对两次运行的误差求和得到。在这个例子中，每个样本各作一次训练样本和检验样本。k折交叉验证是对该方法的推广，把数据分为大小相同的k份，在每次运行，选择其中一份作检验集，而其余的全作为训练集，该过程重复k次，使得每份数据都用于检验恰好一次。同样，总误差是所有k次运行的误差之和。

**4、自助法**

以上方法都是假定训练记录采用不放回抽样，因此，训练集合检验集都不包含重复记录。在**自助（bootstrap）方法**中，训练记录采用有放回抽样，即已经选作训练的记录将放回原来的记录集中，使得它等机率地被重新抽取。如果原始数据有N个记录，可以证明，平均来说，大小为N的自助样本大约包含原始数据中63.2%的记录。这是因为一个记录被自助抽样抽取的概率是1−(1−1/N)N，当N充分大时，该概率逐渐逼近1−e−1=0.632。没有抽中的记录就成为检验集的一部分，将训练集建立的模型应用到检验集上，得到自助样本准确率的一个估计εi。抽样过程重复b次，产生b个自助样本。

按照如何计算分类器的总准确率，有几种不同的自助抽样法。常用的方法之一是**.632自助（.632 bootstrap）**，它通过组合每个自助样本的准确率（εi）和由包含所有标记样本的训练集计算的准确率（accs）计算总准确率（accboot）：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-25-23-image.png" title="" alt="" data-align="center">

### 比较分类器的方法

**1、估计准确度的置信区间**

为确定置信区间，需要建立支配准确率度量的概率分布。本节介绍一种方法，通过将分类任务用二项式实验建模来推导置信区间。二项式实验的特性如下。

（1）实验由N个独立的试验组成，其中每个试验有两种可能的结果：成功或失败。

（2）每个试验成功的概率p是常数。

二项式实验的一个例子是统计N次抛硬币正面朝上的次数。如果X是N次试验观察到的成功次数，则X取一个特定值v的概率由均值Np、方差为Np(1−p)的二项分布给出：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-25-35-image.png" title="" alt="" data-align="center">

例如，如果硬币是均匀的(p=0.5)，抛50次硬币，正面朝上20次的概率是：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-25-56-image.png" title="" alt="" data-align="center">

预测检验记录类标号的任务也可以看作是二项式实验。给定一个包含N个记录的检验集，令X是被模型正确预测的记录数，p是模型真正准确率。通过把预测任务用二项式实验建模，X服从均值为Np、方差为Np(1−p)的二项分布。可以证明经验准确率acc=X/N也是均值为p，方差为p(1−p)N的二项分布。尽管可以用二项分布来估计acc的置信区间，但是当N充分大时，通常用正态分布来近似。根据正态分布，可以推导出acc的置信区间为：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-26-12-image.png" title="" alt="" data-align="center">

其中Zα/2和Z1−α/2分别是在置信水平(1−α)下由标准正态分布得到的上界和下界。因为标准正态分布关于Z=0对称，于是我们有Zα/2=Z1−α/2。重新整理不等式，得到p的置信区间如下：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-26-23-image.png" title="" alt="" data-align="center">

下表给出了在不同置信水平下Zα/2的值：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\1272129-20171216144025452-856297070.png" title="" alt="" data-align="center">

考虑一个模型，它在100个检验记录上具有80%的准确率。在95%的置信水平下，模型的真实准确率的置信区间是什么？根据上表，95%的置信水平对应于Zα/2=1.96。将它代入上面的公式得到置信区间在71.1%和86.7%之间。下表给出了随着记录数N的增大所产生的置信区间：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\1272129-20171216144613733-983602252.png" title="" alt="" data-align="center">

注意，随着N的增大，置信区间变得更加紧凑。

**2、比较两个模型的性能**

考虑一对模型M1和M2，它们在两个独立的检验集D1和D2上进行评估，令n1是D1中的记录数，n2是D2中的记录数。另外，假设M1在D1上的错误率为e1，M2在D2上的错误率为e2。目标是检验e1与e2的观察差是否是统计显著的。

假设n1和n2都充分大，e1和e2可以使用正态分布来近似。如果用d=e1−e2表示错误率的观察差，则d服从均值为dt（其实际差）、方差为σ2d的正态分布。d的方差为：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-26-41-image.png" title="" alt="" data-align="center">

其中，e1(1−e1)/n1和e2(1−e2)/n2是错误率的方差。最后，在置信水平(1−α)下，可以证明实际差dt的置信区间由下式给出：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-26-58-image.png" title="" alt="" data-align="center">

例：模型MA在N1=30个检验记录上的错误率e1=0.15，而MB在N2=5000个检验记录上的错误率e2=0.25。错误率的观察差d=|0.15−0.25|=0.1。在这个例子中，我们使用双侧检验来检查dt=0还是dt≠0。错误率观察差的估计方差计算如下：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-27-10-image.png" title="" alt="" data-align="center">

或˜σd=0.0655。把该值代入公式，我们得到在95%的置信水平下，dt置信区间如下：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-27-26-image.png" title="" alt="" data-align="center">

由于该区间跨越了值0，我们可以断言在95%的置信水平下，该观察差不是统计显著的。

**3、比较两种分类法的性能**

假设我们想用k折交叉验证的方法比较两种分类法的性能。首先，把数据集D划分为k个大小相等部分，然后，使用每种分类法，在k-1份数据上构建模型，并在剩余的划分上进行检验，这个步骤重复k次，每次使用不同的划分进行检验。

令Mij表示分类技术Li在第j次迭代产生的模型，注意，每对 模型M1j和M2j在相同的划分j上进行检验。用e1j和e2j分别表示它们的错误率，它们在第j折上的错误率之差可以记作dj=e1j−e2j。如果k充分大，则dj服从于均值为dcvt（错误率的真实差）、方差为σcv的正态分布。与前面的方法不同，观察的差的总方差用下式进行估计：

<img title="" src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-27-35-image.png" alt="" data-align="center">

其中，¯¯¯d是平均差。对于这个方法，我们需要用t分布计算dcvt的置信区间：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-27-44-image.png" title="" alt="" data-align="center">

系数 t1−α,k−1可以通过两个参数（置信水平(1−α)和自由度k−1）查概率表得到。该t分布的概率表在下标中给出。

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\1272129-20171216210441843-645180052.png" title="" alt="" data-align="center">

例：假设两个分类技术产生的模型的准确率估计差的均值等于0.05，标准差等于0.002。如果使用30折交叉验证方法估计准确率，则在95%置信水平下，真是准确率差为：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-28-01-image.png" title="" alt="" data-align="center">

因为置信区间不跨越0值，两个分类法的观察差是统计显著的。

## KNN算法

KNN算法采用测量不同特征值之间的距离方法进行分类。

假设在一个N维空间中有很多个点，然后这些点被分为几个类。相同类的点，肯定是聚集在一起的，它们之间的距离相比于和其他类的点来说，非常近。如果现在有个新的点，我们不知道它的类别，但我们知道了它的坐标，那只要计算它和已存在的所有点的距离，然后以最近的k个点的多数类作为它的类别，则完成了它的分类。这个k就是kNN中的k值。

**KNN算法的重点**

（1）怎么度量邻近度

距离是点和点之间距离。但除了距离，其实我们也可以考虑两个点之间的相似度，越相似，就代表两个点距离越近。同理，也可以考虑相异度，越相异，就代表两个点距离越远。其实距离的度量就是相异性度量的其中一种。

（2）k值怎么取

k值的选取关乎整个分类器的性能。如果k值取得过小，容易受噪点的影响而导致分类错误。而k值取得过大，又容易分类不清，混淆了其他类别的点。

（3）数据的预处理

拿到数据，不能直接就开始套用算法，而是需要先规范数据。例如通过一个人的年龄和工资来进行分类，很明显工资的数值远大于年龄，如果不对它进行一个统一的规范，必然工资这个特征会左右的分类，而让年龄这个特征无效化。

## 邻近度的度量

临近度的度量，主要考虑相似性和相异性的度量。

一般的，把相似度定义为s，常常在0（不相似）和1（完全相似）之间取值。而相异度d有时在0（不相异）和1（完全相异）之间取值，有时也在0和∞之间取值。

当相似度（相异度）落在区间[0,1]之间时，我们可以定义d = 1 - s（或 s = 1 - d）。另一种简单的方法是定义相似度为负的相异度（或相反）。

通常，具有若干属性的对象之间的邻近度用单个属性的邻近度的组合来定义，下图是单个属性的对象之间的邻近度。

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\1272129-20180104224014424-1003961262.png" title="" alt="" data-align="center">

下面讨论更复杂的涉及多个属性的对象之间的邻近性度量

#### 1、距离

一维、二维、三维或高纬空间中两个点**x**和**y**之间的**欧几里得距离（Euclidean distance）**d由如下公式定义：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-36-13-image.png" title="" alt="" data-align="center">

其中，n是维数，而xk和yk分别是**x**和**y**的第k个属性值（分量）。

欧几里得距离是最常用的距离公式。距离对特征都是区间或比率的对象非常有效。

#### 2、二元数据的相似性度量

两个仅包含二元属性的对象之间的相似性度量也称为**相似系数（similarity coefficient）**，并且通常在0和1直接取值，值为1表明两个对象完全相似，而值为0表明对象一点也不相似。

设**x**和**y**是两个对象，都由n个二元属性组成。这样的两个对象（即两个二元向量）的比较可生成如下四个量（频率）：

f00=x取0并且y取0的属性个数

f01=x取0并且y取1的属性个数

f10=x取1并且y取0的属性个数

f11=x取1并且y取1的属性个数

**简单匹配系数（Simple Matching Coefficient,SMC）**一种常用的相似性系数是**简单匹配系数**，定义如下：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-36-25-image.png" title="" alt="" data-align="center">

该度量对出现和不出现都进行计数。因此，**SMC**可以在一个仅包含是非题的测验中用来发现回答问题相似的学生。

**Jaccard系数（Jaccard Coefficient）**假定**x**和**y**是两个数据对象，代表一个事务矩阵的两行（两个事务）。如果每个非对称的二元属性对应于商店的一种商品，则**1**表示该商品被购买，而**0**表示该商品未被购买。由于未被顾客购买的商品数远大于被其购买的商品数，因而像**SMC**这样的相似性度量将会判定所有的事务都是类似的。这样，常常使用**Jaccard系数**来处理仅包含非对称的二元属性的对象。Jaccard系数通常用符号**J**表示，由如下等式定义：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-36-37-image.png" title="" alt="" data-align="center">

#### 3、余弦相似度

文档的相似性度量不仅应当像Jaccard度量一样需要忽略0-0匹配，而且还必须能够处理非二元向量。下面定义的**余弦相似度（cosine similarity）**就是文档相似性最常用的度量之一。如果**x**和**y**是两个文档向量，则

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-36-54-image.png" title="" alt="" data-align="center">

其中，“▪”表示向量点积，x⋅y=∑nk=1xkyk，||x||是向量**x**的长度，||x||=√∑nk=1x2k=√x⋅x

余弦相似度公式还可以写为：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-37-02-image.png" title="" alt="" data-align="center">

**x**和**y**被它们的长度除，将它们规范化成具有长度1。这意味在计算相似度时，余弦相似度不考虑两个数据对象的量值。（当量值是重要的时，欧几里得距离可能是一种更好的选择）

余弦相似度为1，则**x**和**y**之间夹角为0度，**x**和**y**是相同的；如果余弦相似度为0，则**x**和**y**之间的夹角为90度，并且它们不包含任何相同的词。

#### 4、广义Jaccard系数

 广义Jaccard系数可以用于文档数据，并在二元属性情况下归约为Jaccard系数。该系数用EJ表示：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-37-21-image.png" title="" alt="" data-align="center">

知道度量的方法后，我们还要考虑实际的邻近度计算问题

**1、距离度量的标准化和相关性**

距离度量的一个重要问题是当属性具有不同的值域时如何处理（这种情况通常称作“变量具有不同的尺度”）。前面，使用欧几里得距离，基于年龄和收入两个属性来度量人之间的距离。除非这两个属性是标准化的，否则两个人之间的距离将被收入所左右。

一个相关的问题是，除值域不同外，当某些属性之间还相关时，如何计算距离。当属性相关、具有不同的值域（不同的方差）、并且数据分布近似高斯（正态）分布时，欧几里得距离的拓广，**Mahalanobis距离**是有用。

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-37-40-image.png" title="" alt="" data-align="center">

其中∑−1是数据协方差矩阵的逆。注意，协方差矩阵∑是这样的矩阵，它的第ij个元素是第i个和第j个属性的协方差。

计算Mahalanobis距离的费用昂贵，但是对于其属性相关的对象来说是值得的。如果属性相对来说不相关，只是具有不同的值域，则只需要对变量进行标准化就足够了。

一般采用d′=(d−dmin)/(dmax−dmin)来变化欧几米得距离的特征值域。

**2、组合异种属性的相似度**

前面的相似度定义所基于的方法都假定所有属性具有相同类型。当属性具有不同类型时，就需要更一般的方法。直截了当的方法是使用上文的表分别计算出每个属性之间的相似度，然后使用一种导致0和1之间相似度的方法组合这些相似度。总相似度一般定义为所有属性相似度的平均值。

不幸的是，如果某些属性是非对称属性，这种方法效果不好。处理该问题的最简单方法是：如果两个对象在非对称属性上的值都是0，则在计算对象相似度时忽略它们。类似的方法也能很好地处理遗漏值。

概括地说，下面的算法可以有效地计算具有不同类型属性的两个对象**x**和**y**之间的相似度。修改该过程可以很轻松地处理相异度。

算法：

1：对于第k个属性，计算相似度sk(x,y)，在区间[0,1]中

2：对于第k个属性，定义一个指示变量δk，如下：

     δk=0，如果第k个属性是非对称属性，并且两个对象在该属性上的值都是0，或者如果一个对象的第k个属性具有遗漏值

     δk=0，否则

3：使用如下公式计算两个对象之间的总相似度：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-37-52-image.png" title="" alt="" data-align="center">

**3、使用权值**

在前面的大部分讨论中，所有的属性在计算临近度时都会被同等对待。但是，当某些属性对临近度的定义比其他属性更重要时，我们并不希望这种同等对待的方式。为了处理这种情况，可以通过对每个属性的贡献加权来修改临近度公式。

如果权wk的和为1，则上面的公式变成：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-38-05-image.png" title="" alt="" data-align="center">

欧几里得距离的定义也可以修改为：

<img src="file:///D:\下载\241603054刘辰雲\241603054_刘辰雲_学习笔记\imp\2025-02-15-08-38-13-image.png" title="" alt="" data-align="center">

## 算法代码

 算法伪码：

（1）计算已知类别数据集中的点与当前点之间的距离；

（2）按照距离递增次序排序；

（3）选取与当前点距离最小的k个点；

（4）确定前k个点所在类别的出现频率；

（5）返回前k个点出现频率最高的类别作为当前点的预测分类。

           # -*- coding: utf-8 -*-
    
    """kNN最近邻算法最重要的三点：
       (1)确定k值。k值过小，对噪声非常敏感；k值过大，容易误分类
       (2)采用适当的临近性度量。对于不同的类型的数据，应考虑不同的度量方法。除了距离外，也可以考虑相似性。
       (3)数据预处理。需要规范数据，使数据度量范围一致。
    """
    
    import pandas as pd
    import numpy as np
    
    class kNN:
        def __init__(self,X,y=None,test='YES'):
            """参数X为训练样本集，支持list，array和DataFrame；
    
           参数y为类标号，支持list,array,Series
           默认参数y为空值，表示类标号字段没有单独列出来，而是存储在数据集X中的最后一个字段；
           参数y不为空值时，数据集X中不能含有字段y
    
           参数test默认为'YES'，表是将原训练集拆分为测试集和新的训练集         """
        if isinstance(X,pd.core.frame.DataFrame) != True:  #将数据集转换为DataFrame格式
            self.X = pd.DataFrame(X)        else:
            self.X = X        if y is None:                                      #将特征和类别分开
            self.y = self.X.iloc[:,-1]
            self.X = self.X.iloc[:,:-1]
            self.max_data = np.max(self.X,axis=0)          #获取每个特征的最大值，为下面规范数据用
            self.min_data = np.min(self.X,axis=0)          #获取每个特征的最小值，为下面规范数据用
            max_set = np.zeros_like(self.X); max_set[:] = self.max_data  #以每个特征的最大值，构建一个与训练集结构一样的数据集
            min_set = np.zeros_like(self.X); min_set[:] = self.min_data  #以每个特征的最小值，构建一个与训练集结构一样的数据集
            self.X = (self.X - min_set)/(max_set - min_set)  #规范训练集
        else:
            self.max_data = np.max(self.X,axis=0)
            self.min_data = np.min(self.X,axis=0)
            max_set = np.zeros_like(self.X); max_set[:] = self.max_data
            min_set = np.zeros_like(self.X); min_set[:] = self.min_data
            self.X = (self.X - min_set)/(max_set - min_set)            if isinstance(y,pd.core.series.Series) != True:
                self.y = pd.Series(y)            else:
                self.y = y              if test == 'YES':        #如果test为'YES'，将原训练集拆分为测试集和新的训练集
            self.test = 'YES'    #设置self.test，后面knn函数判断测试数据需不需要再规范
            allCount = len(self.X)
            dataSet = [i for i in range(allCount)]
            testSet = []            for i in range(int(allCount*(1/5))):
                randomnum = dataSet[int(np.random.uniform(0,len(dataSet)))]
                testSet.append(randomnum)
                dataSet.remove(randomnum)
            self.X,self.testSet_X = self.X.iloc[dataSet],self.X.iloc[testSet]
            self.y,self.testSet_y = self.y.iloc[dataSet],self.y.iloc[testSet]        else:
            self.test = 'NO'
    
    def getDistances(self,point):  #计算训练集每个点与计算点的欧几米得距离
        points = np.zeros_like(self.X)   #获得与训练集X一样结构的0集
        points[:] = point                
        minusSquare = (self.X - points)**2          EuclideanDistances = np.sqrt(minusSquare.sum(axis=1))  #训练集每个点与特殊点的欧几米得距离
        return EuclideanDistances        
    def getClass(self,point,k):    #根据距离最近的k个点判断计算点所属类别
        distances = self.getDistances(point)
        argsort = distances.argsort(axis=0)     #根据数值大小，进行索引排序
        classList = list(self.y.iloc[argsort[0:k]])
        classCount = {}        for i in classList:            if i not in classCount:
                classCount[i] = 1
            else:
                classCount[i] += 1
        maxCount = 0
        maxkey = 'x'
        for key in classCount.keys():            if classCount[key] > maxCount:
                maxCount = classCount[key]
                maxkey = key        return maxkey        
    def knn(self,testData,k):     #kNN计算，返回测试集的类别
        if self.test == 'NO':     #如果self.test == 'NO'，需要规范测试数据（参照上面__init__）
            testData = pd.DataFrame(testData)
            max_set = np.zeros_like(testData); max_set[:] = self.max_data
            min_set = np.zeros_like(testData); min_set[:] = self.min_data
            testData = (testData - min_set)/(max_set - min_set)   #规范测试集
        if testData.shape == (len(testData),1):  #判断testData是否是一行记录
            label = self.getClass(testData.iloc[0],k)            return label                        #一行记录直接返回类型
        else:
            labels = []            for i in range(len(testData)):
                point = testData.iloc[i,:]
                label = self.getClass(point,k)
                labels.append(label)            return labels                       #多行记录则返回类型的列表
    
    def errorRate(self,knn_class,real_class):   #计算kNN错误率,knn_class为算法计算的类别，real_class为真实的类别
        error = 0
        allCount = len(real_class)
        real_class = list(real_class)        for i in range(allCount):            if knn_class[i] != real_class[i]:
                error += 1
        return error/allCount  

下面利用sklearn库里的iris数据（sklearn是数据挖掘算法库），进行上述代码测试

```
from sklearn import datasets
sets = datasets.load_iris() #载入iris数据集
X = sets.data #特征值数据集
y = sets.target #类别数据集
myknn = kNN(X,y)
knn_class = myknn.knn(myknn.testSet_X,4)
errorRate = myknn.errorRate(knn_class,myknn.testSet_y)
```

　　　　

# 1. 朴素贝叶斯相关的统计学知识

                在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树,KNN,逻辑回归，支持向量机等，都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系，要么是决策函数Y=f(X),要么是条件分布P(Y|X)。但是朴素贝叶斯却是生成方法，也就是直接找出特征输出Y和特征X的联合分布P(X,Y),然后用P(Y|X)=P(X,Y)/P(X)得出。

　　　　朴素贝叶斯很直观，计算量也不大，在很多领域有广泛的应用。

　　　　贝叶斯学派的思想可以概括为先验概率+数据=后验概率。也就是说在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。数据大家好理解，被频率学派攻击的是先验概率，一般来说先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，于是贝叶斯学派大胆的假设先验分布的模型，比如正态分布，beta分布等。这个假设一般没有特定的依据，因此一直被频率学派认为很荒谬。虽然难以从严密的数学逻辑里推出贝叶斯学派的逻辑，但是在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类。

　　　　条件独立公式，如果X和Y相互独立，则有：

P(X,Y)=P(X)P(Y)

　　　　条件概率公式：

P(Y|X)=P(X,Y)/P(X)

P(X|Y)=P(X,Y)/P(Y)

或者说:

P(Y|X)=P(X|Y)P(Y)/P(X)

全概率公式

P(X)=∑kP(X|Y=Yk)P(Yk)其中∑kP(Yk)=1

从上面的公式很容易得出贝叶斯公式：

P(Yk|X)=P(X|Yk)P(Yk)∑kP(X|Y=Yk)P(Yk)

# 2. 朴素贝叶斯的模型

　　　　从统计学知识回到我们的数据分析。假如分类模型样本是：(x(1)1,x(1)2,...x(1)n,y1),(x(2)1,x(2)2,...x(2)n,y2),...(x(m)1,x(m)2,...x(m)n,ym)

　　　　即有m个样本，每个样本有n个特征，特征输出有K个类别，定义为C1,C2,...,CK。

　　　　从样本可以学习得到朴素贝叶斯的先验分布P(Y=Ck)(k=1,2,...K),接着学习到条件概率分布P(X=x|Y=Ck)=P(X1=x1,X2=x2,...Xn=xn|Y=Ck),然后用贝叶斯公式得到X和Y的联合分布P(X,Y)了。联合分布P(X,Y)定义为：

P(X,Y=Ck)=P(Y=Ck)P(X=x|Y=Ck)=P(Y=Ck)P(X1=x1,X2=x2,...Xn=xn|Y=Ck)(1)(2)

　　　　从上面的式子可以看出P(Y=Ck)比较容易通过最大似然法求出，得到的P(Y=Ck)就是类别Ck在训练集里面出现的频数。但是P(X1=x1,X2=x2,...Xn=xn|Y=Ck)很难求出,这是一个超级复杂的有n个维度的条件分布。朴素贝叶斯模型在这里做了一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出:

P(X1=x1,X2=x2,...Xn=xn|Y=Ck)=P(X1=x1|Y=Ck)P(X2=x2|Y=Ck)...P(Xn=xn|Y=Ck)

　　　　从上式可以看出，这个很难的条件分布大大的简化了，但是这也可能带来预测的不准确性。如果真是非常不独立的话，那就尽量不要使用朴素贝叶斯模型了，考虑使用其他的分类方法比较好。但是一般情况下，样本的特征之间独立这个条件的确是弱成立的，尤其是数据量非常大的时候。虽然牺牲了准确性，但是得到的好处是模型的条件分布的计算大大简化了，这就是贝叶斯模型的选择。

　　　　最后回到要解决的问题，问题是给定测试集的一个新样本特征(x(test)1,x(test)2,...x(test)n，如何判断它属于哪个类型？

　　　　既然是贝叶斯模型，当然是后验概率最大化来判断分类了。只要计算出所有的K个条件概率P(Y=Ck|X=X(test)),然后找出最大的条件概率对应的类别，这就是朴素贝叶斯的预测了。

# 3. 朴素贝叶斯的推断过程

　　　　上节已经对朴素贝叶斯的模型也预测方法做了一个大概的解释，这里对朴素贝叶斯的推断过程做一个完整的诠释过程。

　　　　预测的类别Cresult是使P(Y=Ck|X=X(test))最大化的类别，数学表达式为：

Cresult=argmaxCkP(Y=Ck|X=X(test))=argmaxCkP(X=X(test)|Y=Ck)P(Y=Ck)/P(X=X(test))(3)(4)

　　　　由于对于所有的类别计算P(Y=Ck|X=X(test))时，上式的分母是一样的，都是P(X=X(test)，因此，预测公式可以简化为：

Cresult=argmaxCkP(X=X(test)|Y=Ck)P(Y=Ck)　　　

　　　　接着利用朴素贝叶斯的独立性假设，就可以得到通常意义上的朴素贝叶斯推断公式:

Cresult=argmaxCkP(Y=Ck)n∏j=1P(Xj=X(test)j|Y=Ck)

# 4. 朴素贝叶斯的参数估计

　　　　在上一节中，知道只要求出P(Y=Ck)和P(Xj=X(test)j|Y=Ck)(j=1,2,...n)，通过比较就可以得到朴素贝叶斯的推断结果。这一节讨论怎么通过训练集计算这两个概率。

　　　　对于P(Y=Ck),比较简单，通过极大似然估计我们很容易得到P(Y=Ck)为样本类别Ck出现的频率，即样本类别Ck出现的次数mk除以样本总数m。

　　　　对于P(Xj=X(train)j|Y=Ck)(j=1,2,...n),这个取决于先验条件：

　　　　a) 如果Xj是离散的值，那么我们可以假设Xj符合多项式分布，这样得到P(Xj=X(test)j|Y=Ck) 是在样本类别Ck中，特征X(train)j出现的频率。即：

P(Xj=X(train)j|Y=Ck)=mkjtrainmk

　　　　其中mk为样本类别Ck总的特征计数，而mkjtrain为类别为Ck的样本中，第j维特征X(train)j出现的计数。

　　　　某些时候，可能某些类别在样本中没有出现，这样可能导致P(Xj=X(train)j|Y=Ck)为0，这样会影响后验的估计，为了解决这种情况，引入了拉普拉斯平滑，即此时有：

P(Xj=X(train)j|Y=Ck)=mkjtrain+λmk+Ojλ　　　

　　　　其中λ 为一个大于0的常数，常常取为1。Oj为第j个特征的取值个数。

　　　　b)如果Xj是非常稀疏的离散值，即各个特征出现概率很低，这时可以假设Xj符合伯努利分布，即特征Xj出现记为1，不出现记为0。即只要Xj出现即可，我们不关注Xj的次数。这样得到P(Xj=X(train)j|Y=Ck) 是在样本类别Ck中，X(train)j出现的频率。此时有：

P(Xj=X(train)j|Y=Ck)=P(Xj=1|Y=Ck)X(train)j+(1−P(Xj=1|Y=Ck))(1−X(train)j)

　　　　其中，X(train)j取值为0和1。

　　　　c)如果Xj是连续值，通常取Xj的先验概率为正态分布，即在样本类别Ck中，Xj的值符合正态分布。这样P(Xj=X(train)j|Y=Ck)的概率分布是：

P(Xj=X(train)j|Y=Ck)=1√2πσ2kexp(−(X(train)j−μk)22σ2k)

　　　　其中μk和σ2k是正态分布的期望和方差，可以通过极大似然估计求得。μk为在样本类别Ck中，所有Xj的平均值。σ2k为在样本类别Ck中，所有Xj的方差。对于一个连续的样本值，带入正态分布的公式，就可以求出概率分布了。

# 5.  朴素贝叶斯算法过程

　　　　假设训练集为m个样本n个维度，如下：

(x(1)1,x(1)2,...x(1)n,y1),(x(2)1,x(2)2,...x(2)n,y2),...(x(m)1,x(m)2,...x(m)n,ym)

　　　　共有K个特征输出类别，分别为C1,C2,...,CK,每个特征输出类别的样本个数为m1,m2,...,mK,在第k个类别中，如果是离散特征，则特征Xj各个类别取值为mkjl。其中l取值为1,2,...Sj，Sj为特征j不同的取值数。

　　　　输出为实例X(test)的分类。

　　　　算法流程如下：

　　　　1) 如果没有Y的先验概率，则计算Y的K个先验概率：P(Y=Ck)=(mk+λ)/(m+Kλ)，否则P(Y=Ck)为输入的先验概率。

　　　　2) 分别计算第k个类别的第j维特征的第l个个取值条件概率：P(Xj=xjl|Y=Ck)

　　　　　　a)如果是离散值:

P(Xj=xjl|Y=Ck)=mkjl+λmk+Sjλ

　　　　　　λ可以取值为1，或者其他大于0的数字。

　　　　　　b)如果是稀疏二项离散值:P(Xj=xjl|Y=Ck)=P(j|Y=Ck)xjl+(1−P(j|Y=Ck)(1−xjl)

　　　　　　 此时l只有两种取值。

　　　　　　c)如果是连续值不需要计算各个l的取值概率，直接求正态分布的参数:

P(Xj=xj|Y=Ck)=1√2πσ2kexp(−(xj−μk)22σ2k)

　　　　　　需要求出μk和σ2k。 μk为在样本类别Ck中，所有Xj的平均值。σ2k为在样本类别Ck中，所有Xj的方差。

　　　　3）对于实例X(test)，分别计算：

P(Y=Ck)n∏j=1P(Xj=x(test)j|Y=Ck)

　　　　4）确定实例X(test)的分类Cresult

Cresult=argmaxCkP(Y=Ck)n∏j=1P(Xj=X(test)j|Y=Ck)

　　　　 从上面的计算可以看出，没有复杂的求导和矩阵运算，因此效率很高。

# 6.  朴素贝叶斯算法小结

　　　　朴素贝叶斯算法的主要原理基本已经做了总结，朴素贝叶斯的优缺点做一个总结。

　　　　朴素贝叶斯的主要优点有：

　　　　1）朴素贝叶斯模型发源于古典数学理论，有稳定的分类效率。

　　　　2）对小规模的数据表现很好，能个处理多分类任务，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去增量训练。

　　　　3）对缺失数据不太敏感，算法也比较简单，常用于文本分类。

　　　　朴素贝叶斯的主要缺点有：　　　

　　　　1） 理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型给定输出类别的情况下,假设属性之间相互独立，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。

　　　　2）需要知道先验概率，且先验概率很多时候取决于假设，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。

　　　　3）由于是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率。

　　　　4）对输入数据的表达形式很敏感。

**logistic回归**

#### 1. **logistic回归的基本思想**

　　logistic回归是一种分类方法，用于两分类问题。其基本思想为：

　　a. 寻找合适的假设函数，即分类函数，用以预测输入数据的判断结果；

　　b. 构造代价函数，即损失函数，用以表示预测的输出结果与训练数据的实际类别之间的偏差；

　　c. 最小化代价函数，从而获取最优的模型参数。

#### 2. **逻辑回归的过程**

　　逻辑函数（sigmoid函数）：

![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327145305061-1651951183.png)

　　该函数的图像：

![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327145353045-1320754918.png)

　　**假设函数（分类函数）：**

        ![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327145629358-1749300508.png)![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327145749154-788889441.png)

 　　判定边界：

　　 线性边界和非线性边界如下：

　　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327160012217-2050634607.png)     ![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327160034670-272041032.png)

          图1  线性边界                                               图2  非线性边界

线性边界的边界形式：

![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327160353733-1002654164.png)

非线性边界的边界形式可表示为：![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327160458248-247677565.png)

　　hθ(x)函数的值表示结果取1的概率，因此对于输入x分类结果为类别1和类别0的概率分别为：

                     ![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327160840701-1374347121.png)    （概率公式）

　　**代价函数为：**　

![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327164650373-1187972598.png)

其中：

![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327164751061-1089378039.png)，

等价于：

![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327164833373-2021621795.png)

代入得到代价函数：

![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327164942311-1694186287.png)

　　这样构建的 Cost(hθ(x),y)函数的特点是：当实际的 y=1 且 hθ 也为 1 时误差为 0，当 y=1 但 hθ 不为 1 时误差随着 hθ 的变小而变大；当实际的 y=0 且 hθ 也为 0 时代价为 0，当 y=0且hθ 不为 0 时误差随着 hθ 的变大而变大。

---

　　实际上，Cost函数和***J(θ)***函数是基于最大似然估计推导得到的。

　　前面的概率公式可简化为：

　　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327171209154-158798084.png)

　　对应的似然函数为：

　　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327171310639-1291658739.png)

　　对数似然函数为：

　　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327171439623-229059505.png)

　　最大似然估计就是要求得使***l(θ***)取最大值时的***θ***，其实这里可以使用***梯度上升法***求解，求得的***θ***就是要求的最佳参数。

  　前面提到的代价函数即可表示为：　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327171718264-129985269.png)

---

## 梯度下降法求*J(θ)*的最小值

　　使用梯度下降法求J(θ)的最小值，***θ***的更新过程：

　　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327172616514-1520076027.png)

其中：***α***学习步长。

　　求偏导：

　　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327172746061-1112884675.png)

更新过程可改写为：

![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327173002061-1029128754.png)

***α***为一常量，所以***1/m***一般将省略，所以最终的***θ***更新过程为：

![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327173100717-1554536913.png)

---

　　求解过程中用到如下的公式：

　　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327172853092-1404864409.png)

---

**梯度下降算法的向量化解法：**

 　　训练数据的矩阵形式表示如下，其中x的每一行为一条训练样本。

　　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327213710717-531806532.png)

　　参数***θ***的矩阵形式为：

　　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327213809279-919078769.png)

　　计算x.***θ***（点乘）并记为A:

 　　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327214955842-1435881919.png)

　　求***hθ(x)-y***并记为***E***：

　　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327215103561-1536674662.png)

　　***g(A)***的参数***A***为一列向量，所以实现***g***函数时要支持列向量作为参数，并返回列向量。由上式可知***hθ(x)-y***可以由***g(A)-y***一次计算求得。

　　***θ***更新过程，当***j=0***时：

　　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327215233842-1657565777.png)

　　***θj***同理：

　　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327215339795-1573401651.png)

　　综合起来：

　　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327215433467-786598610.png)

　　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327215558686-1605804486.png)

　　综上所述，vectorization后***θ***更新的步骤如下：

　　（1）求***A=x.θ***；

　　（2）求***E=g(A)-y***；

　　（3）求**θ:=****θ-α.x'.E，**x'表示矩阵x的转置。

　　也可以综合起来写成：

 　　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327215655670-1169836469.png)

　　**正则化逻辑回归：**

　　假设函数：

　　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327225801936-1389400306.png)

　　梯度下降法最小化该代价函数：

　　![](https://images2015.cnblogs.com/blog/1131087/201703/1131087-20170327230051279-1408063113.png)

# 支持向量机

支持向量机（support vector machine， 以下简称 svm）是机器学习里的重要方法，特别适用于中小型样本、非线性、高维的分类和回归问题。本系列力图展现 svm 的核心思想和完整推导过程，以飨读者。

# 一、原理概述

机器学习的一大任务就是分类（Classification）。如下图所示，假设一个二分类问题，给定一个数据集，里面所有的数据都事先被标记为两类，能很容易找到一个超平面（hyperplane）将其完美分类。  

# 

![](https://img2018.cnblogs.com/blog/1188231/201905/1188231-20190524202603094-2092182121.png)

然而实际上可以找到无数个超平面将这两类分开，那么哪一个超平面是效果最好的呢？  

# 

![](https://img2018.cnblogs.com/blog/1188231/201905/1188231-20190524202616956-412854737.png)

要回答这个问题，首先就要定义什么叫做“效果好”？在面临机器学习问题的时候普遍不是很关心训练数据的分类正确与否，而是关心一个新数据出现时其能否被模型正确分类。如果新数据被分类的准确率高，可以认为是“效果好”，或者说有较好的泛化能力。因此这里的问题就转化为：上图中哪一个超平面对新数据的分类准确率最高？

然而令人沮丧的是，没人能确切地回答哪个超平面最好，因为没人能把真实世界中的所有数据都拿过来测试。从广义上来说，大部分的理论研究都是对真实情况的模拟，譬如我们用人均收入来衡量一个国家的人民生活水平，这里的人均收入甚至只是一个不太接近的近似，因为不可能把每个国家中所有人的收入都拿出来一一比较。我们的大脑善于把繁琐的细节组合起来并高度抽象化，形成模型和假设，来逼近真实情况。

所以，在这个问题上我们能做的，也只有提出假设，建立模型，验证假设。而在 svm 中，这个假设就是：拥有最大“间隔”的超平面效果最好。

间隔（margin）指的是所有数据点中到这个超平面的最小距离。如下图所示，实线为超平面，虚线为间隔边界，黑色箭头为间隔，即虚线上的点到超平面的距离。可以看出，虚线上的三个点（2蓝1红）到超平面的距离都是一样的，实际上只有这三个点共同决定了超平面的位置，因而它们被称为“支持向量（support vectors）”，而“支持向量机”也由此而得名。

# 

![](https://img2020.cnblogs.com/blog/1188231/202004/1188231-20200427222219247-2128144902.png)

于是我们来到了svm的核心思想 —— 寻找一个超平面，使其到数据点的间隔最大。原因主要是这样的超平面分类结果较为稳健（robust），对于最难分的数据点(即离超平面最近的点)也能有足够大的确信度将它们分开，因而泛化到新数据点效果较好。

# 

![](https://img2018.cnblogs.com/blog/1188231/201905/1188231-20190524202634935-1644706046.png)

比如上左图的红线和紫线虽然能将两类数据完美分类，但注意到这两个超平面本身非常靠近数据点。以紫线为例，圆点为测试数据，其正确分类为蓝色，但因为超平面到数据点的间隔太近，以至于被错分成了黄色。 而上右图中因为间隔较大，圆点即使比所有蓝点更靠近超平面，最终也能被正确分类。

# 二、间隔最大化与优化问题（线性可分类svm）

# 

![](https://img2018.cnblogs.com/blog/1188231/201905/1188231-20190524202646714-127113500.png)

给定一组数据 {(x1,y1),(x2,y2),…,(xm,ym)}， 其中 xi∈Rd， yi∈{−1,+1} 。如果两类样本线性可分，即存在一个超平面 w⊤x+b=0 将两类样本分隔开，这样可以得到：

yisign(w⊤xi+b)=1⟺yi(w⊤xi+b)>0(1.1)

对于任意的 ζ>0 ，(1.1) 式等价于 yi(ζw⊤xi+ζb)>0 ，则说明 (w,b) 具有放缩不变性，为了后面优化方便，令 min|w⊤x+b|=1 ，即：

{w⊤xi+b⩾+1,ifyi=+1w⊤xi+b⩽−1,ifyi=−1

这样 (1.1) 式就转化为

yi(w⊤xi+b)≥1(1.2)

上一节提到我们希望间隔最大化，那么间隔该如何表示？ 样本空间任意点 x 到超平面的距离为 dist=|w⊤x+b|||w|| (证明见附录)，间隔为距离超平面 w⊤x+b=0 最近的样本到超平面距离的两倍，即：

γ=2min|w⊤x+b|||w||=2||w||

这样线性 svm 的优化目标即为：

maxw,b2||w||⟹minw,b12||w||2 s.t. yi(w⊤xi+b)≥1,i=1,2,…,m(1.3)

这是一个凸二次规划（convex quadratic programming）问题，可以用现成的软件包计算。该问题有 d+1 个变量 (d 为 xi 的维度) 和 m 个约束，如果 d 很大，则求解困难。所以现实中一般采用对偶算法（dual algorithm），通过求解原始问题（primal problem）的对偶问题（dual problem），来获得原始问题的最优解。这样做的优点，一是对偶问题往往更容易求解，二是能自然地引入核函数，进而高效地解决高维非线性分类问题。

# 三、拉格朗日对偶性

原始的优化问题 (1.3) 式与对偶问题的转换关系如下，里面的一些具体原理可参阅前文 ([拉格朗日乘子法 - KKT条件 - 对偶问题](https://www.cnblogs.com/massquantity/p/10807311.html)) ：

# 

![](https://img2018.cnblogs.com/blog/1188231/201906/1188231-20190615060235626-1645495622.png)

由拉格朗日乘子法，注意这里有 m 个样本，于是为每条约束引入拉格朗日乘子 αi⩾0 ，(1.3) 式的拉格朗日函数为：

L(w,b,α)=12||w||2+m∑i=1αi(1−yi(w⊤xi+b))(2.1)

其相应的对偶问题为：

maxαminw,b12||w||2+m∑i=1αi(1−yi(w⊤xi+b))s.t.αi⩾0,i=1,2,…,m(2.2)

上式内层对 (w,b) 的优化属于无约束优化问题，则令偏导等于零：

∂L∂w=0⟹w=m∑i=1αiyixi∂L∂b=0⟹m∑i=1αiyi=0

代入 (2.2) 式得：

maxαm∑i=1αi−12m∑i=1m∑j=1αiαjyiyjx⊤ixjs.t.αi⩾0,i=1,2,…,mm∑i=1αiyi=0(2.3)

此依然属于二次规划问题，求出 α 后，即可得原始问题的参数 (w,b) 。该二次规划问题有 m 个变量， (m+1)项约束，因而适用于 d 很大，m 适中 ( d>>m ) 的情况。而当 m 很大时，问题变得不可解，这时候可使用 SMO 这类专为 svm 设计的优化算法，后文详述。 另外上文也提到 (1.3) 式的原始二次规划问题适用于 d 比较小的低维数据，所以 scikit-learn 中的 [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) 有 **dual** : bool, (default=True) 这个选项，当样本数 > 特征数时，宜设为 False，选择解原始优化问题，可见使用 svm 并不是非要转化为对偶问题不可 。而相比之下 [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) 则没有 dual 这个选项，因为 SVC 中实现了核函数，必须先转化为对偶问题，后文再议。

[前文](https://www.cnblogs.com/massquantity/p/10807311.html)已证明 (2.2) 式的最优解应满足 KKT 条件，由 KKT 条件中的互补松弛条件可知 αi(1−yi(w⊤xi+b))=0 。当 αi>0 时， yi(w⊤xi+b)=1 ，说明这些样本点 (xi,yi) 一定在间隔边界上，它们被称为“支持向量”，这些点共同决定了分离超平面。这是 svm 的一个重要特性： 训练完成后，大部分训练样本不需要保留，最终模型仅与支持向量有关。

于是可根据支持向量求 w：

w=m∑i=1αiyixi=m∑i:αi=00⋅yixi+m∑i:αi>0αiyixi=∑i∈SVαiyixi(2.5)

其中 SV 代表所有支持向量的集合。对于任意支持向量 (xs,ys)，都有 ys(w⊤xs+b)=1 。将 (2.5) 式代入，并利用 y2s=1 ：

ys(∑i∈SVαiyix⊤ixs+b)=y2s⟹b=ys−∑i∈SVαiyix⊤ixs

实践中，为了得到对 b 更稳健的估计，通常对所有支持向量求解得到 b 的平均值：

b=1|SV|∑s∈SV(ys−∑i∈SVαiyix⊤ixs)

于是线性 svm 的假设函数可表示为：

f(x)=sign(∑i∈SVαiyix⊤ix+b)

---

## 点到超平面的距离

# 

![](https://img2018.cnblogs.com/blog/1188231/201905/1188231-20190524202920678-1392694048.png)

如上图所示，想要求 x 到超平面 w⊤x+b=0 的距离 d 。设 x0 和 x1 是超平面上的两个点，则：

{w⊤x0+b=0w⊤x1+b=0⟹w⊤(x0−x1)=0

即 w 垂直于超平面。

证明1：

d=||x−x1||cosθ=||x−x1|||w⊤(x−x1)|||w||⋅||x−x1||=|w⊤x−w⊤x1|||w||=|w⊤x+b|||w||

证明2： 由于 w||w|| 为单位向量：

w⊤x0+b=w⊤(x−d⋅w||w||)+b=0⟹d=|w⊤x+b|||w||

# 人工神经网络--ANN

　　神经网络是一种模拟人脑的神经网络以期能够实现类人工智能的机器学习技术。人脑中的神经网络是一个非常复杂的组织。成人的大脑中估计有1000亿个神经元之多。

　　那么机器学习中的神经网络是如何实现这种模拟的，并且达到一个惊人的良好效果的？

**一. 前言**

　　让我们来看一个经典的神经网络。这是一个包含三个层次的神经网络。红色的是**输入层**，绿色的是**输出层**，紫色的是**中间层**（也叫**隐藏层**）。输入层有3个输入单元，隐藏层有4个单元，输出层有2个单元。后文中，我们统一使用这种颜色来表达神经网络的结构。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151219151604318-1557737289.jpg)

1. 设计一个神经网络时，输入层与输出层的节点数往往是固定的，中间层则可以自由指定；
2. 神经网络结构图中的拓扑与箭头代表着**预测**过程时数据的流向，跟**训练**时的数据流有一定的区别；
3. 结构图里的关键不是圆圈（代表“神经元”），而是连接线（代表“神经元”之间的连接）。每个连接线对应一个不同的**权重**（其值称为**权值**），这是需要训练得到的。  

　　除了从左到右的形式表达的结构图，还有一种常见的表达形式是从下到上来表示一个神经网络。这时候，输入层在图的最下方。输出层则在图的最上方，如下图：

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151219174631693-775181930.jpg)

　　从左到右的表达形式以Andrew Ng和LeCun的文献使用较多，Caffe里使用的则是从下到上的表达。在本文中使用Andrew Ng代表的从左到右的表达形式。

　　下面从简单的神经元开始说起，一步一步介绍神经网络复杂结构的形成。

**二. 神经元**

　　**1.引子**　

　　对于神经元的研究由来已久，1904年生物学家就已经知晓了神经元的组成结构。

　　一个神经元通常具有多个**树突**，主要用来接受传入信息；而**轴突**只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“**突触**”。

　　人脑中的神经元形状可以用下图做简单的说明：

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151229121248198-818698949.jpg)

　　**2.结构** 

　　神经元模型是一个包含输入，输出与计算功能的模型。输入可以类比为神经元的树突，而输出可以类比为神经元的轴突，计算则可以类比为细胞核。

　　下图是一个典型的神经元模型：包含有3个输入，1个输出，以及2个计算功能。

　　注意中间的箭头线。这些线称为“连接”。每个上有一个“权值”。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151219153856802-307732621.jpg)

　　连接是神经元中最重要的东西。每一个连接上都有一个权重。

　　一个神经网络的训练算法就是让权重的值调整到最佳，以使得整个网络的预测效果最好。

　　使用a来表示输入，用w来表示权值。一个表示连接的有向箭头可以这样理解：在初端，传递的信号大小仍然是a，端中间有加权参数w，经过这个加权后的信号会变成a*w，因此在连接的末端，信号的大小就变成了a*w。

　　在其他绘图模型里，有向箭头可能表示的是值的不变传递。而在神经元模型里，每个有向箭头表示的是值的加权传递。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151219180614819-1652574235.jpg)

　　如果将神经元图中的所有变量用符号表示，并且写出输出的计算公式的话，就是下图。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230201441792-1505283920.jpg)

　　可见z是在输入和权值的线性加权和叠加了一个**函数g**的值。在MP模型里，函数g是sgn函数，也就是取符号函数。这个函数当输入大于0时，输出1，否则输出0。

　　下面对神经元模型的图进行一些扩展。首先将sum函数与sgn函数合并到一个圆圈里，代表神经元的内部计算。其次，把输入a与输出z写到连接线的左上方，便于后面画复杂的网络。最后说明，一个神经元可以引出多个代表输出的有向箭头，但值都是一样的。

　　神经元可以看作一个计算与存储单元。计算是神经元对其的输入进行计算功能。存储是神经元会暂存计算结果，并传递到下一层。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230204036479-461440948.jpg)

　　当“神经元”组成网络以后，描述网络中的某个“神经元”时，更多地会用“**单元**”（unit）来指代。同时由于神经网络的表现形式是一个有向图，有时也会用“**节点**”（node）来表达同样的意思。 

　　**3.效果** 

　　神经元模型的使用可以这样理解：

        样本有四个属性，其中三个属性已知，一个属性未知。我们需要做的就是通过三个已知属性**预测**未知属性。

　　具体办法就是使用神经元的公式进行计算。三个已知属性的值是a1，a2，a3，未知属性的值是z。z可以通过公式计算出来。

　　这里，已知的属性称之为**特征**，未知的属性称之为**目标**。假设特征与目标之间确实是线性关系，并且我们已经得到表示这个关系的权值w1，w2，w3。那么通过神经元模型预测新样本的目标。

　　**4.影响**

　　1943年发布的MP模型，虽然简单，但已经建立了神经网络大厦的地基。但是，MP模型中，权重的值都是预先设置的，因此不能学习。

　　1949年心理学家Hebb提出了Hebb学习率，认为人脑神经细胞的**突触**（也就是连接）上的强度上可以变化的。于是计算科学家们开始考虑用调整权值的方法来让机器学习。这为后面的学习算法奠定了基础。

　　尽管神经元模型与Hebb学习律都已诞生，但限于当时的计算机能力，直到接近10年后，第一个真正意义的神经网络才诞生。

**三. 单层神经网络（感知器）**

　　**1.引子**　　

　　1958年，计算科学家Rosenblatt提出了由两层神经元组成的神经网络。他给它起了一个名字--“感知器”（Perceptron）（有的文献翻译成“感知机”，下文统一用“感知器”来指代）。

　　感知器是当时首个可以学习的人工神经网络。Rosenblatt现场演示了其学习识别简单图像的过程，在当时的社会引起了轰动。

　　人们认为已经发现了智能的奥秘，许多学者和科研机构纷纷投入到神经网络的研究中。美国军方大力资助了神经网络的研究，并认为神经网络比“原子弹工程”更重要。这段时间直到1969年才结束，这个时期可以看作神经网络的第一次高潮。

　　**2.结构**

　　下面来说明感知器模型。

　　在原来MP模型的“输入”位置添加神经元节点，标志其为“输入单元”。其余不变，于是我们就有了下图：从本图开始，我们将权值w1, w2, w3写到“连接线”的中间。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151221151959015-1876891081.jpg)

　　在“感知器”中，有两个层次。分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算。输出层里的“输出单元”则需要对前面一层的输入进行计算。

　　我们把需要计算的层次称之为“计算层”，并把拥有一个计算层的网络称之为“单层神经网络”。有一些文献会按照网络拥有的层数来命名，例如把“感知器”称为两层神经网络。但在本文里，我们根据计算层的数量来命名。

　　假如我们要预测的目标不再是一个值，而是一个向量，例如[2,3]。那么可以在输出层再增加一个“输出单元”。

　　下图显示了带有两个输出单元的单层神经网络，其中输出单元z1的计算公式如下图。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230204223917-579926148.jpg)

　　可以看到，z1的计算跟原先的z并没有区别。

　　我们已知一个神经元的输出可以向多个神经元传递，因此z2的计算公式如下图。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230204258057-82126781.jpg)

　　可以看到，z2的计算中除了三个新的权值：w4，w5，w6以外，其他与z1是一样的。

　　整个网络的输出如下图。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230204606760-610264555.jpg)

　　目前的表达公式有一点不让人满意的就是：w4，w5，w6是后来加的，很难表现出跟原先的w1，w2，w3的关系。

　　因此我们改用二维的下标，用wx,y来表达一个权值。下标中的x代表后一层神经元的序号，而y代表前一层神经元的序号（序号的顺序从上到下）。

　　例如，w1,2代表后一层的第1个神经元与前一层的第2个神经元的连接的权值（这种标记方式参照了Andrew Ng的课件）。根据以上方法标记，我们有了下图。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151230205437995-673856644.jpg)

　　如果我们仔细看输出的计算公式，会发现这两个公式就是线性代数方程组。因此可以用矩阵乘法来表达这两个公式。

　　例如，输入的变量是[a1，a2，a3]T（代表由a1，a2，a3组成的列向量），用向量**a**来表示。方程的左边是[z1，z2]T，用向量**z**来表示。

　　系数则是矩阵**W**（2行3列的矩阵，排列形式与公式中的一样）。

　　于是，输出公式可以改写成：

g(**W** * **a**) = **z**;

　　这个公式就是神经网络中从前一层计算后一层的**矩阵运算。**

　　**3.效果**

　　与神经元模型不同，感知器中的权值是通过训练得到的。因此，根据以前的知识我们知道，感知器类似一个**逻辑回归**模型，可以做线性分类任务。

　　可以用**决策分界**来形象的表达分类的效果。决策分界就是在二维的数据平面中划出一条直线，当数据的维度是3维的时候，就是划出一个平面，当数据的维度是n维时，就是划出一个n-1维的超平面。

　　下图显示了在二维平面中划出决策分界的效果，也就是感知器的分类效果。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151231073138323-962584420.png)

　　**4.影响**　

　　感知器只能做简单的线性分类任务。但是当时的人们热情太过于高涨，并没有人清醒的认识到这点。于是，当人工智能领域的巨擘Minsky指出这点时，事态就发生了变化。

　　Minsky在1969年出版了一本叫《Perceptron》的书，里面用详细的数学证明了感知器的弱点，尤其是感知器对XOR（异或）这样的简单分类任务都无法解决。

　　Minsky认为，如果将计算层增加到两层，计算量则过大，而且没有有效的学习算法。所以，他认为研究更深层的网络是没有价值的。（本文成文后一个月，即2016年1月，Minsky在美国去世。谨在本文中纪念这位著名的计算机研究专家与大拿。）

　　由于Minsky的巨大影响力以及书中呈现的悲观态度，让很多学者和实验室纷纷放弃了神经网络的研究。神经网络的研究陷入了冰河期。这个时期又被称为“AI winter”。

　　接近10年以后，对于两层神经网络的研究才带来神经网络的复苏。

**四. 两层神经网络（多层感知器）**

　　**1.引子**

　　两层神经网络是本文的重点，因为正是在这时候，神经网络开始了大范围的推广与使用。

　　Minsky说过单层神经网络无法解决异或问题。但是当增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。不过两层神经网络的计算是一个问题，没有一个较好的解法。

　　1986年，Rumelhar和Hinton等人提出了反向传播（Backpropagation，BP）算法，解决了两层神经网络所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。目前，大量的教授神经网络的教材，都是重点介绍两层（带一个隐藏层）神经网络的内容。 

　　这时候的Hinton还很年轻，30年以后，正是他重新定义了神经网络，带来了神经网络复苏的又一春。

　　**2.结构**

　　两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，中间层和输出层都是计算层。我们扩展上节的单层神经网络，在右边新加一个层次（只含有一个节点）。

　　现在，我们的权值矩阵增加到了两个，我们用上标来区分不同层次之间的变量。

　　例如ax(y)代表第y层的第x个节点。z1，z2变成了a1(2)，a2(2)。下图给出了a1(2)，a2(2)的计算公式。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151222164731249-360921014.jpg)

　　计算最终输出z的方式是利用了中间层的a1(2)，a2(2)和第二个权值矩阵计算得到的，如下图。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151222171056156-387680541.jpg)

　　假设我们的预测目标是一个向量，那么与前面类似，只需要在“输出层”再增加节点即可。

　　我们使用向量和矩阵来表示层次中的变量。**a**(1)，**a**(2)，**z**是网络中传输的向量数据。**W**(1)和**W**(2)是网络的矩阵参数。如下图。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151222171328140-1303075636.jpg)

　　使用矩阵运算来表达整个计算公式的话如下：

  g(**W**(1) * **a**(1)) = **a**(2); 

g(**W**(2) * **a**(2)) = **z**;

　　由此可见，使用矩阵运算来表达是很简洁的，而且也不会受到节点数增多的影响（无论有多少节点参与运算，乘法两端都只有一个变量）。因此神经网络的教程中大量使用矩阵运算来描述。

　　需要说明的是，至今为止，我们对神经网络的结构图的讨论中都没有提到偏置节点（bias unit）。事实上，这些节点是默认存在的。它本质上是一个只含有存储功能，且存储值永远为1的单元。在神经网络的每个层次中，除了输出层以外，都会含有这样一个偏置单元。正如线性回归模型与逻辑回归模型中的一样。

　　偏置单元与后一层的所有节点都有连接，我们设这些参数值为向量**b**，称之为偏置。如下图。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151226111144687-604911384.jpg)

　　可以看出，偏置节点很好认，因为其没有输入（前一层中没有箭头指向它）。有些神经网络的结构图中会把偏置节点明显画出来，有些不会。一般情况下，我们都不会明确画出偏置节点。 

　　在考虑了偏置以后的一个神经网络的矩阵运算如下：

  g(**W**(1) * **a**(1) + **b**(1)) = **a**(2); 

g(**W**(2) * **a**(2) + **b**(2)) = **z**;

　　需要说明的是，在两层神经网络中，我们不再使用sgn函数作为函数g，而是使用平滑函数sigmoid作为函数g。我们把函数g也称作激活函数（active function）。

　　事实上，神经网络的本质就是通过参数与激活函数来拟合特征与目标之间的真实函数关系。初学者可能认为画神经网络的结构图是为了在程序中实现这些圆圈与线，但在一个神经网络的程序中，既没有“线”这个对象，也没有“单元”这个对象。实现一个神经网络最需要的是线性代数库。

　　**3.效果**

　　与单层神经网络不同。理论证明，两层神经网络可以无限逼近任意连续函数。

　　这是什么意思呢？也就是说，面对复杂的非线性分类任务，两层（带一个隐藏层）神经网络可以分类的很好。

　　下面就是一个例子，红色的线与蓝色的线代表数据。而红色区域和蓝色区域代表由神经网络划开的区域，两者的分界线就是决策分界。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151231073619073-461403542.png)

　　可以看到，这个两层神经网络的决策分界是非常平滑的曲线，而且分类的很好。有趣的是，前面已经学到过，单层网络只能做线性分类任务。而两层神经网络中的后一层也是线性分类层，应该只能做线性分类任务。为什么两个线性分类任务结合就可以做非线性分类任务？

　　我们可以把输出层的决策分界单独拿出来看一下。就是下图。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151231074314604-2050732128.png)

　　可以看到，输出层的决策分界仍然是直线。关键就是，从输入层到隐藏层时，数据发生了空间变换。也就是说，两层神经网络中，隐藏层对原始的数据进行了一个空间变换，使其可以被线性分类，然后输出层的决策分界划出了一个线性分类分界线，对其进行分类。

　　这样就导出了两层神经网络可以做非线性分类的关键--隐藏层。联想到我们一开始推导出的矩阵公式，我们知道，矩阵和向量相乘，本质上就是对向量的坐标空间进行一个变换。因此，隐藏层的参数矩阵的作用就是使得数据的原始坐标空间从线性不可分，转换成了线性可分。

　　两层神经网络通过两层的线性模型模拟了数据内真实的非线性函数。因此，多层的神经网络的本质就是复杂函数拟合。

　　下面来讨论一下隐藏层的节点数设计。在设计一个神经网络时，输入层的节点数需要与特征的维度匹配，输出层的节点数要与目标的维度匹配。而中间层的节点数，却是由设计者指定的。因此，“自由”把握在设计者的手中。但是，节点数设置的多少，却会影响到整个模型的效果。如何决定这个自由层的节点数呢？目前业界没有完善的理论来指导这个决策。一般是根据经验来设置。较好的方法就是预先设定几个可选值，通过切换这几个值来看整个模型的预测效果，选择效果最好的值作为最终选择。这种方法又叫做Grid Search（网格搜索）。

　　了解了两层神经网络的结构以后，我们就可以看懂其它类似的结构图。例如EasyPR字符识别网络架构（下图）。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151226122337406-1923048422.jpg)

　　EasyPR使用了字符的图像去进行字符文字的识别。输入是120维的向量。输出是要预测的文字类别，共有65类。根据实验，我们测试了一些隐藏层数目，发现当值为40时，整个网络在测试集上的效果较好，因此选择网络的最终结构就是120，40，65。

　　**4.训练**

　　下面简单介绍一下两层神经网络的训练。

　　在Rosenblat提出的感知器模型中，模型中的参数可以被训练，但是使用的方法较为简单，并没有使用目前机器学习中通用的方法，这导致其扩展性与适用性非常有限。从两层神经网络开始，神经网络的研究人员开始使用机器学习相关的技术进行神经网络的训练。例如用大量的数据（1000-10000左右），使用算法进行优化等等，从而使得模型训练可以获得性能与数据利用上的双重优势。

　　机器学习模型训练的目的，就是使得参数尽可能的与真实的模型逼近。具体做法是这样的。首先给所有参数赋上随机值。我们使用这些随机生成的参数值，来预测训练数据中的样本。样本的预测目标为yp，真实目标为y。那么，定义一个值loss，计算公式如下。

loss = (yp - y)2

　　这个值称之为**损失**（loss），我们的目标就是使对所有训练数据的损失和尽可能的小。

　　如果将先前的神经网络预测的矩阵公式带入到yp中（因为有z=yp），那么我们可以把损失写为关于参数（parameter）的函数，这个函数称之为**损失函数**（loss function）。下面的问题就是求：如何优化参数，能够让损失函数的值最小。

　　此时这个问题就被转化为一个优化问题。一个常用方法就是高等数学中的求导，但是这里的问题由于参数不止一个，求导后计算导数等于0的运算量很大，所以一般来说解决这个优化问题使用的是**梯度下降**算法。梯度下降算法每次计算参数在当前的梯度，然后让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时截止。一般这个时候，所有的参数恰好达到使损失函数达到一个最低值的状态。

　　在神经网络模型中，由于结构复杂，每次计算梯度的代价很大。因此还需要使用**反向传播**算法。反向传播算法是利用了神经网络的结构进行的计算。不一次计算所有参数的梯度，而是从后往前。首先计算输出层的梯度，然后是第二个参数矩阵的梯度，接着是中间层的梯度，再然后是第一个参数矩阵的梯度，最后是输入层的梯度。计算结束以后，所要的两个参数矩阵的梯度就都有了。

　　反向传播算法可以直观的理解为下图。梯度的计算从后往前，一层层反向传播。前缀E代表着相对导数的意思。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151229120754198-2003498733.jpg)

　　反向传播算法的启示是数学中的**链式法则**。在此需要说明的是，尽管早期神经网络的研究人员努力从生物学中得到启发，但从BP算法开始，研究者们更多地从数学上寻求问题的最优解。不再盲目模拟人脑网络是神经网络研究走向成熟的标志。正如科学家们可以从鸟类的飞行中得到启发，但没有必要一定要完全模拟鸟类的飞行方式，也能制造可以飞天的飞机。

　　优化问题只是训练中的一个部分。机器学习问题之所以称为学习问题，而不是优化问题，就是因为它不仅要求数据在训练集上求得一个较小的误差，在测试集上也要表现好。因为模型最终是要部署到没有见过训练数据的真实场景。提升模型在测试集上的预测效果的主题叫做**泛化**（generalization），相关方法被称作正则化（regularization）。神经网络中常用的泛化技术有**权重衰减**等。

　　**5.影响**

　　两层神经网络在多个地方的应用说明了其效用与价值。10年前困扰神经网络界的异或问题被轻松解决。神经网络在这个时候，已经可以发力于语音识别，图像识别，自动驾驶等多个领域。

　　历史总是惊人的相似，神经网络的学者们再次登上了《纽约时报》的专访。人们认为神经网络可以解决许多问题。就连娱乐界都开始受到了影响，当年的《终结者》电影中的阿诺都赶时髦地说一句：我的CPU是一个神经网络处理器，一个会学习的计算机。

　　但是神经网络仍然存在若干的问题：尽管使用了BP算法，一次神经网络的训练仍然耗时太久，而且困扰训练优化的一个问题就是局部最优解问题，这使得神经网络的优化较为困难。同时，隐藏层的节点数需要调参，这使得使用不太方便，工程和研究人员对此多有抱怨。

　　90年代中期，由Vapnik等人发明的SVM（Support Vector Machines，支持向量机）算法诞生，很快就在若干个方面体现出了对比神经网络的优势：无需调参；高效；全局最优解。基于以上种种理由，SVM迅速打败了神经网络算法成为主流。

　　神经网络的研究再次陷入了冰河期。当时，只要你的论文中包含神经网络相关的字眼，非常容易被会议和期刊拒收，研究界那时对神经网络的不待见可想而知。

**五. 多层神经网络（深度学习）**

　　**1.引子**　　

　　在被人摒弃的10年中，有几个学者仍然在坚持研究。这其中的棋手就是加拿大多伦多大学的Geoffery Hinton教授。

　　2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。与传统的训练方式不同，“深度信念网络”有一个“**预训练**”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“**微调**”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法赋予了一个新名词--“**深度学习**”。

 　　很快，深度学习在语音识别领域暂露头角。接着，2012年，深度学习技术又在图像识别领域大展拳脚。Hinton与他的学生在ImageNet竞赛中，用多层的卷积神经网络成功地对包含一千类别的一百万张图片进行了训练，取得了分类错误率15%的好成绩，这个成绩比第二名高了近11个百分点，充分证明了多层神经网络识别效果的优越性。

　　在这之后，关于深度神经网络的研究与应用不断涌现。

　　**2.结构**

　　我们延续两层神经网络的方式来设计一个多层神经网络。

　　在两层神经网络的输出层后面，继续添加层次。原来的输出层变成中间层，新加的层次成为新的输出层。所以可以得到下图。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151224204339234-1994620313.jpg)

　　依照这样的方式不断添加，我们可以得到更多层的多层神经网络。公式推导的话其实跟两层神经网络类似，使用矩阵运算的话就仅仅是加一个公式而已。

　　在已知输入**a**(1)，参数**W**(1)，**W**(2)，**W**(3)的情况下，输出**z**的推导公式如下：

     g(**W**(1) * **a**(1)) = **a**(2); 

    g(**W**(2) * **a**(2)) = **a**(3);

g(**W**(3) * **a**(3)) = **z**;

　　多层神经网络中，输出也是按照一层一层的方式来计算。从最外面的层开始，算出所有单元的值以后，再继续计算更深一层。只有当前层所有单元的值都计算完毕以后，才会算下一层。有点像计算向前不断推进的感觉。所以这个过程叫做“正向传播”。

　　下面讨论一下多层神经网络中的参数。

　　首先我们看第一张图，可以看出**W**(1)中有6个参数，**W**(2)中有4个参数，**W**(3)中有6个参数，所以整个神经网络中的参数有16个（这里我们不考虑偏置节点，下同）。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151224212531484-570745053.jpg) 

　　假设我们将中间层的节点数做一下调整。第一个中间层改为3个单元，第二个中间层改为4个单元。

　　经过调整以后，整个网络的参数变成了33个。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151224213620234-1075501325.jpg)

　　虽然层数保持不变，但是第二个神经网络的参数数量却是第一个神经网络的接近两倍之多，从而带来了更好的表示（represention）能力。表示能力是多层神经网络的一个重要性质，下面会做介绍。

　　在参数一致的情况下，我们也可以获得一个“更深”的网络。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151224213703109-813423001.jpg) 

　　上图的网络中，虽然参数数量仍然是33，但却有4个中间层，是原来层数的接近两倍。这意味着一样的参数数量，可以用更深的层次去表达。

　　**3.效果**

　　与两层层神经网络不同。多层神经网络中的层数增加了很多。

　　增加更多的层次有什么好处？更深入的表示特征，以及更强的函数模拟能力。

　　更深入的表示特征可以这样理解，随着网络的层数增加，每一层对于前一层次的抽象表示更深入。在神经网络中，每一层神经元学习到的是前一层神经元值的更抽象的表示。例如第一个隐藏层学习到的是“边缘”的特征，第二个隐藏层学习到的是由“边缘”组成的“形状”的特征，第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征。通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。

　　关于逐层特征学习的例子，可以参考下图。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151231075103229-1126297331.png) 

　　更强的函数模拟能力是由于随着层数的增加，整个网络的参数就越多。而神经网络其实本质就是模拟特征与目标之间的真实关系函数的方法，更多的参数意味着其模拟的函数可以更加的复杂，可以有更多的**容量**（capcity）去拟合真正的关系。

　　通过研究发现，在参数数量一样的情况下，更深的网络往往具有比浅层的网络更好的识别效率。这点也在ImageNet的多次大赛中得到了证实。从2012年起，每年获得ImageNet冠军的深度神经网络的层数逐年增加，2015年最好的方法GoogleNet是一个多达22层的神经网络。

　　在最新一届的ImageNet大赛上，目前拿到最好成绩的MSRA团队的方法使用的更是一个深达152层的网络！关于这个方法更多的信息有兴趣的可以查阅ImageNet网站。

　　**4.训练**

　　在单层神经网络时，我们使用的激活函数是sgn函数。到了两层神经网络时，我们使用的最多的是sigmoid函数。而到了多层神经网络时，通过一系列的研究发现，ReLU函数在训练多层神经网络时，更容易收敛，并且预测性能更好。因此，目前在深度学习中，最流行的非线性函数是ReLU函数。ReLU函数不是传统的非线性函数，而是分段线性函数。其表达式非常简单，就是y=max(x,0)。简而言之，在x大于0，输出就是输入，而在x小于0时，输出就保持为0。这种函数的设计启发来自于生物神经元对于激励的线性响应，以及当低于某个阈值后就不再响应的模拟。

　　在多层神经网络中，训练的主题仍然是优化和泛化。当使用足够强的计算芯片（例如GPU图形加速卡）时，梯度下降算法以及反向传播算法在多层神经网络中的训练中仍然工作的很好。目前学术界主要的研究既在于开发新的算法，也在于对这两个算法进行不断的优化，例如，增加了一种带动量因子（momentum）的梯度下降算法。　

　　在深度学习中，泛化技术变的比以往更加的重要。这主要是因为神经网络的层数增加了，参数也增加了，表示能力大幅度增强，很容易出现**过拟合现象**。因此正则化技术就显得十分重要。目前，Dropout技术，以及数据扩容（Data-Augmentation）技术是目前使用的最多的正则化技术。

　　多层神经网络的研究仍在进行中。现在最为火热的研究技术包括RNN，LSTM等，研究方向则是图像理解方面。图像理解技术是给计算机一幅图片，让它用语言来表达这幅图片的意思。ImageNet竞赛也在不断召开，有更多的方法涌现出来，刷新以往的正确率。

**六. 回顾**

　　**1.影响**　　

　　我们回顾一下神经网络发展的历程。神经网络的发展历史曲折荡漾，既有被人捧上天的时刻，也有摔落在街头无人问津的时段，中间经历了数次大起大落。

　　从单层神经网络（感知器）开始，到包含一个隐藏层的两层神经网络，再到多层的深度神经网络，一共有三次兴起过程。详见下图。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151228170208120-1856567090.jpg) 

　　上图中的顶点与谷底可以看作神经网络发展的高峰与低谷。图中的横轴是时间，以年为单位。纵轴是一个神经网络影响力的示意表示。如果把1949年Hebb模型提出到1958年的感知机诞生这个10年视为落下（没有兴起）的话，那么神经网络算是经历了“三起三落”这样一个过程，跟“小平”同志类似。俗话说，天将降大任于斯人也，必先苦其心志，劳其筋骨。经历过如此多波折的神经网络能够在现阶段取得成功也可以被看做是磨砺的积累吧。

　　历史最大的好处是可以给现在做参考。科学的研究呈现螺旋形上升的过程，不可能一帆风顺。同时，这也给现在过分热衷深度学习与人工智能的人敲响警钟，因为这不是第一次人们因为神经网络而疯狂了。1958年到1969年，以及1985年到1995，这两个十年间人们对于神经网络以及人工智能的期待并不现在低，可结果如何大家也能看的很清楚。

　　因此，冷静才是对待目前深度学习热潮的最好办法。如果因为深度学习火热，或者可以有“钱景”就一窝蜂的涌入，那么最终的受害人只能是自己。神经网络界已经两次有被人们捧上天了的境况，相信也对于捧得越高，摔得越惨这句话深有体会。因此，神经网络界的学者也必须给这股热潮浇上一盆水，不要让媒体以及投资家们过分的高看这门技术。很有可能，三十年河东，三十年河西，在几年后，神经网络就再次陷入谷底。根据上图的历史曲线图，这是很有可能的。

　　**2.效果**　　

　　下面说一下神经网络为什么能这么火热？简而言之，就是其学习效果的强大。随着神经网络的发展，其表示性能越来越强。

　　从单层神经网络，到两层神经网络，再到多层神经网络，下图说明了，随着网络层数的增加，以及激活函数的调整，神经网络所能拟合的决策分界平面的能力。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151228134016120-1091351096.jpg) 

　　可以看出，随着层数增加，其非线性分界拟合能力不断增强。图中的分界线并不代表真实训练出的效果，更多的是示意效果。

　　神经网络的研究与应用之所以能够不断地火热发展下去，与其强大的函数拟合能力是分不开关系的。

　　**3.外因**　　

　　当然，光有强大的内在能力，并不一定能成功。一个成功的技术与方法，不仅需要内因的作用，还需要时势与环境的配合。神经网络的发展背后的外在原因可以被总结为：更强的计算性能，更多的数据，以及更好的训练方法。只有满足这些条件时，神经网络的函数拟合能力才能得已体现，见下图。

![](https://images2015.cnblogs.com/blog/673793/201512/673793-20151228170149135-2107087462.jpg) 

　　之所以在单层神经网络年代，Rosenblat无法制作一个双层分类器，就在于当时的计算性能不足，Minsky也以此来打压神经网络。但是Minsky没有料到，仅仅10年以后，计算机CPU的快速发展已经使得我们可以做两层神经网络的训练，并且还有快速的学习算法BP。

　　但是在两层神经网络快速流行的年代。更高层的神经网络由于计算性能的问题，以及一些计算方法的问题，其优势无法得到体现。直到2012年，研究人员发现，用于高性能计算的图形加速卡（GPU）可以极佳地匹配神经网络训练所需要的要求：高并行性，高存储，没有太多的控制需求，配合预训练等算法，神经网络才得以大放光彩。

　　互联网时代，大量的数据被收集整理，更好的训练方法不断被发现。所有这一切都满足了多层神经网络发挥能力的条件。

　　“时势造英雄”，正如Hinton在2006年的论文里说道的

　　“**... provided that computers were fast enough, data sets were big enough, and the initial weights were close enough to a good solution. All three conditions are now satisfied.**”，

　　外在条件的满足也是神经网络从神经元得以发展到目前的深度神经网络的重要因素。

　　除此以外，一门技术的发扬没有“伯乐”也是不行的。在神经网络漫长的历史中，正是由于许多研究人员的锲而不舍，不断钻研，才能有了现在的成就。前期的Rosenblat，Rumelhart没有见证到神经网络如今的流行与地位。但是在那个时代，他们为神经网络的发展所打下的基础，却会永远流传下去，不会退色。

**七. 展望**

　　**1.量子计算**

　　回到我们对神经网络历史的讨论，根据历史趋势图来看，神经网络以及深度学习会不会像以往一样再次陷入谷底？作者认为，这个过程可能取决于量子计算机的发展。

　　根据一些最近的研究发现，人脑内部进行的计算可能是类似于量子计算形态的东西。而且目前已知的最大神经网络跟人脑的神经元数量相比，仍然显得非常小，仅不及1%左右。所以未来真正想实现人脑神经网络的模拟，可能需要借助量子计算的强大计算能力。

　　各大研究组也已经认识到了量子计算的重要性。谷歌就在开展量子计算机D-wave的研究，希望用量子计算来进行机器学习，并且在前段时间有了突破性的进展。国内方面，阿里和中科院合作成立了量子计算实验室，意图进行量子计算的研究。

　　如果量子计算发展不力，仍然需要数十年才能使我们的计算能力得以突飞猛进的发展，那么缺少了强大计算能力的神经网络可能会无法一帆风顺的发展下去。这种情况可以类比为80-90年时期神经网络因为计算能力的限制而被低估与忽视。假设量子计算机真的能够与神经网络结合，并且助力真正的人工智能技术的诞生，而且量子计算机发展需要10年的话，那么神经网络可能还有10年的发展期。直到那时期以后，神经网络才能真正接近实现AI这一目标。

　　**2.人工智能**

　　最后，作者想简单地谈谈对目前人工智能的看法。虽然现在人工智能非常火热，但是距离真正的人工智能还有很大的距离。就拿计算机视觉方向来说，面对稍微复杂一些的场景，以及易于混淆的图像，计算机就可能难以识别。因此，这个方向还有很多的工作要做。

　　就普通人看来，这么辛苦的做各种实验，以及投入大量的人力就是为了实现一些不及孩童能力的视觉能力，未免有些不值。但是这只是第一步。虽然计算机需要很大的运算量才能完成一个普通人简单能完成的识图工作，但计算机最大的优势在于并行化与批量推广能力。使用计算机以后，我们可以很轻易地将以前需要人眼去判断的工作交给计算机做，而且几乎没有任何的推广成本。这就具有很大的价值。正如火车刚诞生的时候，有人嘲笑它又笨又重，速度还没有马快。但是很快规模化推广的火车就替代了马车的使用。人工智能也是如此。这也是为什么目前世界上各著名公司以及政府都对此热衷的原因。

　　目前看来，神经网络要想实现人工智能还有很多的路要走，但方向至少是正确的，下面就要看后来者的不断努力了。

**八 总结**

　　神经网络的发展历史，从神经元开始，历经单层神经网络，两层神经网络，直到多层神经网络。神经网络内部实际上就是矩阵计算，在程序中的实现没有“点”和“线”的对象。多层的神经网络可以无限逼近真实的对应函数，从而模拟数据之间的真实关系。
